# =========================
# SECTION 1 / 4
# =========================
import sys
!{sys.executable} -m pip install "numpy<2" --quiet
!{sys.executable} -m pip install scikit-image opencv-python matplotlib scipy scikit-learn tensorflow tifffile --quiet
import json
import os, math, random, warnings
import numpy as np
import pandas as pd
from scipy import ndimage
import cv2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401
import tensorflow as tf
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from scipy import ndimage
from PIL import Image
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Average
from tensorflow.keras.layers import (
    Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate,
    Conv2DTranspose, BatchNormalization, Activation,
    Add, Multiply, Dropout, Reshape, Dense, LeakyReLU, Flatten,
    Lambda, GlobalAveragePooling2D, Conv3D, MaxPooling3D, Conv3DTranspose,
    LayerNormalization, MultiHeadAttention, Attention, Embedding,
    ZeroPadding3D, Cropping3D, SpatialDropout3D, ReLU
)
from tensorflow.keras.applications import InceptionV3
from scipy.linalg import sqrtm
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l2
from tensorflow.keras.mixed_precision import set_global_policy
from tensorflow.keras.utils import Sequence
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import scipy.ndimage as ndimage
from scipy import stats, spatial
import scipy
from scipy.spatial.distance import cdist
import tifffile
from skimage.morphology import ball
try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False
    print("âš ï¸  PIL not available, using OpenCV for image saving")
warnings.filterwarnings("ignore", message="numpy.*")
gc.collect()
# -------------------------
# Helpers & metrics (ADDED)
# -------------------------
def set_global_seed(seed: int = 42):
    """Deterministic-ish runs across numpy/tensorflow/python hash."""
    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)

def safe_makedirs(path: str):
    os.makedirs(path, exist_ok=True)

def normalize01(x, eps=1e-8):
    x = x.astype(np.float32)
    mn, mx = np.min(x), np.max(x)
    return (x - mn) / (mx - mn + eps)

def clip01(x):
    return np.clip(x, 0.0, 1.0).astype(np.float32)

def robust_minmax_match(src, ref, eps=1e-6):
    """Histogram match using percentile anchors for robustness."""
    s1, s99 = np.percentile(src, 1), np.percentile(src, 99)
    r1, r99 = np.percentile(ref, 1), np.percentile(ref, 99)
    out = (src - s1) / (s99 - s1 + eps)
    return clip01(out) * (r99 - r1) + r1

def patchify_2d(img, patch_size=64, stride=64):
    """Return (N, H, W) patches + (y,x) positions."""
    H, W = img.shape[:2]
    ps, st = patch_size, stride
    patches, pos = [], []
    for y in range(0, max(H-ps+1, 1), st):
        for x in range(0, max(W-ps+1, 1), st):
            yy, xx = min(y, H-ps), min(x, W-ps)
            patches.append(img[yy:yy+ps, xx:xx+ps])
            pos.append((yy, xx))
    return np.stack(patches, 0), pos

def gaussian3d_kernel(size=5, sigma=1.0):
    ax = np.arange(-size//2 + 1., size//2 + 1.)
    xx, yy, zz = np.meshgrid(ax, ax, ax, indexing="ij")
    k = np.exp(-(xx**2 + yy**2 + zz**2)/(2.*sigma**2))
    return k / np.sum(k)

# --- lightweight spatial metrics (ADDED)
def morans_I(volume, window=3):
    """Very lightweight spatial autocorrelation proxy (not exact Moran's I)."""
    w = np.ones((window, window, window), np.float32); w /= w.sum()
    mu = ndimage.convolve(volume, w, mode="nearest")
    num = np.mean((volume - mu) * (mu - mu.mean()))
    den = np.var(volume) + 1e-8
    return float(num / den)

def semivariogram_1d(line, max_lag=16):
    """Simple 1D semivariogram along a line."""
    line = line.astype(np.float32).ravel()
    lags = np.arange(1, max_lag+1)
    gamma = []
    for h in lags:
        diff = line[h:] - line[:-h]
        gamma.append(0.5 * np.mean(diff*diff))
    return lags, np.array(gamma, np.float32)

# (kept duplicate name for compatibility; does not reduce lines)
def percentile_thresholds(arr, ps=(20, 40, 60, 80)):
    arr = arr.astype(np.float32)
    return np.percentile(arr, ps)

# -------------------------
# GPU config & mixed precision
# -------------------------
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(f"âœ… Using {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs")
        policy = tf.keras.mixed_precision.Policy('mixed_float16')
        tf.keras.mixed_precision.set_global_policy(policy)
        print("âœ… Mixed precision enabled")
    except RuntimeError as e:
        print(f"âŒ GPU configuration error: {e}")
# ============== ADD THESE STANDALONE FUNCTIONS HERE ==============
# ============== REPLACE EXISTING calculate_fid_score FUNCTION ==============
def calculate_enhanced_quality_metrics(real_images, generated_images, real_volume=None, generated_volume=None):
    """Enhanced quality metrics focusing on geological realism"""
    metrics = {}

    # Basic intensity statistics
    metrics['intensity_mean_real'] = float(np.mean(real_images))
    metrics['intensity_mean_gen'] = float(np.mean(generated_images))
    metrics['intensity_std_real'] = float(np.std(real_images))
    metrics['intensity_std_gen'] = float(np.std(generated_images))

    # Texture analysis using GLCM-like features
    def texture_energy(image):
        if image.ndim == 3:
            image = np.mean(image, axis=2)
        grad_x = np.gradient(image, axis=1)
        grad_y = np.gradient(image, axis=0)
        return float(np.mean(grad_x**2 + grad_y**2))

    metrics['texture_energy_real'] = texture_energy(real_images[0] if real_images.ndim == 4 else real_images)
    metrics['texture_energy_gen'] = texture_energy(generated_images[0] if generated_images.ndim == 4 else generated_images)

    # 3D spatial metrics if volumes are provided
    if real_volume is not None and generated_volume is not None:
        metrics['morans_i_real'] = morans_I(real_volume)
        metrics['morans_i_gen'] = morans_I(generated_volume)

        # Volume connectivity
        metrics['porosity_real'] = float(np.mean(real_volume < 0.5))
        metrics['porosity_gen'] = float(np.mean(generated_volume < 0.5))

    return metrics

def calculate_phase_connectivity(volume, phase_thresholds=None):
    """Calculate phase connectivity metrics"""
    if phase_thresholds is None:
        phase_thresholds = np.percentile(volume, [20, 40, 60, 80])

    phases = np.zeros_like(volume, dtype=int)
    phases[volume < phase_thresholds[0]] = 4
    phases[(volume >= phase_thresholds[0]) & (volume < phase_thresholds[1])] = 3
    phases[(volume >= phase_thresholds[1]) & (volume < phase_thresholds[2])] = 2
    phases[(volume >= phase_thresholds[2]) & (volume < phase_thresholds[3])] = 1
    phases[volume >= phase_thresholds[3]] = 0

    connectivity_metrics = {}
    phase_names = ['Silicates', 'Carbonate', 'Others', 'Clay', 'Kerogen']

    for phase_id, phase_name in enumerate(phase_names):
        phase_mask = (phases == phase_id).astype(np.uint8)
        labeled, num_features = ndimage.label(phase_mask)

        if num_features > 0:
            component_sizes = np.bincount(labeled.ravel())[1:]  # exclude background
            connectivity_metrics[phase_name] = {
                'num_components': num_features,
                'largest_component_size': np.max(component_sizes) if len(component_sizes) > 0 else 0,
                'mean_component_size': np.mean(component_sizes) if len(component_sizes) > 0 else 0,
                'connectivity_ratio': np.max(component_sizes) / np.sum(component_sizes) if np.sum(component_sizes) > 0 else 0
            }
        else:
            connectivity_metrics[phase_name] = {
                'num_components': 0,
                'largest_component_size': 0,
                'mean_component_size': 0,
                'connectivity_ratio': 0
            }

    return connectivity_metrics

def calculate_equivalent_modulus_fem(volume, youngs_modulus_dict, phase_thresholds=None):
    """Calculate equivalent modulus using FEM approach (simplified)"""
    if phase_thresholds is None:
        phase_thresholds = np.percentile(volume, [20, 40, 60, 80])

    # Assign phases based on intensity
    phases = np.zeros_like(volume, dtype=int)
    phases[volume < phase_thresholds[0]] = 4  # Kerogen
    phases[(volume >= phase_thresholds[0]) & (volume < phase_thresholds[1])] = 3  # Clay
    phases[(volume >= phase_thresholds[1]) & (volume < phase_thresholds[2])] = 2  # Others
    phases[(volume >= phase_thresholds[2]) & (volume < phase_thresholds[3])] = 1  # Carbonate
    phases[volume >= phase_thresholds[3]] = 0  # Silicates

    # Convert to material properties
    phase_to_modulus = {
        0: youngs_modulus_dict['Silicates'],
        1: youngs_modulus_dict['Carbonate'],
        2: youngs_modulus_dict['Others'],
        3: youngs_modulus_dict['Clay'],
        4: youngs_modulus_dict['Kerogen']
    }

    # Calculate volume fractions and equivalent modulus (rule of mixtures)
    total_voxels = volume.size
    equivalent_modulus = 0

    for phase_id in range(5):
        phase_fraction = np.sum(phases == phase_id) / total_voxels
        equivalent_modulus += phase_fraction * phase_to_modulus[phase_id]

    return equivalent_modulus

def calculate_volume_statistics(volume):
    """Calculate comprehensive volume statistics"""
    stats = {
        'mean_intensity': float(np.mean(volume)),
        'std_intensity': float(np.std(volume)),
        'min_intensity': float(np.min(volume)),
        'max_intensity': float(np.max(volume)),
        'porosity_05': float(np.mean(volume < 0.5)),
        'porosity_07': float(np.mean(volume < 0.7)),
        'homogeneity': float(1 - (np.std(volume) / (np.mean(volume) + 1e-8))),
        'anisotropy_ratio': float(np.std(volume) / (np.mean(volume) + 1e-8))
    }
    return stats

def calculate_spatial_correlation(volume, max_lag=20):
    """Calculate spatial correlation in 3D volume"""
    center_x, center_y, center_z = [s//2 for s in volume.shape]

    # Calculate correlation along each axis
    corr_x = [np.corrcoef(volume[center_x, center_y, :-lag],
                      volume[center_x, center_y, lag:])[0,1]
          for lag in range(1, min(max_lag, volume.shape[2]//2))]
    corr_y = [np.corrcoef(volume[center_x, :-lag, center_z],
                      volume[center_x, lag:, center_z])[0,1]
          for lag in range(1, min(max_lag, volume.shape[1]//2))]
    corr_z = [np.corrcoef(volume[:-lag, center_y, center_z],
                      volume[lag:, center_y, center_z])[0,1]
          for lag in range(1, min(max_lag, volume.shape[0]//2))]

    return {
        'correlation_x': np.nanmean(corr_x) if len(corr_x) > 0 else 0,
        'correlation_y': np.nanmean(corr_y) if len(corr_y) > 0 else 0,
        'correlation_z': np.nanmean(corr_z) if len(corr_z) > 0 else 0,
        'anisotropy_index': max(np.nanmean(corr_x), np.nanmean(corr_y), np.nanmean(corr_z)) /
                       (min(np.nanmean(corr_x), np.nanmean(corr_y), np.nanmean(corr_z)) + 1e-8)
    }
class ShaleProcessor:
    """Process SEM images and create simplified 5-phase shale model"""

    def __init__(self):
        # Define mineral phase mappings (simplified 5-phase model)
        self.phase_mapping = {
            'silicate': [1, 2],    # Quartz, Feldspar
            'carbonate': [3, 4],   # Calcite, Dolomite
            'clay': [5, 6, 7, 8], # Kaolinite, Illite, Chlorite, Montmorillonite
            'kerogen': [9],       # Organic matter
            'others': [10, 11]    # Other minerals
        }

        # Young's modulus for each phase (GPa) - from Table 2
        self.youngs_modulus = {
            'silicate': 89.6,
            'carbonate': 74.6,
            'clay': 22.3,
            'kerogen': 9.2,
            'others': 12.392
        }

    def simplify_sem_image(self, sem_image):
        """Convert multi-mineral SEM image to 5-phase simplified model"""
        simplified = np.zeros_like(sem_image, dtype=np.uint8)

        # Map minerals to 5 phases (1-5)
        phase_id = 1
        for phase, mineral_ids in self.phase_mapping.items():
            for mineral_id in mineral_ids:
                simplified[sem_image == mineral_id] = phase_id
            phase_id += 1

        return simplified

# LINE 58: ADD ShaleDataset CLASS
class ShaleDataset(Dataset):
    """Dataset for shale SEM images"""

    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.tif'))]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_files[idx])
        image = Image.open(img_path).convert('L')  # Convert to grayscale
        image = np.array(image)

        if self.transform:
            image = self.transform(image)

        # Normalize to [0, 1]
        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0) / 255.0

        return image
class DCGANGenerator(nn.Module):
    """DCGAN Generator for 2D shale samples"""

    def __init__(self, latent_dim=100, feature_maps=64, channels=1):
        super(DCGANGenerator, self).__init__()
        self.latent_dim = latent_dim

        self.main = nn.Sequential(
            # Input: latent_dim x 1 x 1
            nn.ConvTranspose2d(latent_dim, feature_maps * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(feature_maps * 8),
            nn.ReLU(True),
            # State: (feature_maps*8) x 4 x 4

            nn.ConvTranspose2d(feature_maps * 8, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 4),
            nn.ReLU(True),
            # State: (feature_maps*4) x 8 x 8

            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 2),
            nn.ReLU(True),
            # State: (feature_maps*2) x 16 x 16

            nn.ConvTranspose2d(feature_maps * 2, feature_maps, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps),
            nn.ReLU(True),
            # State: (feature_maps) x 32 x 32

            nn.ConvTranspose2d(feature_maps, channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # Output: channels x 64 x 64
        )

    def forward(self, x):
        return self.main(x)
# ADD DCGAN DISCRIMINATOR
class DCGANDiscriminator(nn.Module):
    """DCGAN Discriminator for 2D shale samples"""

    def __init__(self, feature_maps=64, channels=1):
        super(DCGANDiscriminator, self).__init__()

        self.main = nn.Sequential(
            # Input: channels x 64 x 64
            nn.Conv2d(channels, feature_maps, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps) x 32 x 32

            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*2) x 16 x 16

            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*4) x 8 x 8

            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*8) x 4 x 4

            nn.Conv2d(feature_maps * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
            # Output: 1 x 1 x 1
        )

    def forward(self, x):
        return self.main(x).view(-1)

# ADD DCGAN TRAINER
class DCGANTrainer:
    """Trainer for DCGAN"""

    def __init__(self, generator, discriminator, device):
        self.generator = generator.to(device)
        self.discriminator = discriminator.to(device)
        self.device = device

        # Initialize weights
        self.generator.apply(self.weights_init)
        self.discriminator.apply(self.weights_init)

    def weights_init(self, m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            nn.init.normal_(m.weight.data, 0.0, 0.02)
        elif classname.find('BatchNorm') != -1:
            nn.init.normal_(m.weight.data, 1.0, 0.02)
            nn.init.constant_(m.bias.data, 0)

    def train(self, dataloader, num_epochs=800, lr_g=0.002, lr_d=0.001):
        optimizer_g = torch.optim.Adam(self.generator.parameters(), lr=lr_g, betas=(0.5, 0.999))
        optimizer_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))

        criterion = nn.BCELoss()

        g_losses = []
        d_losses = []

        for epoch in range(num_epochs):
            for i, real_imgs in enumerate(dataloader):
                batch_size = real_imgs.size(0)
                real_imgs = real_imgs.to(self.device)

                # Train Discriminator
                optimizer_d.zero_grad()

                # Real images
                real_labels = torch.ones(batch_size, device=self.device)
                real_output = self.discriminator(real_imgs)
                d_loss_real = criterion(real_output, real_labels)

                # Fake images
                noise = torch.randn(batch_size, 100, 1, 1, device=self.device)
                fake_imgs = self.generator(noise)
                fake_labels = torch.zeros(batch_size, device=self.device)
                fake_output = self.discriminator(fake_imgs.detach())
                d_loss_fake = criterion(fake_output, fake_labels)

                d_loss = d_loss_real + d_loss_fake
                d_loss.backward()
                optimizer_d.step()

                # Train Generator
                optimizer_g.zero_grad()
                fake_output = self.discriminator(fake_imgs)
                g_loss = criterion(fake_output, real_labels)  # Trick discriminator
                g_loss.backward()
                optimizer_g.step()

                if i % 50 == 0:
                    g_losses.append(g_loss.item())
                    d_losses.append(d_loss.item())
                    print(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{len(dataloader)}], '
                          f'D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')

        return g_losses, d_losses
class SliceGANGenerator(nn.Module):
    """SliceGAN Generator for 3D shale volume elements"""

    def __init__(self, latent_dim=64, feature_maps=64, channels=1):
        super(SliceGANGenerator, self).__init__()

        self.main = nn.Sequential(
            # Input: latent_dim x 4 x 4 x 4
            nn.ConvTranspose3d(latent_dim, feature_maps * 8, 4, 1, 0, bias=False),
            nn.BatchNorm3d(feature_maps * 8),
            nn.ReLU(True),
            # State: (feature_maps*8) x 4 x 4 x 4

            nn.ConvTranspose3d(feature_maps * 8, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm3d(feature_maps * 4),
            nn.ReLU(True),
            # State: (feature_maps*4) x 8 x 8 x 8

            nn.ConvTranspose3d(feature_maps * 4, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm3d(feature_maps * 2),
            nn.ReLU(True),
            # State: (feature_maps*2) x 16 x 16 x 16

            nn.ConvTranspose3d(feature_maps * 2, feature_maps, 4, 2, 1, bias=False),
            nn.BatchNorm3d(feature_maps),
            nn.ReLU(True),
            # State: (feature_maps) x 32 x 32 x 32

            nn.ConvTranspose3d(feature_maps, channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # Output: channels x 64 x 64 x 64
        )

    def forward(self, x):
        return self.main(x)
class SliceGANDiscriminator(nn.Module):
    """SliceGAN Discriminator for 2D slices"""

    def __init__(self, feature_maps=64, channels=1):
        super(SliceGANDiscriminator, self).__init__()

        self.main = nn.Sequential(
            # Input: channels x 64 x 64
            nn.Conv2d(channels, feature_maps, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps) x 32 x 32

            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*2) x 16 x 16

            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*4) x 8 x 8

            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*8) x 4 x 4

            nn.Conv2d(feature_maps * 8, 1, 4, 1, 0, bias=False)
            # Output: 1 x 1 x 1
        )

    def forward(self, x):
        return self.main(x).view(-1)
class SliceGANTrainer:
    """SliceGAN Trainer with WGAN-GP"""

    def __init__(self, generator, discriminators, device):
        self.generator = generator.to(device)
        self.discriminators = [d.to(device) for d in discriminators]  # 3 discriminators for x,y,z
        self.device = device
        self.gradient_penalty_weight = 10

    def compute_gradient_penalty(self, real_samples, fake_samples, discriminator):
        """Compute gradient penalty for WGAN-GP"""
        alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=self.device)
        interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)
        d_interpolates = discriminator(interpolates)

        gradients = torch.autograd.grad(
            outputs=d_interpolates,
            inputs=interpolates,
            grad_outputs=torch.ones_like(d_interpolates),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]

        gradients = gradients.view(gradients.size(0), -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()

        return gradient_penalty

    def train(self, dataloader, num_epochs=1000, lr_g=0.0025, lr_d=0.0023):
        optimizer_g = torch.optim.Adam(self.generator.parameters(), lr=lr_g, betas=(0.5, 0.999))
        optimizers_d = [torch.optim.Adam(d.parameters(), lr=lr_d, betas=(0.5, 0.999))
                       for d in self.discriminators]

        g_losses = []
        d_losses = []

        for epoch in range(num_epochs):
            for i, real_imgs in enumerate(dataloader):
                batch_size = real_imgs.size(0)
                real_imgs = real_imgs.to(self.device)

                # Train Discriminators
                for dim, (discriminator, optimizer_d) in enumerate(zip(self.discriminators, optimizers_d)):
                    optimizer_d.zero_grad()

                    # Generate 3D volume and take slices
                    noise = torch.randn(batch_size, 64, 4, 4, 4, device=self.device)
                    fake_volumes = self.generator(noise)

                    # Extract slices along current dimension
                    if dim == 0:  # x-axis
                        fake_slices = fake_volumes[:, :, torch.randint(0, 64, (1,)), :, :]
                    elif dim == 1:  # y-axis
                        fake_slices = fake_volumes[:, :, :, torch.randint(0, 64, (1,)), :]
                    else:  # z-axis
                        fake_slices = fake_volumes[:, :, :, :, torch.randint(0, 64, (1,))]

                    fake_slices = fake_slices.squeeze(2) if dim == 0 else fake_slices.squeeze(3) if dim == 1 else fake_slices.squeeze(4)

                    # Real samples
                    real_output = discriminator(real_imgs)
                    fake_output = discriminator(fake_slices.detach())

                    # WGAN loss with gradient penalty
                    d_loss = -torch.mean(real_output) + torch.mean(fake_output)
                    gradient_penalty = self.compute_gradient_penalty(real_imgs, fake_slices, discriminator)
                    d_loss += self.gradient_penalty_weight * gradient_penalty

                    d_loss.backward()
                    optimizer_d.step()

                # Train Generator every 5 discriminator steps
                if i % 5 == 0:
                    optimizer_g.zero_grad()

                    g_loss = 0
                    noise = torch.randn(batch_size, 64, 4, 4, 4, device=self.device)
                    fake_volumes = self.generator(noise)

                    for dim, discriminator in enumerate(self.discriminators):
                        # Extract slices
                        if dim == 0:
                            fake_slices = fake_volumes[:, :, torch.randint(0, 64, (1,)), :, :]
                        elif dim == 1:
                            fake_slices = fake_volumes[:, :, :, torch.randint(0, 64, (1,)), :]
                        else:
                            fake_slices = fake_volumes[:, :, :, :, torch.randint(0, 64, (1,))]

                        fake_slices = fake_slices.squeeze(2) if dim == 0 else fake_slices.squeeze(3) if dim == 1 else fake_slices.squeeze(4)

                        fake_output = discriminator(fake_slices)
                        g_loss += -torch.mean(fake_output)

                    g_loss.backward()
                    optimizer_g.step()

                    g_losses.append(g_loss.item())
                    d_losses.append(d_loss.item())

                    if i % 50 == 0:
                        print(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{len(dataloader)}], '
                              f'D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}')

        return g_losses, d_losses
class BasicBlock3D(nn.Module):
    """Basic 3D residual block"""

    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock3D, self).__init__()
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3,
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm3d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv3d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm3d(out_channels)
            )

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += self.shortcut(residual)
        out = self.relu(out)

        return out
class ResNet183D(nn.Module):
    """ResNet-18 adapted for 3D volume elements"""

    def __init__(self, num_classes=1):
        super(ResNet183D, self).__init__()

        self.in_channels = 64

        # Initial convolution
        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)

        # Residual layers
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)

        # Classifier
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, in_channels, out_channels, blocks, stride):
        layers = []
        layers.append(BasicBlock3D(in_channels, out_channels, stride))
        self.in_channels = out_channels
        for _ in range(1, blocks):
            layers.append(BasicBlock3D(out_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x
class EquivalentModulusPredictor:
    """Train and predict equivalent modulus using ResNet-18"""

    def __init__(self, model, device):
        self.model = model.to(device)
        self.device = device

    def train(self, train_loader, val_loader, num_epochs=500, lr=0.001):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.MSELoss()

        train_losses = []
        val_losses = []

        for epoch in range(num_epochs):
            # Training
            self.model.train()
            train_loss = 0
            for volumes, moduli in train_loader:
                volumes, moduli = volumes.to(self.device), moduli.to(self.device)

                optimizer.zero_grad()
                outputs = self.model(volumes)
                loss = criterion(outputs.squeeze(), moduli)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            # Validation
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for volumes, moduli in val_loader:
                    volumes, moduli = volumes.to(self.device), moduli.to(self.device)
                    outputs = self.model(volumes)
                    loss = criterion(outputs.squeeze(), moduli)
                    val_loss += loss.item()

            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            train_losses.append(train_loss)
            val_losses.append(val_loss)

            if epoch % 50 == 0:
                print(f'Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')

        return train_losses, val_losses

    def predict(self, test_loader):
        self.model.eval()
        predictions = []
        actuals = []

        with torch.no_grad():
            for volumes, moduli in test_loader:
                volumes = volumes.to(self.device)
                outputs = self.model(volumes)
                predictions.extend(outputs.cpu().numpy())
                actuals.extend(moduli.numpy())

        return np.array(predictions), np.array(actuals)
class FIDCalculator:
    """Calculate FID score for generated samples"""

    def __init__(self, device):
        self.device = device
        # Use pre-trained Inception v3
        self.inception = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)
        self.inception.fc = nn.Identity()  # Remove final layer
        self.inception = self.inception.to(device).eval()

    def calculate_fid(self, real_images, generated_images):
        """Calculate FID score between real and generated images"""

        def get_features(images):
            features = []
            with torch.no_grad():
                for img_batch in images:
                    img_batch = img_batch.to(self.device)
                    # Resize to 299x299 for Inception v3
                    img_batch = F.interpolate(img_batch, size=(299, 299), mode='bilinear')
                    # Repeat single channel to 3 channels
                    if img_batch.size(1) == 1:
                        img_batch = img_batch.repeat(1, 3, 1, 1)
                    feat = self.inception(img_batch)
                    features.append(feat.cpu().numpy())
            return np.concatenate(features, axis=0)

        real_features = get_features(real_images)
        gen_features = get_features(generated_images)

        # Calculate statistics
        mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)
        mu_gen, sigma_gen = np.mean(gen_features, axis=0), np.cov(gen_features, rowvar=False)

        # Calculate FID
        diff = mu_real - mu_gen
        covmean = self._sqrtm(sigma_real.dot(sigma_gen))

        if np.iscomplexobj(covmean):
            covmean = covmean.real

        fid = diff.dot(diff) + np.trace(sigma_real + sigma_gen - 2 * covmean)
        return fid

    def _sqrtm(self, matrix):
        """Matrix square root"""
        U, s, V = np.linalg.svd(matrix)
        return U.dot(np.diag(np.sqrt(s))).dot(V)
# -------------------------
# AdvancedMineralProcessor (unchanged logic, kept to preserve lines)
# -------------------------
# -------------------------
class AdvancedMineralProcessor:
    """Advanced mineral composition processor with exact paper methodology"""

    def __init__(self, excel_path):
        self.excel_path = excel_path
        self.mineral_data = {}

        self.sample_mapping = {
            'sample1': 'Sample 10555\\10555',
            'sample2': 'Sample 11203\\11203',
            'sample3': 'Sample 11206\\11206',
            'sample4': 'Sample 12162\\12162',
            'sample5': 'Sample 17699\\17699',
            'sample6': 'Sample 19472\\19472',
            'sample7': 'Sample 21298\\21298',
            'sample8': 'Sample 23285\\23285'
        }
        self.five_phase_model = {
            'Silicates': ['Quartz', 'Alkali Feldspar', 'Plagioclase'],
            'Carbonate': ['Calcite', 'Dolomite', 'Ankerite', 'Siderite'],
            'Clay': ['Illite', 'Chlorite', 'Kaolinite', 'Muscovite', 'Biotite'],
            'Kerogen': [],
            'Others': ['Pyrite', 'Zircon', 'Rutile', 'Ilmenite', 'Apatite', 'Monazite', 'Unclassified']
        }

        self.youngs_modulus = {
            'Silicates': 89.6,
            'Carbonate': 74.6,
            'Clay': 22.3,
            'Kerogen': 9.2,
            'Others': 12.392
        }

        self.density_values = {
            'Silicates': 2.65,
            'Carbonate': 2.71,
            'Clay': 2.60,
            'Kerogen': 1.30,
            'Others': 3.50
        }

    def load_and_parse_excel(self):
        print("ðŸ“Š Loading and parsing Excel data...")
        try:
            df = pd.read_excel(self.excel_path, sheet_name='Mineral quant_all samples', header=None)
            print(f"ðŸ“ Excel shape: {df.shape}")
            self.process_single_sheet_data(df)
            print("âœ… Excel data loaded and parsed successfully")
        except Exception as e:
            print(f"âŒ Error loading Excel: {e}")
            print("ðŸ”„ Creating realistic compositions based on paper...")
            self.create_realistic_compositions()

    def process_single_sheet_data(self, df):
        print("ðŸ“Š Processing single sheet Excel data...")
        area_start = self.find_data_start(df, 'MODAL AREA%')
        wt_start = self.find_data_start(df, 'MODAL WT%')
        assay_start = self.find_data_start(df, 'ASSAY')
        properties_start = self.find_data_start(df, 'PROPERTIES')
        print(f"ðŸ“Š Section starts - Area: {area_start}, WT: {wt_start}, Assay: {assay_start}, Properties: {properties_start}")

        for sample_key, excel_key in self.sample_mapping.items():
            sample_data = {
                'five_phase_area': {},
                'five_phase_wt': {},
                'assay': {},
                'properties': {},
                'detailed_area': {},
                'detailed_wt': {}
            }
            if area_start >= 0:
                self.extract_mineral_data(df, area_start, self.find_sample_column(df, excel_key, area_start),
                                          sample_data['detailed_area'], sample_data['five_phase_area'])
            if wt_start >= 0:
                self.extract_mineral_data(df, wt_start, self.find_sample_column(df, excel_key, wt_start),
                                          sample_data['detailed_wt'], sample_data['five_phase_wt'])
            if assay_start >= 0:
                self.extract_assay_data(df, assay_start, self.find_sample_column(df, excel_key, assay_start), sample_data['assay'])
            if properties_start >= 0:
                self.extract_properties_data(df, properties_start, self.find_sample_column(df, excel_key, properties_start), sample_data['properties'])

            carbon_content = sample_data['assay'].get('C', 0)
            kerogen_content = carbon_content * 1.2
            sample_data['five_phase_area']['Kerogen'] = kerogen_content
            sample_data['five_phase_wt']['Kerogen'] = kerogen_content
            self.normalize_compositions(sample_data)
            self.mineral_data[sample_key] = sample_data
            print(f"âœ… Processed {sample_key}: {sample_data['five_phase_area']}")

    def find_data_start(self, df, keyword='Quartz'):
        for idx, row in df.iterrows():
            for cell in row:
                if isinstance(cell, str) and keyword in cell:
                    return idx
        return 0

    def find_sample_column(self, df, excel_key, data_start):
        header_row = data_start - 1 if data_start > 0 else 0
        for col_idx in range(len(df.columns)):
            cell_value = df.iloc[header_row, col_idx]
            if isinstance(cell_value, str) and excel_key in cell_value:
                return col_idx
        return -1

    def extract_mineral_data(self, df, start_row, col_idx, detailed_dict, five_phase_dict):
        mineral_rows = {}
        for idx in range(start_row, len(df)):
            mineral_name = df.iloc[idx, 0]
            if isinstance(mineral_name, str) and mineral_name.strip():
                try:
                    value = float(df.iloc[idx, col_idx])
                    detailed_dict[mineral_name] = value
                    mineral_rows[mineral_name] = value
                except (ValueError, TypeError):
                    continue
        for phase, minerals in self.five_phase_model.items():
            phase_total = 0
            for mineral in minerals:
                for detailed_mineral, value in mineral_rows.items():
                    if mineral.lower() in detailed_mineral.lower():
                        phase_total += value
                        break
            five_phase_dict[phase] = phase_total

    def extract_assay_data(self, df, start_row, col_idx, assay_dict):
        for idx in range(start_row, len(df)):
            element = df.iloc[idx, 0]
            if isinstance(element, str) and element.strip():
                try:
                    value = float(df.iloc[idx, col_idx])
                    assay_dict[element] = value
                except (ValueError, TypeError):
                    continue

    def extract_properties_data(self, df, start_row, col_idx, properties_dict):
        for idx in range(start_row, len(df)):
            property_name = df.iloc[idx, 0]
            if isinstance(property_name, str) and property_name.strip():
                try:
                    value = float(df.iloc[idx, col_idx])
                    properties_dict[property_name] = value
                except (ValueError, TypeError):
                    continue

    def normalize_compositions(self, sample_data):
        for comp_type in ['five_phase_area', 'five_phase_wt']:
            total = sum(sample_data[comp_type].values())
            if total > 0:
                for phase in sample_data[comp_type]:
                    sample_data[comp_type][phase] = (sample_data[comp_type][phase] / total) * 100

    def create_realistic_compositions(self):
        print("ðŸ”„ Creating realistic compositions based on paper...")
        realistic_data = {
            'sample1': {'Silicates': 75.71, 'Carbonate': 1.64, 'Clay': 20.14, 'Kerogen': 0.26, 'Others': 2.51},
            'sample2': {'Silicates': 75.29, 'Carbonate': 6.39, 'Clay': 15.20, 'Kerogen': 1.03, 'Others': 3.12},
            'sample3': {'Silicates': 63.41, 'Carbonate': 8.59, 'Clay': 24.28, 'Kerogen': 1.34, 'Others': 3.72},
            'sample4': {'Silicates': 67.29, 'Carbonate': 15.81, 'Clay': 13.30, 'Kerogen': 2.57, 'Others': 3.60},
            'sample5': {'Silicates': 90.48, 'Carbonate': 1.19, 'Clay': 5.94, 'Kerogen': 0.24, 'Others': 2.39},
            'sample6': {'Silicates': 44.08, 'Carbonate': 1.40, 'Clay': 50.52, 'Kerogen': 0.22, 'Others': 4.00},
            'sample7': {'Silicates': 88.79, 'Carbonate': 1.04, 'Clay': 6.84, 'Kerogen': 0.17, 'Others': 3.33},
            'sample8': {'Silicates': 93.95, 'Carbonate': 2.80, 'Clay': 1.41, 'Kerogen': 0.46, 'Others': 1.84}
        }
        for sample_key in self.sample_mapping.keys():
            composition = realistic_data.get(sample_key, realistic_data['sample1'])
            self.mineral_data[sample_key] = {
                'five_phase_area': composition,
                'five_phase_wt': composition,
                'assay': {'C': composition['Kerogen'] / 1.2},
                'properties': {'Sample Density': 2.70, 'Hardness': 5.5},
                'detailed_area': composition,
                'detailed_wt': composition
            }

    def get_sample_composition(self, sample_name):
        return self.mineral_data.get(sample_name, {})

    def get_phase_modulus(self, phase):
        return self.youngs_modulus.get(phase, 12.392)

    def get_phase_density(self, phase):
        return self.density_values.get(phase, 2.70)

# -------------------------
# AdvancedSEMProcessor (MODIFIED: create_training_patches now multi-scale & overlapped)
# -------------------------
class AdvancedSEMProcessor:
    """Advanced SEM image processor with patch generation"""

    def __init__(self, base_path):
        self.base_path = base_path

    def load_sample_images(self, sample_name):
        sample_path = os.path.join(self.base_path, sample_name)
        print(f"ðŸ”¬ Loading SEM images from: {sample_path}")

        data = {'mineral_map': None, 'bse': None, 'minerals': {}, 'elements': {}, 'metadata': {}}

        mineral_map_path = self.find_file(sample_path, ['Mineral Map.tif', 'Mineral_Map.tif',
                                                       'mineral_map.tif', '10555_Mineral_Map_scale.tif'])
        if mineral_map_path:
            data['mineral_map'] = self.load_tiff_image(mineral_map_path)
            print(f"âœ… Loaded Mineral Map: {os.path.basename(mineral_map_path)}")

        bse_path = self.find_file(sample_path, ['BSE.tif', 'bse.tif', 'Backscattered.tif'])
        if bse_path:
            data['bse'] = self.load_tiff_image(bse_path)
            print(f"âœ… Loaded BSE: {os.path.basename(bse_path)}")

        minerals_path = os.path.join(sample_path, 'Minerals')
        if os.path.exists(minerals_path):
            for file in os.listdir(minerals_path):
                if file.endswith(('.tif', '.tiff')):
                    mineral_name = os.path.splitext(file)[0]
                    img_path = os.path.join(minerals_path, file)
                    data['minerals'][mineral_name] = self.load_tiff_image(img_path)
                    print(f"âœ… Loaded mineral: {mineral_name}")

        elements_path = os.path.join(sample_path, 'Elements')
        if os.path.exists(elements_path):
            for file in os.listdir(elements_path):
                if file.endswith(('.tif', '.tiff')):
                    element_name = os.path.splitext(file)[0]
                    img_path = os.path.join(elements_path, file)
                    data['elements'][element_name] = self.load_tiff_image(img_path)
                    print(f"âœ… Loaded element: {element_name}")

        return data

    def find_file(self, directory, possible_names):
        for name in possible_names:
            path = os.path.join(directory, name)
            if os.path.exists(path):
                return path
        return None

    def load_tiff_image(self, file_path):
        try:
            img = tifffile.imread(file_path)
            if img.dtype == np.uint16:
                img = img.astype(np.float32) / 65535.0
            elif img.dtype == np.uint8:
                img = img.astype(np.float32) / 255.0
            else:
                img = img.astype(np.float32)
                if img.max() > 1.0:
                    img = img / img.max()
            if img.shape[0] != 1024 or img.shape[1] != 1024:
                img = cv2.resize(img, (1024, 1024), interpolation=cv2.INTER_CUBIC)
                print(f"ðŸ”„ Resized to: {img.shape}")
            return img
        except Exception as e:
            print(f"âŒ Error loading {file_path}: {e}")
            try:
                img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)
                if img is None:
                    raise ValueError("OpenCV could not read image")
                if img.dtype == np.uint16:
                    img = img.astype(np.float32) / 65535.0
                elif img.dtype == np.uint8:
                    img = img.astype(np.float32) / 255.0
                else:
                    img = img.astype(np.float32)
                    if img.max() > 1.0:
                        img = img / img.max()
                if img.shape[0] != 1024 or img.shape[1] != 1024:
                    img = cv2.resize(img, (1024, 1024), interpolation=cv2.INTER_CUBIC)
                return img
            except Exception as e2:
                print(f"âŒ OpenCV also failed: {e2}")
                return None

    # -------- MODIFIED: multi-scale + overlap
    def create_training_patches(self, image, grid_size=10, patch_size=64, use_overlap=True):
        """Create multi-scale, overlapped training patches (improves continuity)."""
        if image is None:
            print("âŒ No image provided for patch creation")
            return None, None
        img = normalize01(image)
        stride = patch_size//2 if use_overlap else patch_size
        p1, pos = patchify_2d(img, patch_size=patch_size, stride=stride)
        s2 = cv2.resize(img, (img.shape[1]//2, img.shape[0]//2), interpolation=cv2.INTER_AREA)
        p2, _  = patchify_2d(s2, patch_size=patch_size, stride=stride)
        s3 = cv2.resize(img, (img.shape[1]//4, img.shape[0]//4), interpolation=cv2.INTER_AREA)
        p3, _  = patchify_2d(s3, patch_size=patch_size, stride=stride)
        patches = np.concatenate([p1, p2, p3], axis=0)
        patches = patches[..., None] if patches.ndim==3 else patches  # ensure channel
        print(f"âœ… Created {patches.shape[0]} patches of {patches.shape[1:]} (3 scales, stride={stride})")
        return patches, pos

    def enhance_image_quality(self, image):
        if image is None:
            return None
        print("âœ¨ Enhancing image quality...")
        img_uint8 = (clip01(image) * 255).astype(np.uint8)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(img_uint8)
        denoised = cv2.fastNlMeansDenoising(enhanced, h=10)
        smoothed = cv2.GaussianBlur(denoised, (3, 3), 1.0)
        return smoothed.astype(np.float32) / 255.0
# =========================
# SECTION 2 / 4
# =========================

# ---------- Mineral palette (ADDED)
class MineralPalette:
    PHASES = ['Silicates','Carbonate','Others','Clay','Kerogen']
    DEFAULT = {
        'Silicates': (1.00, 0.85, 0.20),
        'Carbonate': (0.20, 0.80, 0.20),
        'Others'   : (0.85, 0.25, 0.25),
        'Clay'     : (0.55, 0.35, 0.85),
        'Kerogen'  : (0.15, 0.15, 0.15),
    }
    P2I = {p:i for i,p in enumerate(PHASES)}
    I2P = {i:p for p,i in P2I.items()}

    def __init__(self, base_path):
        self.base_path = base_path
        self.palette = dict(self.DEFAULT)

    def _mean_rgb(self, arr):
        if arr.ndim == 2:
            rgb = np.stack([arr]*3, -1)
        elif arr.shape[-1] > 3:
            rgb = arr[..., :3]
        else:
            rgb = arr
        rgb = normalize01(rgb)
        return tuple(np.mean(rgb.reshape(-1,3), axis=0))

    def try_extract_from_folder(self, sample_name):
        sample_dir = os.path.join(self.base_path, sample_name, "Minerals")
        if not os.path.isdir(sample_dir): return
        buckets = {p:[] for p in self.PHASES}
        for f in os.listdir(sample_dir):
            if not f.lower().endswith(('.tif','.tiff','.png','.jpg','.jpeg')): continue
            fp = os.path.join(sample_dir, f)
            try:
                arr = tifffile.imread(fp)
            except Exception:
                arr = cv2.imread(fp, cv2.IMREAD_UNCHANGED)
            if arr is None: continue
            lname = f.lower()
            if any(k in lname for k in ["quartz","feldspar","plagio","silic"]): phase="Silicates"
            elif any(k in lname for k in ["calcite","dolomite","ankerite","siderite","carbon"]): phase="Carbonate"
            elif any(k in lname for k in ["illite","chlorite","kaolin","muscov","biotite","clay"]): phase="Clay"
            elif any(k in lname for k in ["kerogen","organic"]): phase="Kerogen"
            else: phase="Others"
            buckets[phase].append(self._mean_rgb(arr))
        for phase, cols in buckets.items():
            if len(cols):
                self.palette[phase] = tuple(np.mean(np.array(cols), axis=0))

    def colorize_labels(self, label_vol):
        H,W,D = label_vol.shape
        rgb = np.zeros((H,W,D,3), np.float32)
        for i, phase in self.I2P.items():
            rgb[label_vol==i] = self.palette[phase]
        return rgb

    def id_to_color(self, idx):
        return self.palette[self.I2P[int(idx)]]

# ---------- Continuity/morphology (ADDED)
class VolumeContinuityEnforcer:
    def __init__(self, smooth_sigma=1.0, cc_min_frac=0.0005):
        self.sigma = float(smooth_sigma)
        self.cc_min_frac = float(cc_min_frac)
        self._gauss3d = gaussian3d_kernel(5, 1.0)

    def anisotropic_diffusion_3d(self, vol, iters=6, kappa=45.0, gamma=0.12):
        vol = vol.astype(np.float32)
        for _ in range(iters):
            gx, gy, gz = np.gradient(vol)
            gmag = np.sqrt(gx*gx + gy*gy + gz*gz)
            c = 1.0/(1.0+(gmag/kappa)**2)

        # Calculate divergence term properly
            cx = np.gradient(c * gx, axis=0)
            cy = np.gradient(c * gy, axis=1)
            cz = np.gradient(c * gz, axis=2)
            vol = vol + gamma * (cx + cy + cz)
        return np.clip(vol, 0, 1)

    def gaussian_3d(self, vol, sigma=None):
        return ndimage.gaussian_filter(vol, self.sigma if sigma is None else float(sigma))

    def sharpen_boundaries(self, vol, alpha=0.3):
        blur = self.gaussian_3d(vol, sigma=1.2)
        return clip01(vol + alpha*(vol - blur))

    def smooth_labels_3d(self, lbl, radius=1):
        se = ball(int(radius))
        out = lbl.copy()
        for _ in range(2):
            votes = np.zeros((lbl.shape[0], lbl.shape[1], lbl.shape[2], 5), np.int32)
            for lab in range(5):
                votes[..., lab] = ndimage.convolve((out==lab).astype(np.int32), se, mode='nearest')
            out = np.argmax(votes, axis=-1).astype(np.int32)
        return out

    def prune_small_components(self, lbl):
        H,W,D = lbl.shape; total = H*W*D
        cleaned = lbl.copy()
        for lab in range(5):
            mask = (lbl==lab).astype(np.uint8)
            L, n = ndimage.label(mask)
            if n <= 1: continue
            sizes = np.bincount(L.ravel())
            keep = set(np.where(sizes >= max(8, int(self.cc_min_frac*total)))[0]); keep.discard(0)
            keep_mask = np.isin(L, list(keep))
            holes = (~keep_mask) & (lbl==lab)
            if holes.any():
                _, idx = ndimage.distance_transform_edt(~holes, return_indices=True)
                cleaned[holes] = lbl[tuple(idx[:, holes])]
        return cleaned

# ---------- Labeling + composition prior (ADDED)
class LabelAssigner:
    def __init__(self, target_comp):
        self.target = target_comp

    def thresholds_from_intensity(self, vol):
        return percentile_thresholds(vol, (20,40,60,80))

    def assign(self, vol):
        t20,t40,t60,t80 = self.thresholds_from_intensity(vol)
        L = np.zeros_like(vol, np.int32)
        L[vol<t20] = 4
        L[(vol>=t20)&(vol<t40)] = 3
        L[(vol>=t40)&(vol<t60)] = 2
        L[(vol>=t60)&(vol<t80)] = 1
        L[vol>=t80] = 0
        return L

    def soft_rebalance(self, lbl):
        keys = ['Silicates','Carbonate','Others','Clay','Kerogen']
        tgt = np.array([self.target.get(k,0) for k in keys], np.float32)
        cur = np.array([(lbl==i).mean()*100.0 for i in range(5)], np.float32)
        diff = tgt - cur
        if np.all(np.abs(diff) < 0.5): return lbl
        out = lbl.copy()
        order = np.argsort(-np.abs(diff))
        kernel = np.ones((3,3,3), np.uint8)
        for cls in order:
            if diff[cls] <= 0: continue
            donor = int(np.argmax(cur - tgt))
            if donor == cls: continue
            donor_mask = (out==donor)
            neighbor = ndimage.binary_dilation((out==cls), structure=kernel)
            border = donor_mask & neighbor
            need = int(((diff[cls]/100.0) * lbl.size) / 4)
            idxs = np.where(border.ravel())[0]
            if idxs.size == 0: continue
            sel = np.random.choice(idxs, size=min(need, idxs.size), replace=False)
            out.ravel()[sel] = cls
            cur = np.array([(out==i).mean()*100.0 for i in range(5)], np.float32)
        return out

class CompositionTools:
    @staticmethod
    def composition_from_labels(lbl):
        return dict(zip(['Silicates','Carbonate','Others','Clay','Kerogen'],
                        [(lbl==i).mean()*100.0 for i in range(5)]))

    @staticmethod
    def similarity_score(a, b):
        keys = ['Silicates','Carbonate','Others','Clay','Kerogen']
        A = np.array([a.get(k,0) for k in keys], np.float32)
        B = np.array([b.get(k,0) for k in keys], np.float32)
        return float(1.0 - np.mean(np.abs(A - B)/100.0))

# ---------- Multi-scale assembler (ADDED)
class RealisticShaleAssembler:
    """
    Advanced shale assembler that creates geologically realistic 3D shale volumes
    with proper bedding, fractures, and mineral distributions
    """

    def __init__(self, vol_size=128, patch_size=64, mineral_composition=None):
        self.V = int(vol_size)
        self.P = int(patch_size)
        self.mineral_comp = mineral_composition or {
            'Silicates': 70, 'Carbonate': 15, 'Clay': 10, 'Kerogen': 3, 'Others': 2
        }

    def create_realistic_shale_volume(self, sem_reference):
        """Create a geologically realistic 3D shale volume"""
        print("ðŸ—ï¸ Creating realistic 3D shale volume with geological features...")

        V = self.V
        volume = np.zeros((V, V, V), dtype=np.float32)

        # Step 1: Create base sedimentary structure
        volume = self._create_sedimentary_bedding(volume)

        # Step 2: Add mineral phase distributions
        volume = self._add_mineral_phases(volume, sem_reference)

        # Step 3: Add natural fractures and cracks
        volume = self._add_fracture_systems(volume)

        # Step 4: Add pore space and organic matter
        volume = self._add_pore_organic_matter(volume)

        # Step 5: Apply geological smoothing and blending
        volume = self._geological_post_processing(volume)

        print("âœ… Realistic 3D shale volume created")
        return volume

    def _create_sedimentary_bedding(self, volume):
        """Create horizontal bedding layers characteristic of shale"""
        V = volume.shape[0]

        # Create multiple bedding layers with varying thickness
        z_pos = 0
        bed_id = 0

        while z_pos < V:
            # Random bed thickness (2-15 voxels)
            bed_thickness = np.random.randint(2, 15)
            if z_pos + bed_thickness >= V:
                break

            # Each bed has unique properties
            bed_intensity = 0.3 + 0.6 * np.random.random()
            bed_texture = self._create_bed_texture(V, V, bed_thickness)

            # Apply bedding with gradual transitions
            for z in range(bed_thickness):
                if z_pos + z < V:
                    transition_factor = self._transition_profile(z, bed_thickness)
                    volume[:, :, z_pos + z] = bed_intensity * bed_texture[:, :, z] * transition_factor

            z_pos += bed_thickness
            bed_id += 1

        return volume

    def _create_bed_texture(self, h, w, depth):
        """Create realistic sedimentary texture for each bed"""
        texture = np.zeros((h, w, depth), dtype=np.float32)

        for z in range(depth):
            # Base sedimentary pattern with laminations
            x, y = np.meshgrid(np.linspace(0, 4*np.pi, w), np.linspace(0, 4*np.pi, h))

            # Multiple frequency components for natural look
            base = (np.sin(0.5*x + z*0.1) * np.cos(0.5*y) * 0.4 +
                   np.sin(1.5*x) * np.cos(1.5*y + z*0.2) * 0.3 +
                   np.sin(3.0*x + z*0.05) * np.cos(3.0*y) * 0.2 +
                   np.random.normal(0, 0.1, (h, w)))

            # Add grain-like structures
            grains = self._add_mineral_grains(h, w)
            texture[:, :, z] = np.clip(base + grains*0.3, 0, 1)

        return texture

    def _add_mineral_grains(self, h, w):
        """Add realistic mineral grain distributions"""
        grains = np.zeros((h, w), dtype=np.float32)

        # Add quartz/feldspar grains (bright)
        n_grains = np.random.randint(50, 150)
        for _ in range(n_grains):
            size = np.random.randint(2, 8)
            center_x = np.random.randint(size, w-size)
            center_y = np.random.randint(size, h-size)
            intensity = 0.7 + 0.3 * np.random.random()

            for i in range(-size, size+1):
                for j in range(-size, size+1):
                    if 0 <= center_x+i < w and 0 <= center_y+j < h:
                        dist = np.sqrt(i**2 + j**2)
                        if dist <= size:
                            factor = 1.0 - (dist / size)
                            grains[center_y+j, center_x+i] = max(
                                grains[center_y+j, center_x+i],
                                intensity * factor
                            )

        return grains

    def _add_mineral_phases(self, volume, sem_reference):
        """Distribute mineral phases according to composition"""
        V = volume.shape[0]

        # Create probability maps for each mineral phase
        phase_maps = {}
        total_prob = 0

        for phase, percentage in self.mineral_comp.items():
            if percentage > 0:
                # Each phase has characteristic spatial distribution
                if phase == 'Silicates':
                    # Silicates: clustered grains
                    prob_map = self._create_silicate_distribution(V, V, V)
                elif phase == 'Clay':
                    # Clay: fine-grained, layered
                    prob_map = self._create_clay_distribution(V, V, V)
                elif phase == 'Carbonate':
                    # Carbonate: vein-like patterns
                    prob_map = self._create_carbonate_distribution(V, V, V)
                elif phase == 'Kerogen':
                    # Kerogen: organic matter, often in layers
                    prob_map = self._create_kerogen_distribution(V, V, V)
                else:
                    # Others: random distribution
                    prob_map = np.random.random((V, V, V))

                phase_maps[phase] = prob_map * (percentage / 100.0)
                total_prob += phase_maps[phase]

        # Normalize and assign phases
        if total_prob.max() > 0:
            for phase in phase_maps:
                phase_maps[phase] = phase_maps[phase] / total_prob

        # Blend SEM reference texture with mineral phases - FIXED
        if sem_reference is not None:
            # Convert to grayscale if needed
            if sem_reference.ndim == 3:
                sem_gray = np.mean(sem_reference, axis=2)  # Convert RGB to grayscale
            else:
                sem_gray = sem_reference

            sem_resized = cv2.resize(sem_gray, (V, V))  # Now this is 2D
            for z in range(V):
                volume[:, :, z] = 0.7 * volume[:, :, z] + 0.3 * sem_resized

        return volume

    def _create_silicate_distribution(self, h, w, d):
        """Create clustered silicate grain distribution"""
        dist_map = np.zeros((h, w, d), dtype=np.float32)

        # Create grain clusters
        n_clusters = np.random.randint(5, 15)
        for _ in range(n_clusters):
            center_x = np.random.randint(0, w)
            center_y = np.random.randint(0, h)
            center_z = np.random.randint(0, d)
            cluster_size = np.random.randint(10, 30)

            for x in range(max(0, center_x-cluster_size), min(w, center_x+cluster_size)):
                for y in range(max(0, center_y-cluster_size), min(h, center_y+cluster_size)):
                    for z in range(max(0, center_z-cluster_size//2), min(d, center_z+cluster_size//2)):
                        dist = np.sqrt((x-center_x)**2 + (y-center_y)**2 + ((z-center_z)*2)**2)
                        if dist < cluster_size:
                            intensity = 1.0 - (dist / cluster_size)
                            dist_map[y, x, z] = max(dist_map[y, x, z], intensity)

        return dist_map

    def _create_clay_distribution(self, h, w, d):
        """Create fine-grained clay distribution"""
        dist_map = np.zeros((h, w, d), dtype=np.float32)

        # Clay is fine-grained and often layered
        for z in range(d):
            # Horizontal layering with fine texture
            base = np.random.normal(0.5, 0.1, (h, w))
            # Add fine laminations
            for i in range(3):
                lam_thickness = np.random.randint(1, 3)
                lam_pos = np.random.randint(0, h-lam_thickness)
                base[lam_pos:lam_pos+lam_thickness, :] += 0.2

            dist_map[:, :, z] = np.clip(base, 0, 1)

        return dist_map

    def _create_carbonate_distribution(self, h, w, d):
        """Create vein-like carbonate distribution"""
        dist_map = np.zeros((h, w, d), dtype=np.float32)

        # Carbonate often forms veins
        n_veins = np.random.randint(3, 8)
        for _ in range(n_veins):
            # Random vein orientation
            if np.random.random() > 0.5:
                # Horizontal vein
                z_level = np.random.randint(5, d-5)
                thickness = np.random.randint(1, 4)
                intensity = 0.8 + 0.2 * np.random.random()

                for z in range(max(0, z_level-thickness), min(d, z_level+thickness)):
                    dist_map[:, :, z] = np.maximum(dist_map[:, :, z], intensity)
            else:
                # Vertical vein
                x_start = np.random.randint(0, w)
                vein_width = np.random.randint(2, 6)
                for x in range(x_start, min(w, x_start+vein_width)):
                    dist_map[:, x, :] = np.maximum(dist_map[:, x, :], 0.7)

        return dist_map

    def _create_kerogen_distribution(self, h, w, d):
        """Create organic matter (kerogen) distribution"""
        dist_map = np.zeros((h, w, d), dtype=np.float32)

        # Kerogen is often in discrete layers or patches
        n_patches = np.random.randint(10, 30)
        for _ in range(n_patches):
            center_x = np.random.randint(0, w)
            center_y = np.random.randint(0, h)
            center_z = np.random.randint(0, d)
            patch_size = np.random.randint(3, 10)

            for x in range(max(0, center_x-patch_size), min(w, center_x+patch_size)):
                for y in range(max(0, center_y-patch_size), min(h, center_y+patch_size)):
                    for z in range(max(0, center_z-1), min(d, center_z+2)):  # Flat patches
                        dist = np.sqrt((x-center_x)**2 + (y-center_y)**2)
                        if dist < patch_size:
                            intensity = 0.3 + 0.4 * (1.0 - dist/patch_size)
                            dist_map[y, x, z] = max(dist_map[y, x, z], intensity)

        return dist_map

    def _add_fracture_systems(self, volume):
        """Add natural fracture systems"""
        V = volume.shape[0]

        # Add vertical fractures (common in shale)
        n_fractures = np.random.randint(3, 8)
        for _ in range(n_fractures):
            fracture_x = np.random.randint(10, V-10)
            fracture_width = np.random.randint(1, 3)
            fracture_length = np.random.randint(20, V-20)

            for x in range(fracture_x, min(V, fracture_x+fracture_width)):
                for y in range(fracture_length):
                    for z in range(V):
                        # Darken fracture areas
                        volume[y, x, z] *= 0.6

        # Add some horizontal parting
        n_partings = np.random.randint(2, 5)
        for _ in range(n_partings):
            parting_z = np.random.randint(5, V-5)
            parting_thickness = np.random.randint(1, 2)

            for z in range(parting_z, min(V, parting_z+parting_thickness)):
                volume[:, :, z] *= 0.7

        return volume

    def _add_pore_organic_matter(self, volume):
        """Add pore space and additional organic matter"""
        V = volume.shape[0]

        # Add micro-porosity
        porosity_mask = np.random.random((V, V, V)) > 0.95  # 5% porosity
        volume[porosity_mask] *= 0.3  # Dark pores

        # Add scattered organic matter
        organic_mask = np.random.random((V, V, V)) > 0.98  # 2% additional organic
        volume[organic_mask] *= 0.5

        return volume

    def _geological_post_processing(self, volume):
        """Apply final geological processing"""
        # Gentle smoothing to mimic natural processes
        volume = ndimage.gaussian_filter(volume, sigma=0.8)

        # Enhance contrasts slightly
        volume = np.clip(volume * 1.1, 0, 1)

        # Add final subtle noise for natural appearance
        noise = np.random.normal(0, 0.02, volume.shape)
        volume = np.clip(volume + noise, 0, 1)

        return volume

    def _transition_profile(self, position, length):
        """Create smooth transition between beds"""
        # Smooth sigmoid-like transition
        x = (position / length) * 2 - 1  # -1 to 1
        return 1.0 / (1.0 + np.exp(-4*x))  # Smooth step function
# ---------- Enhanced Continuous Assembler (ADDED)
class EnhancedContinuousAssembler:
    """
    Enhanced continuous assembler that creates highly continuous 3D shale volumes
    with perfect geological continuity and natural appearance
    """

    def __init__(self, vol_size=128, patch_size=64):
        self.V = int(vol_size)
        self.P = int(patch_size)

    def create_continuous_shale_volume(self, sem_reference, target_composition):
        """Create highly continuous 3D shale volume with perfect geological coherence"""
        print("ðŸ—ï¸ Creating enhanced continuous 3D shale volume...")

        V = self.V
        volume = np.zeros((V, V, V), dtype=np.float32)

        # Create base geological structure with perfect continuity
        volume = self._create_perfect_geological_continuity(volume)

        # Incorporate SEM reference texture
        if sem_reference is not None:
            volume = self._blend_sem_texture(volume, sem_reference)

        # Apply composition-aware mineral distribution
        volume = self._apply_composition_distribution(volume, target_composition)

        # Final geological refinement
        volume = self._geological_refinement(volume)

        print("âœ… Enhanced continuous 3D shale volume created")
        return clip01(volume)

    def _create_perfect_geological_continuity(self, volume):
        """Create perfectly continuous geological structure"""
        V = volume.shape[0]

        # Create base sedimentary pattern with perfect continuity
        x, y, z = np.meshgrid(np.linspace(0, 4*np.pi, V),
                             np.linspace(0, 4*np.pi, V),
                             np.linspace(0, 2*np.pi, V), indexing='ij')

        # Multi-scale geological patterns
        base_pattern = (np.sin(0.5*x) * np.cos(0.5*y) * 0.4 +
                       np.sin(1.0*x + z*0.3) * np.cos(1.0*y) * 0.3 +
                       np.sin(2.0*x) * np.cos(2.0*y + z*0.1) * 0.2 +
                       np.random.normal(0, 0.05, (V, V, V)))

        # Strong horizontal bedding with perfect continuity
        for z_slice in range(V):
            bedding_strength = 0.3 + 0.2 * np.sin(z_slice * 0.05)
            base_pattern[:, :, z_slice] += bedding_strength

        return normalize01(base_pattern)

    def _blend_sem_texture(self, volume, sem_reference):
        """Blend SEM reference texture while maintaining 3D continuity"""
        if sem_reference.ndim == 3:
            sem_gray = np.mean(sem_reference, axis=2)
        else:
            sem_gray = sem_reference

        sem_resized = cv2.resize(sem_gray, (self.V, self.V))

        # Blend SEM texture with geological structure
        for z in range(self.V):
            blend_factor = 0.6 + 0.2 * np.sin(z * 0.1)  # Varying blend factor
            volume[:, :, z] = (blend_factor * volume[:, :, z] +
                              (1 - blend_factor) * sem_resized)

        return volume

    def _apply_composition_distribution(self, volume, target_composition):
        """Apply composition-aware mineral distribution"""
        # Use thresholds based on composition percentages
        total = sum(target_composition.values())
        cumulative = 0
        thresholds = []

        phases = ['Silicates', 'Carbonate', 'Clay', 'Kerogen', 'Others']
        for phase in phases:
            cumulative += target_composition.get(phase, 0) / total
            thresholds.append(cumulative)

        thresholds = [t * 0.8 + 0.1 for t in thresholds]  # Map to 0.1-0.9 range

        # Apply composition-based intensity mapping
        volume_flat = volume.ravel()
        sorted_idx = np.argsort(volume_flat)

        current_idx = 0
        for i, threshold in enumerate(thresholds):
            phase_count = int(len(volume_flat) * threshold)
            end_idx = min(current_idx + phase_count, len(volume_flat))

            # Assign phase intensities
            phase_intensity = 0.1 + 0.8 * (i / len(thresholds))
            volume_flat[sorted_idx[current_idx:end_idx]] = phase_intensity
            current_idx = end_idx

        return volume_flat.reshape(volume.shape)

    def _geological_refinement(self, volume):
        """Apply final geological refinement"""
        # Gentle 3D smoothing for perfect continuity
        volume = ndimage.gaussian_filter(volume, sigma=(1.2, 1.2, 0.8))

        # Enhance contrasts slightly
        volume = np.clip(volume * 1.1 - 0.05, 0, 1)

        return volume
class MultiScaleVolumeAssembler:
    """
    Assemble a cubic volume from 2D patches across scales with overlap & blending.
    """
    def __init__(self, vol_size=128, patch_size=64, stride=32, smooth_sigma=0.8):
        self.V = int(vol_size)
        self.P = int(patch_size)
        self.S = int(stride)
        self.kernel = gaussian3d_kernel(size=5, sigma=1.0)
        self.sigma = float(smooth_sigma)

    def _augment2d(self, p):
        if np.random.rand()<0.5: p = cv2.flip(p, 1)
        if np.random.rand()<0.5: p = cv2.flip(p, 0)
        k = np.random.choice([0,1,2,3])
        if k: p = np.rot90(p, k)
        return p

    def assemble(self, patch_bank_list):
        V = self.V
        out = np.zeros((V,V,V), np.float32)
        wts = np.zeros_like(out)

        # Improved: Use more slices for better continuity
        n_slices = max(8, V // 16)  # Increased from 2 to 8+
        z_thick  = max(2, V // n_slices)

        for zi in range(n_slices):
            bank = patch_bank_list[zi % len(patch_bank_list)]
            if len(bank) == 0:
                # Create more realistic base pattern
                base = self._create_structured_noise(self.P, self.P)
            else:
                # Select patch with better texture
                base = self._select_best_patch(bank)
            base = normalize01(base.squeeze())
            base = self._augment2d(base)

            # Improved tiling with better blending
            tiled = self._seamless_tile(base, V, V)
            tiled = ndimage.gaussian_filter(tiled, self.sigma * 1.5)  # Increased smoothing

            # Add 3D structure with gradual transitions
            chunk = self._create_3d_chunk(tiled, z_thick)
            if chunk.ndim == 4:
                chunk = np.squeeze(chunk, axis=-1)
            chunk = ndimage.convolve(chunk, self.kernel, mode='nearest', axes=(0,1,2))
# =========================
# ADD THIS METHOD TO THE MultiScaleVolumeAssembler CLASS
# =========================

    def assemble_continuous(self, patch_bank_list):
        """Continuous assembly method that was missing"""
        V = self.V
        out = np.zeros((V,V,V), np.float32)
        wts = np.zeros_like(out)

        # Use more slices for better continuity
        n_slices = 12
        z_thick = max(2, V // n_slices)

        for zi in range(n_slices):
            bank = patch_bank_list[zi % len(patch_bank_list)]
            if len(bank) == 0:
                base = self._create_structured_noise(self.P, self.P)
            else:
                base = self._select_best_patch(bank)

            base = normalize01(base.squeeze())
            if base.ndim == 3:
                base = np.mean(base, axis=2)

            base = self._augment2d(base)

            # Use the seamless tiling method
            tiled = self._seamless_tile_continuous(base, V, V)
            tiled = ndimage.gaussian_filter(tiled, self.sigma)

            # Create 3D chunk with strong continuity
            chunk = self._create_continuous_3d_chunk(tiled, z_thick)
            if chunk.ndim == 4:
                chunk = np.squeeze(chunk, axis=-1)

            # Apply gentle 3D coherence filtering
            chunk = ndimage.gaussian_filter(chunk, sigma=(0.5, 0.5, 0.3))

            # Safe Z-write
            z0 = zi * z_thick
            if z0 >= V:
                break
            z_len = min(chunk.shape[2], V - z0)
            h_len = min(chunk.shape[0], V)
            w_len = min(chunk.shape[1], V)

            out[:h_len, :w_len, z0:z0+z_len] += chunk[:h_len, :w_len, :z_len]
            wts[:h_len, :w_len, z0:z0+z_len] += 1.0

        out = np.divide(out, np.maximum(wts, 1e-6))
        return clip01(out)

    # Also add this helper method if it doesn't exist
    def _seamless_tile_continuous(self, patch, target_h, target_w):
        """Enhanced seamless tiling for continuous volumes"""
        if patch.ndim == 3:
            patch = np.mean(patch, axis=2)

        # Use more sophisticated tiling with pyramid blending
        h_rep = int(np.ceil(target_h / patch.shape[0])) + 2
        w_rep = int(np.ceil(target_w / patch.shape[1])) + 2

        # Create larger tile and then crop to avoid edge effects
        large_tiled = np.tile(patch, (h_rep, w_rep))

        # Apply multi-scale Gaussian blur for seamless blending
        tiled_smooth = ndimage.gaussian_filter(large_tiled, sigma=1.5)

        # Crop to target size from center
        center_y = large_tiled.shape[0] // 2 - target_h // 2
        center_x = large_tiled.shape[1] // 2 - target_w // 2

        result = large_tiled[center_y:center_y+target_h, center_x:center_x+target_w]

        return result

    # Add this method to create continuous 3D chunks
    def _create_continuous_3d_chunk(self, slice_2d, depth, correlation=0.95):
        """Create 3D chunk with very strong depth correlation"""
        if slice_2d.ndim == 3:
            if slice_2d.shape[2] <= 3:
                slice_2d = np.mean(slice_2d, axis=2)
            else:
                slice_2d = slice_2d[:, :, 0]

        # Create 3D chunk with very strong correlation
        chunk = np.repeat(slice_2d[..., None], depth, axis=2)

        # Very gradual depth variations
        z_profile = np.linspace(0.98, 1.02, depth)  # Minimal variation

        # Apply strong correlation between slices
        for z in range(1, depth):
            chunk[:, :, z] = (correlation * chunk[:, :, z-1] +
                             (1-correlation) * chunk[:, :, z]) * z_profile[z]

        return np.clip(chunk, 0, 1)
    # ============== REPLACE assemble_continuous METHOD ==============
    def assemble_continuous_enhanced(self, patch_bank_list, target_composition=None):
        """Enhanced continuous assembly with geological constraints"""
        V = self.V
        out = np.zeros((V,V,V), np.float32)
        wts = np.zeros_like(out)

        # Use geological layering with composition guidance
        n_layers = 20  # More layers for better continuity
        z_thick = max(2, V // n_layers)

        # Create base geological profile
        base_profile = self._create_geological_profile(V, target_composition)

        for zi in range(n_layers):
            bank = patch_bank_list[zi % len(patch_bank_list)]
            if len(bank) == 0:
                base = self._create_geological_texture_enhanced(self.P, self.P, target_composition)
            else:
                base = self._select_composition_aware_patch(bank, target_composition)

            base = normalize01(base.squeeze())
            if base.ndim == 3:
                base = np.mean(base, axis=2)

            # Apply geological constraints
            tiled = self._geological_constrained_tile(base, V, V, target_composition)

            # Create 3D layer with perfect geological continuity
            chunk = self._create_geological_layer_enhanced(tiled, z_thick, base_profile[zi])

            if chunk.ndim == 4:
                chunk = np.squeeze(chunk, axis=-1)

            # Geological smoothing
            chunk = ndimage.gaussian_filter(chunk, sigma=(0.6, 0.6, 0.4))

            # Safe Z-write
            z0 = zi * z_thick
            if z0 >= V: break
            z_len = min(chunk.shape[2], V - z0)
            h_len = min(chunk.shape[0], V)
            w_len = min(chunk.shape[1], V)

            out[:h_len, :w_len, z0:z0+z_len] += chunk[:h_len, :w_len, :z_len]
            wts[:h_len, :w_len, z0:z0+z_len] += 1.0

        out = np.divide(out, np.maximum(wts, 1e-6))

        # Enhanced geological coherence
        out = self._apply_geological_coherence(out, target_composition)

        return clip01(out)

    def _create_geological_profile(self, depth, target_composition):
        """Create geological depth profile based on composition"""
        profile = np.ones(depth)

        if target_composition:
            # More clay = more layered, more silicates = more homogeneous
            clay_ratio = target_composition.get('Clay', 10) / 100.0
            silicate_ratio = target_composition.get('Silicates', 70) / 100.0

            # Create bedding based on clay content
            if clay_ratio > 0.15:
                for i in range(depth):
                    # Stronger bedding for clay-rich shales
                    profile[i] = 0.8 + 0.4 * np.sin(i * 0.2) * clay_ratio
            else:
                # More homogeneous for silicate-rich shales
                profile = np.ones(depth) * (0.9 + 0.1 * silicate_ratio)

        return profile

    def _create_geological_texture_enhanced(self, h, w, target_composition):
        """Create composition-aware geological texture"""
        # Base sedimentary pattern
        x, y = np.meshgrid(np.linspace(0, 6*np.pi, w), np.linspace(0, 6*np.pi, h))

        # Adjust pattern based on composition
        clay_content = target_composition.get('Clay', 10) / 100.0 if target_composition else 0.1

        base = (np.sin(0.3*x) * np.cos(0.3*y) * 0.5 +
                np.sin(0.8*x + 1) * np.cos(0.8*y - 1) * 0.3 * clay_content +
                np.sin(1.5*x) * np.cos(1.5*y) * 0.2 +
                np.random.rand(h, w) * 0.3)

        # Add fractures and cracks for natural appearance
        if clay_content > 0.2:
            base = self._add_natural_fractures(base, intensity=clay_content)

        return normalize01(base)

    def _select_geological_patch(self, bank):
        """Select patch with geological characteristics"""
        if len(bank) == 0:
            return self._create_geological_texture(self.P, self.P)

        # Score patches based on geological features (variance, layering)
        scores = []
        for patch in bank:
            if patch.ndim > 2:
                patch = np.mean(patch, axis=-1)

            # Calculate geological score (high variance + horizontal features)
            variance = np.var(patch)

            # Detect horizontal features (bedding)
            horizontal_grad = np.abs(np.gradient(patch, axis=0))
            horizontal_score = np.mean(horizontal_grad)

            scores.append(variance * horizontal_score)

        return bank[np.argmax(scores)]

    def _geological_tile(self, patch, target_h, target_w):
        """Tile with geological continuity"""
        if patch.ndim == 3:
            patch = np.mean(patch, axis=2)

        # Create larger tile with geological patterns
        h_rep = int(np.ceil(target_h / patch.shape[0])) + 3
        w_rep = int(np.ceil(target_w / patch.shape[1])) + 3

        large_tiled = np.tile(patch, (h_rep, w_rep))

        # Apply geological blending
        tiled_smooth = ndimage.gaussian_filter(large_tiled, sigma=2.0)

        # Crop to target size from geologically interesting area
        center_y = large_tiled.shape[0] // 2 - target_h // 2
        center_x = large_tiled.shape[1] // 2 - target_w // 2

        result = large_tiled[center_y:center_y+target_h, center_x:center_x+target_w]

        return result

    def _create_geological_layer(self, slice_2d, depth, correlation=0.95):
        """Create geological layer with strong depth correlation"""
        if slice_2d.ndim == 3:
            if slice_2d.shape[2] <= 3:
                slice_2d = np.mean(slice_2d, axis=2)
            else:
                slice_2d = slice_2d[:, :, 0]

        # Create 3D layer with very strong geological correlation
        chunk = np.repeat(slice_2d[..., None], depth, axis=2)

        # Add subtle geological variations
        z_variation = np.linspace(0.97, 1.03, depth)

        # Apply strong geological correlation
        for z in range(1, depth):
            chunk[:, :, z] = (correlation * chunk[:, :, z-1] +
                             (1-correlation) * chunk[:, :, z]) * z_variation[z]

        return np.clip(chunk, 0, 1)
    def _seamless_tile_continuous(self, patch, target_h, target_w):
        """Enhanced seamless tiling for continuous volumes"""
        if patch.ndim == 3:
            patch = np.mean(patch, axis=2)

        # Use more sophisticated tiling with pyramid blending
        h_rep = int(np.ceil(target_h / patch.shape[0])) + 2
        w_rep = int(np.ceil(target_w / patch.shape[1])) + 2

        # Create larger tile and then crop to avoid edge effects
        large_tiled = np.tile(patch, (h_rep, w_rep))

        # Apply multi-scale Gaussian blur for seamless blending
        tiled_smooth = ndimage.gaussian_filter(large_tiled, sigma=1.5)

        # Crop to target size from center
        center_y = large_tiled.shape[0] // 2 - target_h // 2
        center_x = large_tiled.shape[1] // 2 - target_w // 2

        result = large_tiled[center_y:center_y+target_h, center_x:center_x+target_w]

        return result

    def _create_continuous_3d_chunk(self, slice_2d, depth, correlation=0.9):
        """Create 3D chunk with very strong depth correlation"""
        if slice_2d.ndim == 3:
            if slice_2d.shape[2] <= 3:
                slice_2d = np.mean(slice_2d, axis=2)
            else:
                slice_2d = slice_2d[:, :, 0]

        # Create 3D chunk with very strong correlation
        chunk = np.repeat(slice_2d[..., None], depth, axis=2)

        # Very gradual depth variations
        z_profile = np.linspace(0.98, 1.02, depth)  # Minimal variation

        # Apply strong correlation between slices
        for z in range(1, depth):
            chunk[:, :, z] = (correlation * chunk[:, :, z-1] +
                             (1-correlation) * chunk[:, :, z]) * z_profile[z]

        return np.clip(chunk, 0, 1)
    def _create_structured_noise(self, h, w):
        """Create structured noise instead of random noise"""
        x, y = np.meshgrid(np.linspace(0, 4*np.pi, w), np.linspace(0, 4*np.pi, h))
        noise = (np.sin(x) * np.cos(y) +
                np.sin(2*x + 1) * np.cos(2*y - 1) * 0.5 +
                np.random.rand(h, w) * 0.3)
        return normalize01(noise)

    def _select_best_patch(self, bank):
        """Select patch with highest texture quality"""
        if len(bank) == 0:
            return self._create_structured_noise(self.P, self.P)

        # Calculate texture score (variance + entropy)
        scores = []
        for patch in bank:
            if patch.ndim > 2:
                patch = np.mean(patch, axis=-1)
            variance = np.var(patch)
            # Simple entropy approximation
            hist = np.histogram(patch, bins=32)[0]
            hist = hist[hist > 0]
            entropy = -np.sum(hist * np.log(hist))
            scores.append(variance * entropy)

        return bank[np.argmax(scores)]

    def _seamless_tile(self, patch, target_h, target_w):
        """Create seamless tiling with simple replication and smoothing"""
        # Ensure patch is 2D
        if patch.ndim == 3:
            patch = np.mean(patch, axis=2)  # Convert to grayscale if multi-channel

        # Simple tiling without complex blending
        h_rep = int(np.ceil(target_h / patch.shape[0])) + 1
        w_rep = int(np.ceil(target_w / patch.shape[1])) + 1
        tiled = np.tile(patch, (h_rep, w_rep))[:target_h, :target_w]

        # Apply gentle Gaussian blur to reduce visible seams
        tiled = ndimage.gaussian_filter(tiled, sigma=0.8)

        return tiled

    def _create_3d_chunk(self, slice_2d, depth):
        """Create 3D chunk with gradual depth variation as in paper"""
        # Ensure slice_2d is 2D (H, W) not 3D (H, W, C)
        if slice_2d.ndim == 3:
            # If it's 3D, take only the first channel or convert to grayscale
            if slice_2d.shape[2] <= 3:  # RGB or multi-channel
                slice_2d = np.mean(slice_2d, axis=2)  # Convert to grayscale
            else:
                slice_2d = slice_2d[:, :, 0]  # Take first channel

        # Create 3D chunk by repeating the 2D slice with depth correlation
        chunk = np.repeat(slice_2d[..., None], depth, axis=2)

        # Add structured depth variations (not random noise)
        z_profile = np.linspace(0.95, 1.05, depth)
        depth_correlation = 0.8  # Strong correlation between adjacent slices

        for z in range(1, depth):
            # Each slice is strongly correlated with previous slice
            chunk[:, :, z] = (depth_correlation * chunk[:, :, z-1] +
                             (1-depth_correlation) * chunk[:, :, z]) * z_profile[z]

        return np.clip(chunk, 0, 1)

    def assemble(self, patch_bank_list):
        V = self.V
        out = np.zeros((V,V,V), np.float32)
        wts = np.zeros_like(out)

        # Use more slices for better continuity (as in paper)
        n_slices = 12  # Fixed number for consistent results
        z_thick  = max(2, V // n_slices)

        for zi in range(n_slices):
            bank = patch_bank_list[zi % len(patch_bank_list)]
            if len(bank) == 0:
                base = self._create_structured_noise(self.P, self.P)
            else:
                base = self._select_best_patch(bank)

            # Ensure base is 2D and normalized
            base = normalize01(base.squeeze())
            if base.ndim == 3:
                base = np.mean(base, axis=2)  # Convert to grayscale

            base = self._augment2d(base)

            # Improved tiling with better blending
            tiled = self._seamless_tile(base, V, V)
            tiled = ndimage.gaussian_filter(tiled, self.sigma)

            # Add 3D structure with gradual transitions
            chunk = self._create_3d_chunk(tiled, z_thick)
            if chunk.ndim == 4:
                chunk = np.squeeze(chunk, axis=-1)

            # Apply gentle 3D coherence filtering
            chunk = ndimage.gaussian_filter(chunk, sigma=(0.3, 0.3, 0.2))

            # Safe Z-write with proper bounds checking
            z0 = zi * z_thick
            if z0 >= V:
                break
            z_len = min(chunk.shape[2], V - z0)

            # Ensure we don't exceed array bounds
            h_len = min(chunk.shape[0], V)
            w_len = min(chunk.shape[1], V)

            out[:h_len, :w_len, z0:z0+z_len] += chunk[:h_len, :w_len, :z_len]
            wts[:h_len, :w_len, z0:z0+z_len] += 1.0

        out = np.divide(out, np.maximum(wts, 1e-6))
        return clip01(out)

    def make_banks(self, base_img, patch_size=64):
        s1 = normalize01(base_img)
        s2 = cv2.resize(s1, (s1.shape[1]//2, s1.shape[0]//2), interpolation=cv2.INTER_AREA)
        s3 = cv2.resize(s1, (s1.shape[1]//4, s1.shape[0]//4), interpolation=cv2.INTER_AREA)
        b1, _ = patchify_2d(s1, patch_size=patch_size, stride=patch_size//2)
        b2, _ = patchify_2d(s2, patch_size=patch_size, stride=patch_size//2)
        b3, _ = patchify_2d(s3, patch_size=patch_size, stride=patch_size//2)
        return [b1, b2, b3]

# ---------- 3D refiner (ADDED)
def build_refiner3d(input_shape=(128,128,128,1), features=16):
    I = Input(input_shape)
    x1 = Conv3D(features,3,padding='same',activation='relu')(I)
    x1 = Conv3D(features,3,padding='same',activation='relu')(x1)
    p1 = tf.keras.layers.MaxPool3D(2)(x1)
    x2 = Conv3D(features*2,3,padding='same',activation='relu')(p1)
    x2 = Conv3D(features*2,3,padding='same',activation='relu')(x2)
    p2 = tf.keras.layers.MaxPool3D(2)(x2)
    b  = Conv3D(features*4,3,padding='same',activation='relu')(p2)
    b  = Conv3D(features*4,3,padding='same',activation='relu')(b)
    u2 = tf.keras.layers.UpSampling3D(2)(b); u2 = concatenate([u2,x2])
    y2 = Conv3D(features*2,3,padding='same',activation='relu')(u2)
    y2 = Conv3D(features*2,3,padding='same',activation='relu')(y2)
    u1 = tf.keras.layers.UpSampling3D(2)(y2); u1 = concatenate([u1,x1])
    y1 = Conv3D(features,3,padding='same',activation='relu')(u1)
    y1 = Conv3D(features,3,padding='same',activation='relu')(y1)
    O  = Conv3D(1,1,activation='sigmoid')(y1)
    m  = Model(I,O,name="Refiner3D")
    m.compile(optimizer=Adam(1e-4), loss='mse')
    return m

# ---------- Color & export helpers (ADDED)
class ColorAndExportTools:
    def __init__(self, base_path, palette: MineralPalette):
        self.base_path = base_path
        self.palette = palette
        safe_makedirs(os.path.join(base_path, "Output/Colored"))
        safe_makedirs(os.path.join(base_path, "Output/TIFF_Stacks"))

    def save_colored_center_slices(self, label_vol, sample_name):
        outdir = os.path.join(self.base_path, "Output/Colored", sample_name)
        safe_makedirs(outdir)
        rgb = self.palette.colorize_labels(label_vol)
        zc, yc, xc = label_vol.shape[2]//2, label_vol.shape[1]//2, label_vol.shape[0]//2
        plt.imsave(os.path.join(outdir, "slice_Z.png"), rgb[:,:,zc])
        plt.imsave(os.path.join(outdir, "slice_Y.png"), rgb[:,yc,:])
        plt.imsave(os.path.join(outdir, "slice_X.png"), rgb[xc,:,:])
        print(f"âœ… Colored center slices saved: {outdir}")

    def save_tiff_stack_labels(self, label_vol, sample_name):
        outfp = os.path.join(self.base_path, "Output/TIFF_Stacks", f"{sample_name}_labels.tif")
        os.makedirs(os.path.dirname(outfp), exist_ok=True)  # ADD THIS LINE
        print(f"ðŸ’¾ Saving TIFF stack: {outfp}")

    # Enhanced: Ensure we have proper RGB data
        palette = MineralPalette(self.base_path)
        palette.try_extract_from_folder(sample_name)

    # Create proper RGB volume with correct dimensions
        H, W, D = label_vol.shape
        rgb_vol = np.zeros((H, W, D, 3), dtype=np.uint8)

        for phase_id in range(5):
            mask = label_vol == phase_id
            phase_name = palette.I2P[phase_id]
            color = np.array(palette.palette[phase_name]) * 255
            rgb_vol[mask] = color.astype(np.uint8)


    # Ensure the volume has proper contrast and isn't all black
        if np.max(rgb_vol) == 0:
            print("âš ï¸  Warning: RGB volume is all black, applying default colors")
        # Apply default colors if all black
            default_colors = [
                [255, 255, 0],    # Silicates - Yellow
                [0, 255, 0],      # Carbonate - Green
                [255, 0, 0],      # Others - Red
                [128, 0, 128],    # Clay - Purple
                [0, 0, 0]         # Kerogen - Black
            ]
            for phase_id in range(5):
                mask = label_vol == phase_id
                rgb_vol[mask] = default_colors[phase_id]

    # Save as multi-page TIFF with proper metadata
        with tifffile.TiffWriter(outfp, bigtiff=False) as tiff:
            for z in range(D):
            # Ensure each slice is proper RGB
                slice_rgb = rgb_vol[:, :, z, :]
                if slice_rgb.ndim == 2:  # If grayscale, convert to RGB
                    slice_rgb = np.stack([slice_rgb]*3, axis=-1)
                tiff.write(slice_rgb, photometric='rgb',
                        metadata={'minerals': 'Silicates,Carbonate,Clay,Kerogen,Others'})

        print(f"âœ… Color TIFF stack saved: {outfp} (shape: {label_vol.shape})")
        return outfp

def export_rgb_volume_as_tiff(rgb_vol, out_fp):
    vol8 = (clip01(rgb_vol)*255).astype(np.uint8)          # H,W,D,3
    vol_pages = np.moveaxis(vol8, 2, 0)                    # D,H,W,3
    tifffile.imwrite(out_fp, vol_pages)

# ---------- Extra visualizer (ADDED)
class ExtraVisualizer:
    def __init__(self, base_path):
        self.base_path = base_path
        safe_makedirs(os.path.join(base_path, "Output/Visualizations"))

    def orthogrid(self, vol, sample_name, n=6):
        idxs = np.linspace(0, vol.shape[2]-1, n, dtype=int)
        fig, axes = plt.subplots(1, n, figsize=(3*n, 3))
        for i, z in enumerate(idxs):
            axes[i].imshow(vol[:,:,z], cmap='gray'); axes[i].set_title(f"Z={z}")
            axes[i].axis('off')
        out = os.path.join(self.base_path, 'Output/Visualizations', f'{sample_name}_orthogrid.png')
        plt.tight_layout(); plt.savefig(out, dpi=180); plt.close()
        print("âœ… Orthogrid:", out)

    def composition_radar(self, target, achieved, sample_name):
        phases = ['Silicates','Carbonate','Others','Clay','Kerogen']
        values_t = [target[p] for p in phases]
        values_a = [achieved.get(p,0) for p in phases]
        values_t += values_t[:1]; values_a += values_a[:1]
        angles = np.linspace(0, 2*np.pi, len(values_t))
        fig = plt.figure(figsize=(6,6))
        ax = plt.subplot(111, polar=True)
        ax.plot(angles, values_t, linewidth=2, label='Target'); ax.fill(angles, values_t, alpha=0.1)
        ax.plot(angles, values_a, linewidth=2, label='Achieved'); ax.fill(angles, values_a, alpha=0.1)
        ax.set_xticks(angles[:-1]); ax.set_xticklabels(phases)
        ax.set_ylim(0, max(max(values_t), max(values_a)) * 1.1)
        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.2))
        out = os.path.join(self.base_path, 'Output/Visualizations', f'{sample_name}_radar.png')
        plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()
        print("âœ… Radar:", out)
# =========================
# SECTION 3 / 4
# =========================

# --------- Dice + CE loss (ADDED)
def dice_coef(y_true, y_pred, smooth=1.0):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(tf.clip_by_value(y_pred, 1e-7, 1.0), tf.float32)
    axes = tuple(range(1, len(y_pred.shape)-1))
    inter = tf.reduce_sum(y_true*y_pred, axis=axes)
    denom = tf.reduce_sum(y_true+y_pred, axis=axes)
    return tf.reduce_mean((2.*inter+smooth)/(denom+smooth))

def dice_ce_loss(y_true, y_pred, ce_weight=0.5):
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)
    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
    return ce_weight*ce + (1.0-ce_weight)*(1.0 - dice_coef(y_true, y_pred))

# --------- ResidualAttentionUNet (MODIFIED: build_model uses Dice+CE)
class ResidualAttentionUNet:
    """Advanced Residual Attention U-Net for semantic segmentation"""

    def __init__(self, input_size=(64, 64, 1), num_classes=5):
        self.input_size = input_size
        self.num_classes = num_classes
        self.model = self.build_model()

    def residual_block(self, x, filters, kernel_size=3, stride=1):
        shortcut = x
        x = BatchNormalization()(x); x = ReLU()(x)
        x = Conv2D(filters, kernel_size, strides=stride, padding='same')(x)
        x = BatchNormalization()(x); x = ReLU()(x)
        x = Conv2D(filters, kernel_size, strides=1, padding='same')(x)
        if shortcut.shape[-1] != filters or stride != 1:
            shortcut = Conv2D(filters, 1, strides=stride, padding='same')(shortcut)
        x = Add()([x, shortcut])
        return x

    def attention_block(self, x, g, filters):
        theta_x = Conv2D(filters, 1, strides=1, padding='same')(x)
        phi_g   = Conv2D(filters, 1, strides=1, padding='same')(g)
        add     = Add()([theta_x, phi_g])
        relu    = ReLU()(add)
        psi     = Conv2D(1, 1, strides=1, padding='same')(relu)
        sigmoid = Activation('sigmoid')(psi)
        return Multiply()([x, sigmoid])
    def build_resnet18_3d(input_shape=(64, 64, 64, 1)):
        """ResNet-18 3D architecture for equivalent modulus prediction (as in paper)"""
        def residual_block(x, filters, stride=1):
            shortcut = x
            x = Conv3D(filters, 3, strides=stride, padding='same')(x)
            x = BatchNormalization()(x)
            x = ReLU()(x)
            x = Conv3D(filters, 3, strides=1, padding='same')(x)
            x = BatchNormalization()(x)

            if stride != 1 or shortcut.shape[-1] != filters:
                shortcut = Conv3D(filters, 1, strides=stride, padding='same')(shortcut)
                shortcut = BatchNormalization()(shortcut)

            x = Add()([x, shortcut])
            x = ReLU()(x)
            return x

        inputs = Input(input_shape)

    # Initial conv
        x = Conv3D(64, 7, strides=2, padding='same')(inputs)
        x = BatchNormalization()(x)
        x = ReLU()(x)
        x = MaxPooling3D(3, strides=2, padding='same')(x)

    # Residual blocks
        x = residual_block(x, 64)
        x = residual_block(x, 64)
        x = residual_block(x, 128, stride=2)
        x = residual_block(x, 128)
        x = residual_block(x, 256, stride=2)
        x = residual_block(x, 256)
        x = residual_block(x, 512, stride=2)
        x = residual_block(x, 512)

    # Final layers
        x = GlobalAveragePooling3D()(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(1, activation='linear', name='equivalent_modulus')(x)

        model = Model(inputs, outputs, name='ResNet18_3D')
        model.compile(optimizer=Adam(1e-4), loss='mse', metrics=['mae'])
        return model

    def build_model(self):
        inputs = Input(self.input_size)
        e1 = self.residual_block(inputs, 32); p1 = MaxPooling2D(2)(e1)
        e2 = self.residual_block(p1, 64);     p2 = MaxPooling2D(2)(e2)
        e3 = self.residual_block(p2, 128);    p3 = MaxPooling2D(2)(e3)
        b  = self.residual_block(p3, 256)
        d3 = Conv2DTranspose(128, 2, strides=2, padding='same')(b)
        a3 = self.attention_block(e3, d3, 128); d3 = concatenate([d3, a3]); d3 = self.residual_block(d3, 128)
        d2 = Conv2DTranspose(64, 2, strides=2, padding='same')(d3)
        a2 = self.attention_block(e2, d2,  64); d2 = concatenate([d2, a2]); d2 = self.residual_block(d2,  64)
        d1 = Conv2DTranspose(32, 2, strides=2, padding='same')(d2)
        a1 = self.attention_block(e1, d1,  32); d1 = concatenate([d1, a1]); d1 = self.residual_block(d1,  32)
        outputs = Conv2D(self.num_classes, 1, activation='softmax')(d1)
        model = Model(inputs, outputs, name='ResidualAttentionUNet')
        model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_ce_loss, metrics=['accuracy', dice_coef])
        print("âœ… Residual Attention U-Net (Dice+CE) built successfully")
        return model

    def train_fast(self, patches, epochs=60, batch_size=8):
        print("ðŸš€ Training Residual Attention U-Net with enhanced parameters...")
        patches_gray = np.mean(patches, axis=-1) if patches.ndim == 4 else patches
        labels = self.create_synthetic_labels(patches_gray)
        from tensorflow.keras.utils import to_categorical
        labels_categorical = to_categorical(labels, self.num_classes)
        patches_expanded = np.expand_dims(patches_gray, axis=-1) if patches_gray.ndim == 3 else patches_gray

        # Enhanced training with callbacks
        callbacks = [
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
            ),
            tf.keras.callbacks.EarlyStopping(
                monitor='val_dice_coef', patience=10, restore_best_weights=True, mode='max'
            )
        ]

        history = self.model.fit(
            patches_expanded, labels_categorical,
            batch_size=batch_size,
            epochs=epochs,
            validation_split=0.2,
            verbose=1,
            callbacks=callbacks
        )
        print("âœ… U-Net training completed")
        return history
    def train_fast_enhanced(self, patches, target_composition=None, epochs=40, batch_size=8):
        """Enhanced training with composition guidance"""
        print("ðŸš€ Training Enhanced Residual Attention U-Net with composition awareness...")

        patches_gray = np.mean(patches, axis=-1) if patches.ndim == 4 else patches
        labels = self.create_synthetic_labels(patches_gray)

        from tensorflow.keras.utils import to_categorical
        labels_categorical = to_categorical(labels, self.num_classes)
        patches_expanded = np.expand_dims(patches_gray, axis=-1) if patches_gray.ndim == 3 else patches_gray

        # Enhanced training with composition-aware callbacks
        callbacks = [
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
            ),
            tf.keras.callbacks.EarlyStopping(
                monitor='val_dice_coef', patience=10, restore_best_weights=True, mode='max'
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'best_unet_model.keras',
                monitor='val_dice_coef',
                save_best_only=True,
                mode='max',
                verbose=1
            )
        ]

        # Enhanced training with composition guidance
        if target_composition:
            print(f"ðŸŽ¯ Using composition guidance: {target_composition}")
            # Adjust training parameters based on composition
            clay_ratio = target_composition.get('Clay', 10) / 100.0
            if clay_ratio > 0.2:
                # More epochs for clay-rich samples (more complex textures)
                epochs = max(epochs, 50)
                print("ðŸ”„ Increasing epochs for clay-rich sample")

        history = self.model.fit(
            patches_expanded,
            labels_categorical,
            batch_size=batch_size,
            epochs=epochs,
            validation_split=0.2,
            verbose=1,
            callbacks=callbacks
        )
        print("âœ… Enhanced U-Net training completed")
        return history
    def create_synthetic_labels(self, patches):
        labels = np.zeros((patches.shape[0], patches.shape[1], patches.shape[2]), dtype=np.int32)
        for i, patch in enumerate(patches):
            thresholds = np.percentile(patch, [20, 40, 60, 80])
            labels[i][patch < thresholds[0]] = 4
            labels[i][(patch >= thresholds[0]) & (patch < thresholds[1])] = 3
            labels[i][(patch >= thresholds[1]) & (patch < thresholds[2])] = 2
            labels[i][(patch >= thresholds[2]) & (patch < thresholds[3])] = 1
            labels[i][patch >= thresholds[3]] = 0
        return labels

    def segment_patches(self, patches):
        patches_gray = np.mean(patches, axis=-1) if patches.ndim == 4 and patches.shape[-1] == 3 else patches
        patches_expanded = np.expand_dims(patches_gray, axis=-1)
        predictions = self.model.predict(patches_expanded, verbose=0)
        return np.argmax(predictions, axis=-1)

# =========================
# WORKING AdvancedSliceGAN3D CLASS - TENSORFLOW OPERATIONS
# =========================
# =========================
# MEMORY-OPTIMIZED AdvancedSliceGAN3D CLASS
# =========================

class AdvancedSliceGAN3D:
    """Memory-optimized 3D SliceGAN with geological realism"""
    gc.collect()
    def __init__(self, latent_dim=128, volume_size=64):  # Reduced dimensions
        self.latent_dim = latent_dim
        self.volume_size = volume_size
        self.generator = self.build_memory_efficient_generator()
        self.discriminators = self.build_lightweight_discriminators()
        print("ðŸ¤– Memory-optimized SliceGAN 3D Initialized")

    def build_memory_efficient_generator(self):
        """Memory-efficient generator"""
        model = Sequential([
            Dense(4*4*4*128, input_dim=self.latent_dim),  # Reduced dimensions
            Reshape((4, 4, 4, 128)),
            BatchNormalization(),
            ReLU(),

            Conv3DTranspose(64, 4, strides=2, padding='same'),
            BatchNormalization(),
            ReLU(),

            Conv3DTranspose(32, 4, strides=2, padding='same'),
            BatchNormalization(),
            ReLU(),

            Conv3DTranspose(16, 4, strides=2, padding='same'),
            BatchNormalization(),
            ReLU(),

            Conv3DTranspose(8, 4, strides=2, padding='same'),
            BatchNormalization(),
            ReLU(),

            Conv3DTranspose(1, 3, padding='same', activation='tanh')
        ], name='MemoryEfficientGenerator')
        return model

    def build_lightweight_discriminators(self):
        """Lightweight discriminators"""
        def build_discriminator(name_suffix):
            model = Sequential([
                Conv2D(32, 4, strides=2, padding='same', input_shape=(self.volume_size, self.volume_size, 1)),
                LeakyReLU(0.2),

                Conv2D(64, 4, strides=2, padding='same'),
                BatchNormalization(),
                LeakyReLU(0.2),

                Conv2D(128, 4, strides=2, padding='same'),
                BatchNormalization(),
                LeakyReLU(0.2),

                Flatten(),
                Dense(1, activation='sigmoid')
            ], name=f'Discriminator_{name_suffix}')

            model.compile(
                optimizer=Adam(learning_rate=0.0002, beta_1=0.5),
                loss='binary_crossentropy'
            )
            return model

        return {
            'xy': build_discriminator('XY'),
            'xz': build_discriminator('XZ'),
            'yz': build_discriminator('YZ')
        }

    def train_with_memory_management(self, real_patches, target_composition=None, epochs=8, batch_size=4):
        """Training with memory management"""
        print(f"ðŸš€ Training SliceGAN for {epochs} epochs (memory optimized)...")

        # Clear memory
        import gc
        gc.collect()

        # Prepare training data
        real_slices = self.prepare_training_data(real_patches)

        d_losses = {'xy': [], 'xz': [], 'yz': []}
        g_losses = []

        for epoch in range(epochs):
            epoch_d_losses = []

            # Train each discriminator
            for axis_name, discriminator in self.discriminators.items():
                # Get real slices
                real_batch = self.get_real_slices(real_slices, batch_size)

                # Generate fake data
                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
                fake_volumes = self.generator.predict(noise, verbose=0, batch_size=2)  # Smaller batch for prediction
                fake_slices = self.get_fake_slices_numpy(fake_volumes, axis_name)

                # Train discriminator
                d_loss_real = discriminator.train_on_batch(real_batch, np.ones((batch_size, 1)))
                d_loss_fake = discriminator.train_on_batch(fake_slices, np.zeros((batch_size, 1)))

                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
                d_losses[axis_name].append(d_loss)
                epoch_d_losses.append(d_loss)

                # Clear memory after each discriminator
                del fake_volumes, fake_slices
                gc.collect()

            # Train generator
            g_loss = self._train_generator_memory_safe(batch_size)
            g_losses.append(g_loss)

            # Print progress
            avg_d_loss = np.mean(epoch_d_losses)
            print(f"ðŸ“Š Epoch {epoch+1}/{epochs} | G Loss: {g_loss:.4f} | D Loss: {avg_d_loss:.4f}")

            # Clear memory after epoch
            gc.collect()

        print("âœ… SliceGAN training completed")
        return d_losses, g_losses

    def _train_generator_memory_safe(self, batch_size):
        """Memory-safe generator training"""
        # Generate noise
        noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
        valid = np.ones((batch_size, 1))

        total_loss = 0
        trained_axes = 0

        # Train generator for each axis separately to save memory
        for axis_name, discriminator in self.discriminators.items():
            try:
                # Freeze discriminator
                discriminator.trainable = False

                # Create minimal GAN model for this axis
                gan_input = Input(shape=(self.latent_dim,))
                generated_volume = self.generator(gan_input)

                # Extract slices using TensorFlow operations
                if axis_name == 'xy':
                    slices = Lambda(lambda x: tf.reduce_mean(x, axis=1))(generated_volume)
                elif axis_name == 'xz':
                    slices = Lambda(lambda x: tf.reduce_mean(x, axis=2))(generated_volume)
                else:  # yz
                    slices = Lambda(lambda x: tf.reduce_mean(x, axis=3))(generated_volume)

                slices = Reshape((self.volume_size, self.volume_size, 1))(slices)
                validity = discriminator(slices)

                temp_gan = Model(gan_input, validity)
                temp_gan.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),
                               loss='binary_crossentropy')

                # Train generator
                history = temp_gan.train_on_batch(noise, valid)
                total_loss += history
                trained_axes += 1

                # Clean up
                del temp_gan, slices, validity, generated_volume

            except Exception as e:
                print(f"âš ï¸  Warning: Generator training for {axis_name} failed: {e}")
                continue
            finally:
                # Always unfreeze discriminator
                discriminator.trainable = True
                import gc
                gc.collect()

        return total_loss / max(trained_axes, 1)

    def prepare_training_data(self, real_patches):
        """Prepare training data with memory optimization"""
        if real_patches.ndim == 4 and real_patches.shape[-1] > 1:
            patches_gray = np.mean(real_patches, axis=-1)
        else:
            patches_gray = real_patches.squeeze()

        patches_gray = np.expand_dims(patches_gray, axis=-1) if patches_gray.ndim == 3 else patches_gray
        patches_gray = normalize01(patches_gray)

        # Resize to smaller size for memory efficiency
        target_size = self.volume_size
        if patches_gray.shape[1] != target_size:
            resized_patches = np.zeros((patches_gray.shape[0], target_size, target_size, 1), dtype=np.float32)
            for i in range(patches_gray.shape[0]):
                resized_patches[i, :, :, 0] = cv2.resize(patches_gray[i, :, :, 0],
                                                         (target_size, target_size))
            return resized_patches

        return patches_gray.astype(np.float32)

    def get_real_slices(self, real_slices, batch_size):
        """Get real slices for training"""
        idx = np.random.randint(0, real_slices.shape[0], batch_size)
        return real_slices[idx]

    def get_fake_slices_numpy(self, fake_volume, axis_name):
        """Extract slices using NumPy"""
        if axis_name == 'xy':
            slices = np.mean(fake_volume, axis=1)
        elif axis_name == 'xz':
            slices = np.mean(fake_volume, axis=2)
        else:  # yz
            slices = np.mean(fake_volume, axis=3)

        return slices.reshape(fake_volume.shape[0], self.volume_size, self.volume_size, 1)

    def generate_geological_volumes(self, num_samples=6, target_composition=None):
        """Generate volumes with memory management"""
        print(f"ðŸŽ¨ Generating {num_samples} geological volumes...")

        # Generate in smaller batches
        batch_size = 2
        processed_volumes = []

        for i in range(0, num_samples, batch_size):
            current_batch = min(batch_size, num_samples - i)
            noise = np.random.normal(0, 1, (current_batch, self.latent_dim))

            volumes = self.generator.predict(noise, verbose=0, batch_size=1)  # Small batch for prediction
            volumes = volumes[:, :, :, :, 0]  # Remove channel dimension

            for volume in volumes:
                enhanced = self.enhance_geological_features(volume, target_composition)
                processed_volumes.append(enhanced)

            print(f"   Generated {min(i + current_batch, num_samples)}/{num_samples} volumes...")

            # Clear memory
            import gc
            gc.collect()

        print("âœ… Geological volumes generated successfully")
        return np.array(processed_volumes)

    def enhance_geological_features(self, volume, target_composition):
        """Enhance geological features"""
        # Simple enhancements to save memory
        for z in range(volume.shape[2]):
            bedding = 0.05 * np.sin(z * 0.2)  # Reduced effect
            volume[:, :, z] = volume[:, :, z] * (0.95 + bedding)

        return np.clip(volume, 0, 1)
# --------- Composition-guided selector (kept)
class CompositionGuidedSelector:
    def __init__(self, mineral_processor):
        self.mineral_processor = mineral_processor

    def select_best_volume(self, volumes, sample_name, target_composition):
        print("ðŸ† Selecting best volume based on composition matching...")
        best_volume, best_score, best_composition = None, -1, None
        for i, volume in enumerate(volumes):
            volume_comp = self.analyze_volume_composition(volume)
            score = self.calculate_composition_similarity(volume_comp, target_composition)
            if score > best_score:
                best_score = score; best_volume = volume; best_composition = volume_comp
        print(f"âœ… Best volume selected with similarity score: {best_score:.4f}")
        print(f"ðŸ“Š Target composition: {target_composition}")
        print(f"ðŸ“Š Best volume composition: {best_composition}")
        return best_volume, best_score, best_composition

    def analyze_volume_composition(self, volume):
        thresholds = np.percentile(volume, [20, 40, 60, 80])
        composition = {
            'Silicates': np.mean(volume >= thresholds[3]),
            'Carbonate': np.mean((volume >= thresholds[2]) & (volume < thresholds[3])),
            'Others':    np.mean((volume >= thresholds[1]) & (volume < thresholds[2])),
            'Clay':      np.mean((volume >= thresholds[0]) & (volume < thresholds[1])),
            'Kerogen':   np.mean(volume < thresholds[0])
        }
        total = sum(composition.values())
        for phase in composition:
            composition[phase] = (composition[phase] / total) * 100
        return composition

    def calculate_composition_similarity(self, volume_comp, target_comp):
        score = 0
        for phase in target_comp:
            target_val = target_comp[phase]
            volume_val = volume_comp.get(phase, 0)
            phase_similarity = 1 - abs(target_val - volume_val) / 100.0
            score += phase_similarity
        return score / len(target_comp)

# --------- Abaqus exporter (kept)
class AdvancedAbaqusExporter:
    def __init__(self, mineral_processor):
        self.mineral_processor = mineral_processor
        self.base_path = base_path if base_path else r"C:\Users\çº¢ç±³\Desktop\Files"

    def export_to_abaqus(self, volume, sample_name, output_dir):
        print("ðŸ’¾ Exporting to Abaqus INP format...")
        material_volume = self.assign_material_phases(volume)
        nodes, elements = self.create_hexahedral_mesh(material_volume)
        inp_path = os.path.join(output_dir, f"{sample_name}_model.inp")
        self.write_inp_file(inp_path, nodes, elements, material_volume)
        prop_path = os.path.join(output_dir, f"{sample_name}_materials.py")
        self.write_material_properties(prop_path, sample_name)
        print(f"âœ… Abaqus files exported:\n   - INP file: {inp_path}\n   - Material properties: {prop_path}")
        return inp_path, prop_path

    def assign_material_phases(self, volume):
        thresholds = np.percentile(volume, [20, 40, 60, 80])
        mv = np.zeros_like(volume, dtype=np.int32)
        mv[volume < thresholds[0]] = 4
        mv[(volume >= thresholds[0]) & (volume < thresholds[1])] = 3
        mv[(volume >= thresholds[1]) & (volume < thresholds[2])] = 2
        mv[(volume >= thresholds[2]) & (volume < thresholds[3])] = 1
        mv[volume >= thresholds[3]] = 0
        return mv

    def create_hexahedral_mesh(self, material_volume):
        # Smart downsampling for optimal file size and Abaqus compatibility
        target_size = 40  # Balanced size for quality and file size
        original_shape = material_volume.shape

        # Calculate scale factor to reach target size
        max_dim = max(original_shape)
        scale_factor = target_size / max_dim

        new_shape = tuple(max(20, int(s * scale_factor)) for s in original_shape)

        print(f"ðŸ”© Downsampling from {original_shape} to {new_shape} for Abaqus compatibility...")

        # Use order=0 for nearest neighbor to preserve material labels
        material_volume = ndimage.zoom(material_volume,
                                     [new_shape[i]/original_shape[i] for i in range(3)],
                                     order=0)

        nx, ny, nz = material_volume.shape[:3]
        print(f"ðŸ”© Creating mesh with {nx}x{ny}x{nz} elements for Abaqus...")

        nodes = []
        node_id = 1
        node_map = {}

        # Create nodes with proper spacing for Abaqus
        for i in range(nx + 1):
            for j in range(ny + 1):
                for k in range(nz + 1):
                    nodes.append([node_id, float(i), float(j), float(k)])
                    node_map[(i, j, k)] = node_id
                    node_id += 1

        elements = []
        elem_id = 1

        # Create hexahedral elements compatible with Abaqus
        for i in range(nx):
            for j in range(ny):
                for k in range(nz):
                    n1 = node_map[(i, j, k)]
                    n2 = node_map[(i+1, j, k)]
                    n3 = node_map[(i+1, j+1, k)]
                    n4 = node_map[(i, j+1, k)]
                    n5 = node_map[(i, j, k+1)]
                    n6 = node_map[(i+1, j, k+1)]
                    n7 = node_map[(i+1, j+1, k+1)]
                    n8 = node_map[(i, j+1, k+1)]
                    material_id = material_volume[i, j, k] + 1  # Abaqus material IDs start from 1
                    elements.append([elem_id, n1, n2, n3, n4, n5, n6, n7, n8, material_id])
                    elem_id += 1

        print(f"âœ… Mesh created: {len(nodes)} nodes, {len(elements)} elements")
        estimated_size_kb = (len(elements) * 0.2) + (len(nodes) * 0.05)
        print(f"ðŸ“Š Estimated file size: ~{estimated_size_kb:.1f}KB")

        return nodes, elements

    def write_inp_file(self, file_path, nodes, elements, material_volume):
    # Ensure directory exists
        os.makedirs(os.path.dirname(file_path), exist_ok=True)  # ADD THIS LINE
        material_names = ['SILICATES', 'CARBONATE', 'OTHERS', 'CLAY', 'KEROGEN']

        with open(file_path, 'w') as f:
        # Write header
            f.write("*HEADING\n")
            f.write("3D Shale Microstructure - Advanced Mineral Reconstruction\n")
            f.write("** Generated by AdvancedShaleReconstructor3D\n")
            f.write("** Optimized for Abaqus CAE import\n**\n")

            # Write nodes
            f.write("*NODE\n")
            for node in nodes:
                f.write(f"{int(node[0])}, {node[1]:.6f}, {node[2]:.6f}, {node[3]:.6f}\n")

            # Write elements with C3D8R type for reduced integration
            f.write("*ELEMENT, TYPE=C3D8R\n")
            for elem in elements:
                elem_line = f"{int(elem[0])}"
                for node_id in elem[1:9]:
                    elem_line += f", {int(node_id)}"
                f.write(elem_line + "\n")

            # Write element sets for each material
            for mat_id, mat_name in enumerate(material_names):
                f.write(f"*ELSET, ELSET={mat_name}_ELEMENTS\n")
                mat_elements = [e[0] for e in elements if e[-1] == mat_id]

                # Write in chunks of 16 elements per line
                line_elements = []
                for i, elem_id in enumerate(mat_elements):
                    line_elements.append(str(elem_id))
                    if (i + 1) % 16 == 0 or i == len(mat_elements) - 1:
                        f.write(", ".join(line_elements) + "\n")
                        line_elements = []

            # Write material sections
            for mat_id, mat_name in enumerate(material_names):
                f.write(f"*SOLID SECTION, ELSET={mat_name}_ELEMENTS, MATERIAL={mat_name}\n")
                f.write("1.,\n")

            # Write boundary conditions
            f.write("** BOUNDARY CONDITIONS\n")
            f.write("*BOUNDARY\n")

            # Find and write bottom nodes (Z=0)
            bottom_nodes = [node[0] for node in nodes if abs(node[3]) < 1e-6]
            if bottom_nodes:
                # Write bottom nodes in chunks
                f.write("*NSET, NSET=BOTTOM_NODES\n")
                line_nodes = []
                for i, node_id in enumerate(bottom_nodes):
                    line_nodes.append(str(node_id))
                    if (i + 1) % 16 == 0 or i == len(bottom_nodes) - 1:
                        f.write(", ".join(line_nodes) + "\n")
                        line_nodes = []

                f.write("*BOUNDARY\n")
                f.write("BOTTOM_NODES, 3, 3\n")

            f.write("**\n** END OF INPUT FILE\n")

    def write_material_properties(self, file_path, sample_name):
        material_names = ['SILICATES', 'CARBONATE', 'OTHERS', 'CLAY', 'KEROGEN']
        with open(file_path, 'w') as f:
            f.write('"""\nMaterial Properties for Abaqus Analysis\nGenerated by AdvancedShaleReconstructor3D\n"""\n\n')
            f.write('from abaqus import *\nfrom abaqusConstants import *\n\n')
            f.write('def create_materials():\n    """Create material definitions in Abaqus"""\n\n')
            for mat_name in material_names:
                modulus = self.mineral_processor.get_phase_modulus(mat_name)
                density = self.mineral_processor.get_phase_density(mat_name)
                f.write(f'    # {mat_name} material\n')
                f.write(f'    mdb.models["Model-1"].Material(name="{mat_name}_MATERIAL")\n')
                f.write(f'    mdb.models["Model-1"].materials["{mat_name}_MATERIAL"].Elastic(table=(({modulus}, 0.25), ))\n')
                f.write(f'    mdb.models["Model-1"].materials["{mat_name}_MATERIAL"].Density(table=(({density}, ), ))\n')
                f.write(f'    print("Created {mat_name} material: E={modulus} GPa, Ï={density} g/cmÂ³")\n\n')
            f.write('    print("All materials created successfully")\n\n')
            f.write('if __name__ == "__main__":\n    create_materials()\n')
    def export_abaqus_cae_compatible(self, volume, sample_name):
        """Export in CAE-compatible format with small file size (<5MB)"""
        print("ðŸ’¾ Creating CAE-compatible export (<5MB)...")

        abaqus_dir = os.path.join(self.base_path, 'Output/Abaqus')
        os.makedirs(abaqus_dir, exist_ok=True)

    # More aggressive downsampling for small file size
        target_size = 24  # Reduced from 32 to 24 for smaller file
        original_shape = volume.shape
        scale_factor = target_size / max(original_shape)
        new_shape = tuple(max(16, int(s * scale_factor)) for s in original_shape)

        print(f"ðŸ”© Downsampling to {new_shape} for small CAE file...")

        volume_downsampled = ndimage.zoom(volume,
                                    [new_shape[i]/original_shape[i] for i in range(3)],
                                    order=0)  # order=0 for nearest neighbor

    # Assign material phases
        thresholds = np.percentile(volume_downsampled, [20, 40, 60, 80])
        material_volume = np.zeros_like(volume_downsampled, dtype=np.int8)  # Use int8 to save space

        material_volume[volume_downsampled < thresholds[0]] = 5  # Kerogen
        material_volume[(volume_downsampled >= thresholds[0]) & (volume_downsampled < thresholds[1])] = 4  # Clay
        material_volume[(volume_downsampled >= thresholds[1]) & (volume_downsampled < thresholds[2])] = 3  # Others
        material_volume[(volume_downsampled >= thresholds[2]) & (volume_downsampled < thresholds[3])] = 2  # Carbonate
        material_volume[volume_downsampled >= thresholds[3]] = 1  # Silicates

    # Create simplified mesh
        nx, ny, nz = material_volume.shape
        nodes = []
        node_id = 1
        node_map = {}

    # Create nodes with integer coordinates to reduce file size
        for i in range(nx + 1):
            for j in range(ny + 1):
                for k in range(nz + 1):
                    nodes.append([node_id, i, j, k])  # Integer coordinates
                    node_map[(i, j, k)] = node_id
                    node_id += 1

        elements = []
        elem_id = 1

    # Create hexahedral elements
        for i in range(nx):
            for j in range(ny):
                for k in range(nz):
                    n1 = node_map[(i, j, k)]
                    n2 = node_map[(i+1, j, k)]
                    n3 = node_map[(i+1, j+1, k)]
                    n4 = node_map[(i, j+1, k)]
                    n5 = node_map[(i, j, k+1)]
                    n6 = node_map[(i+1, j, k+1)]
                    n7 = node_map[(i+1, j+1, k+1)]
                    n8 = node_map[(i, j+1, k+1)]
                    material_id = material_volume[i, j, k]
                    elements.append([elem_id, n1, n2, n3, n4, n5, n6, n7, n8, material_id])
                    elem_id += 1

    # Write optimized CAE-compatible INP file
        cae_path = os.path.join(abaqus_dir, f"{sample_name}_cae_small.inp")
        self.write_optimized_cae_inp(cae_path, nodes, elements, material_volume)

    # Check file size
        file_size_mb = os.path.getsize(cae_path) / (1024 * 1024)
        print(f"âœ… CAE file saved: {cae_path} ({file_size_mb:.1f} MB)")

        if file_size_mb > 5:
            print("âš ï¸  File size exceeds 5MB, creating even smaller version...")
            return self.create_extra_small_cae(volume, sample_name)

        return cae_path

    def write_optimized_cae_inp(self, file_path, nodes, elements, material_volume):
        """Write optimized INP file for CAE with proper formatting"""
        material_names = ['SILICATES', 'CARBONATE', 'OTHERS', 'CLAY', 'KEROGEN']

        with open(file_path, 'w') as f:
            # Write header with proper formatting
            f.write("*HEADING\n")
            f.write("3D Shale Microstructure - Natural Mineral Distribution\n")
            f.write("** Generated by AdvancedShaleReconstructor3D\n")
            f.write("** Optimized for Abaqus CAE import\n")
            f.write("**\n")

            # Write nodes section
            f.write("*NODE\n")
            for node in nodes:
                f.write(f"{node[0]}, {node[1]}, {node[2]}, {node[3]}\n")

            # Write elements with proper element type
            f.write("*ELEMENT, TYPE=C3D8R\n")
            for elem in elements:
                elem_line = f"{elem[0]}, {elem[1]}, {elem[2]}, {elem[3]}, {elem[4]}, {elem[5]}, {elem[6]}, {elem[7]}, {elem[8]}"
                f.write(elem_line + "\n")

            # Write element sets with proper formatting
            for mat_id, mat_name in enumerate(material_names):
                actual_mat_id = mat_id + 1
                f.write(f"*ELSET, ELSET={mat_name}_ELEMENTS\n")
                mat_elements = [str(e[0]) for e in elements if e[9] == actual_mat_id]

                # Write in chunks of 16 for better readability
                for i in range(0, len(mat_elements), 16):
                    chunk = mat_elements[i:i+16]
                    f.write(", ".join(chunk) + "\n")

            # Write material sections
            for mat_name in material_names:
                f.write(f"*SOLID SECTION, ELSET={mat_name}_ELEMENTS, MATERIAL={mat_name}\n")
                f.write("1.,\n")

            f.write("**END\n")

    def create_extra_small_cae(self, volume, sample_name):
        """Create extra small version (16x16x16) for guaranteed <5MB"""
        print("ðŸ”© Creating extra small version (16x16x16)...")

        abaqus_dir = os.path.join(self.base_path, 'Output/Abaqus')
        os.makedirs(abaqus_dir, exist_ok=True)

    # Force 16x16x16 size
        target_size = (16, 16, 16)
        volume_downsampled = ndimage.zoom(volume,[target_size[i]/volume.shape[i] for i in range(3)],
                                order=0)

    # Assign material phases
        thresholds = np.percentile(volume_downsampled, [20, 40, 60, 80])
        material_volume = np.zeros_like(volume_downsampled, dtype=np.int8)

        material_volume[volume_downsampled < thresholds[0]] = 5  # Kerogen
        material_volume[(volume_downsampled >= thresholds[0]) & (volume_downsampled < thresholds[1])] = 4  # Clay
        material_volume[(volume_downsampled >= thresholds[1]) & (volume_downsampled < thresholds[2])] = 3  # Others
        material_volume[(volume_downsampled >= thresholds[2]) & (volume_downsampled < thresholds[3])] = 2  # Carbonate
        material_volume[volume_downsampled >= thresholds[3]] = 1  # Silicates

    # Create simplified mesh
        nx, ny, nz = material_volume.shape
        nodes = []
        node_id = 1
        node_map = {}

    # Create nodes with integer coordinates to reduce file size
        for i in range(nx + 1):
            for j in range(ny + 1):
                for k in range(nz + 1):
                    nodes.append([node_id, i, j, k])  # Integer coordinates
                    node_map[(i, j, k)] = node_id
                    node_id += 1

        elements = []
        elem_id = 1

    # Create hexahedral elements
        for i in range(nx):
            for j in range(ny):
                for k in range(nz):
                    n1 = node_map[(i, j, k)]
                    n2 = node_map[(i+1, j, k)]
                    n3 = node_map[(i+1, j+1, k)]
                    n4 = node_map[(i, j+1, k)]
                    n5 = node_map[(i, j, k+1)]
                    n6 = node_map[(i+1, j, k+1)]
                    n7 = node_map[(i+1, j+1, k+1)]
                    n8 = node_map[(i, j+1, k+1)]
                    material_id = material_volume[i, j, k]
                    elements.append([elem_id, n1, n2, n3, n4, n5, n6, n7, n8, material_id])
                    elem_id += 1

    # Write optimized CAE-compatible INP file
        cae_path = os.path.join(abaqus_dir, f"{sample_name}_cae_extra_small.inp")
        self.write_optimized_cae_inp(cae_path, nodes, elements, material_volume)  # ADD THIS LINE

        file_size_mb = os.path.getsize(cae_path) / (1024 * 1024)
        print(f"âœ… Extra small CAE file saved: {cae_path} ({file_size_mb:.1f} MB)")
        return cae_path


# --------- ComprehensiveVisualizer (kept)
class ComprehensiveVisualizer:
    """Create comprehensive visualizations of results"""

    def __init__(self, base_path):
        self.base_path = base_path
        self.output_dir = os.path.join(base_path, 'Output')
        os.makedirs(self.output_dir, exist_ok=True)

    def create_3d_volume_visualization(self, volume, sample_name):
        """Create colorful 3D visualization with mineral phase colors"""
        try:
            # Create labeled volume for mineral phases
            thresholds = np.percentile(volume, [20, 40, 60, 80])
            labeled_volume = np.zeros_like(volume, dtype=int)
            labeled_volume[volume < thresholds[0]] = 4  # Kerogen
            labeled_volume[(volume >= thresholds[0]) & (volume < thresholds[1])] = 3  # Clay
            labeled_volume[(volume >= thresholds[1]) & (volume < thresholds[2])] = 2  # Others
            labeled_volume[(volume >= thresholds[2]) & (volume < thresholds[3])] = 1  # Carbonate
            labeled_volume[volume >= thresholds[3]] = 0  # Silicates

            # Create mineral palette
            palette = MineralPalette(self.base_path)
            palette.try_extract_from_folder(sample_name)

            # Create colorful volume
            colorful_volume = np.zeros(volume.shape + (3,), dtype=np.float32)
            for phase_id in range(5):
                mask = labeled_volume == phase_id
                phase_name = palette.I2P[phase_id]
                color = palette.palette[phase_name]
                colorful_volume[mask] = color

            # Create 3 subplots: XY, XZ, YZ slices with colors
            fig = plt.figure(figsize=(18, 6))

            # XY slice (middle)
            ax1 = fig.add_subplot(131)
            xy_slice = colorful_volume[:, :, volume.shape[2]//2]
            ax1.imshow(xy_slice)
            ax1.set_title(f'XY Slice - {sample_name}', fontsize=12, fontweight='bold')
            ax1.set_xlabel('X (voxels)'); ax1.set_ylabel('Y (voxels)')
            ax1.grid(False)

            # XZ slice (middle)
            ax2 = fig.add_subplot(132)
            xz_slice = colorful_volume[:, volume.shape[1]//2, :]
            ax2.imshow(xz_slice)
            ax2.set_title(f'XZ Slice - {sample_name}', fontsize=12, fontweight='bold')
            ax2.set_xlabel('Z (voxels)'); ax2.set_ylabel('X (voxels)')
            ax2.grid(False)

            # YZ slice (middle)
            ax3 = fig.add_subplot(133)
            yz_slice = colorful_volume[volume.shape[0]//2, :, :]
            ax3.imshow(yz_slice)
            ax3.set_title(f'YZ Slice - {sample_name}', fontsize=12, fontweight='bold')
            ax3.set_xlabel('Z (voxels)'); ax3.set_ylabel('Y (voxels)')
            ax3.grid(False)

            # Add legend
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor=palette.palette['Silicates'], label='Silicates (Quartz/Feldspar)'),
                Patch(facecolor=palette.palette['Carbonate'], label='Carbonate (Calcite/Dolomite)'),
                Patch(facecolor=palette.palette['Clay'], label='Clay Minerals'),
                Patch(facecolor=palette.palette['Kerogen'], label='Kerogen/Organic'),
                Patch(facecolor=palette.palette['Others'], label='Others (Pyrite/etc)')
            ]
            fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.05),
                      ncol=3, fontsize=10, framealpha=0.9)

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_3d_mineral_slices.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… Colorful 3D mineral slices saved: {output_path}")

            # Also create a 3D scatter plot for better visualization
            self.create_enhanced_3d_scatter(volume, labeled_volume, sample_name, palette)

        except Exception as e:
            print(f"âš ï¸  Enhanced 3D visualization failed: {e}")
            self.create_3d_scatter_visualization(volume, sample_name)

    def create_enhanced_3d_scatter(self, volume, labeled_volume, sample_name, palette):
        """Create enhanced 3D scatter plot with mineral colors"""
        try:
            # Sample points for visualization (every 4th voxel to reduce density)
            sample_mask = (np.arange(volume.size) % 4 == 0)
            x, y, z = np.meshgrid(np.arange(volume.shape[0]),
                                 np.arange(volume.shape[1]),
                                 np.arange(volume.shape[2]), indexing='ij')

            x_sample = x.ravel()[sample_mask]
            y_sample = y.ravel()[sample_mask]
            z_sample = z.ravel()[sample_mask]
            labels_sample = labeled_volume.ravel()[sample_mask]

            # Create figure
            fig = plt.figure(figsize=(15, 12))
            ax = fig.add_subplot(111, projection='3d')

            # Plot each phase with its color
            for phase_id in range(5):
                phase_mask = labels_sample == phase_id
                if np.sum(phase_mask) > 0:  # Only plot if phase exists
                    phase_name = palette.I2P[phase_id]
                    color = palette.palette[phase_name]
                    ax.scatter(x_sample[phase_mask], y_sample[phase_mask], z_sample[phase_mask],
                              c=[color], s=1, alpha=0.6, label=phase_name, depthshade=False)

            ax.set_xlabel('X (voxels)'); ax.set_ylabel('Y (voxels)'); ax.set_zlabel('Z (voxels)')
            ax.set_title(f'3D Mineral Distribution - {sample_name}\n({volume.shape[0]}Ã—{volume.shape[1]}Ã—{volume.shape[2]} voxels)',
                        fontsize=14, fontweight='bold')

            # Add legend
            ax.legend(loc='upper left', bbox_to_anchor=(0, 1), fontsize=10)

            # Set equal aspect ratio
            max_range = max(volume.shape)
            ax.set_xlim(0, max_range); ax.set_ylim(0, max_range); ax.set_zlim(0, max_range)

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_3d_mineral_scatter.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… Enhanced 3D mineral scatter saved: {output_path}")

        except Exception as e:
            print(f"âš ï¸  Enhanced 3D scatter failed: {e}")
    def create_realistic_shale_visualization(self, volume, sample_name):
        """Create comprehensive visualization of realistic shale model"""
        try:
            # Create mineral-colored volume
            thresholds = np.percentile(volume, [20, 40, 60, 80])
            labeled_volume = np.zeros_like(volume, dtype=int)
            labeled_volume[volume < thresholds[0]] = 4  # Kerogen
            labeled_volume[(volume >= thresholds[0]) & (volume < thresholds[1])] = 3  # Clay
            labeled_volume[(volume >= thresholds[1]) & (volume < thresholds[2])] = 2  # Others
            labeled_volume[(volume >= thresholds[2]) & (volume < thresholds[3])] = 1  # Carbonate
            labeled_volume[volume >= thresholds[3]] = 0  # Silicates

            palette = MineralPalette(self.base_path)
            palette.try_extract_from_folder(sample_name)

            # Create colorful volume
            colorful_volume = np.zeros(volume.shape + (3,), dtype=np.float32)
            for phase_id in range(5):
                mask = labeled_volume == phase_id
                phase_name = palette.I2P[phase_id]
                color = palette.palette[phase_name]
                colorful_volume[mask] = color

            # Create comprehensive visualization
            fig = plt.figure(figsize=(20, 16))

            # 3D subplot
            ax1 = fig.add_subplot(231, projection='3d')
            self._plot_3d_mineral_distribution(ax1, labeled_volume, palette, sample_name)

            # XY slice
            ax2 = fig.add_subplot(232)
            xy_slice = colorful_volume[:, :, volume.shape[2]//2]
            ax2.imshow(xy_slice)
            ax2.set_title('XY Slice - Horizontal Bedding', fontweight='bold')
            ax2.set_xlabel('X'); ax2.set_ylabel('Y')

            # XZ slice
            ax3 = fig.add_subplot(233)
            xz_slice = colorful_volume[:, volume.shape[1]//2, :]
            ax3.imshow(xz_slice)
            ax3.set_title('XZ Slice - Vertical Profile', fontweight='bold')
            ax3.set_xlabel('Z'); ax3.set_ylabel('X')

            # YZ slice
            ax4 = fig.add_subplot(234)
            yz_slice = colorful_volume[volume.shape[0]//2, :, :]
            ax4.imshow(yz_slice)
            ax4.set_title('YZ Slice - Vertical Profile', fontweight='bold')
            ax4.set_xlabel('Z'); ax4.set_ylabel('Y')

            # Original intensity slices
            ax5 = fig.add_subplot(235)
            ax5.imshow(volume[:, :, volume.shape[2]//2], cmap='gray')
            ax5.set_title('Grayscale XY Slice', fontweight='bold')

            # Composition bar chart
            ax6 = fig.add_subplot(236)
            composition = self._calculate_volume_composition(labeled_volume)
            phases = list(composition.keys())
            percentages = list(composition.values())
            bars = ax6.bar(phases, percentages, color=[palette.palette[p] for p in phases])
            ax6.set_title('Mineral Composition', fontweight='bold')
            ax6.set_ylabel('Volume Percentage (%)')
            plt.setp(ax6.xaxis.get_majorticklabels(), rotation=45)

            # Add value labels on bars
            for bar, percentage in zip(bars, percentages):
                height = bar.get_height()
                ax6.text(bar.get_x() + bar.get_width()/2., height,
                        f'{percentage:.1f}%', ha='center', va='bottom', fontweight='bold')

            plt.suptitle(f'Realistic 3D Shale Model - {sample_name}\n'
                        f'Dimensions: {volume.shape[0]}Ã—{volume.shape[1]}Ã—{volume.shape[2]} voxels',
                        fontsize=16, fontweight='bold')

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_realistic_shale_comprehensive.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… Realistic shale visualization saved: {output_path}")

        except Exception as e:
            print(f"âš ï¸  Realistic shale visualization failed: {e}")
            import traceback
            traceback.print_exc()
    def create_enhanced_model_comparison_visualization(self, scattered_vol, continuous_vol, realistic_vol, complete_continuous_vol,
                                                     sample_name, target_comp,
                                                     comp_scattered, comp_continuous, comp_realistic, comp_complete_continuous):
        """Create comprehensive comparison of all four models"""
        try:
            fig = plt.figure(figsize=(24, 18))
            fig.suptitle(f'4-Model Comparison - {sample_name}\nScattered vs Continuous vs Realistic vs Complete Continuous',
                        fontsize=18, fontweight='bold')

            models = ['Scattered', 'Continuous', 'Realistic', 'Complete Continuous']
            volumes = [scattered_vol, continuous_vol, realistic_vol, complete_continuous_vol]
            compositions = [comp_scattered, comp_continuous, comp_realistic, comp_complete_continuous]
            colors = ['blue', 'green', 'red', 'purple']

            # Create 4x4 grid for comprehensive comparison
            for i, (model, vol, comp, color) in enumerate(zip(models, volumes, compositions, colors)):
                row = i

                # XY slice
                ax1 = fig.add_subplot(4, 4, row*4 + 1)
                ax1.imshow(vol[:, :, vol.shape[2]//2], cmap='gray')
                ax1.set_title(f'{model}\nXY Slice', fontweight='bold')
                if row == 0:
                    ax1.set_ylabel('Y voxels')
                if row == 3:
                    ax1.set_xlabel('X voxels')
                similarity = CompositionTools.similarity_score(comp, target_comp)
                ax1.text(0.05, 0.95, f'Sim: {similarity:.3f}',
                        transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                        fontweight='bold')

                # XZ slice
                ax2 = fig.add_subplot(4, 4, row*4 + 2)
                ax2.imshow(vol[:, vol.shape[1]//2, :], cmap='gray')
                ax2.set_title(f'{model}\nXZ Slice', fontweight='bold')
                if row == 0:
                    ax2.set_ylabel('X voxels')
                if row == 3:
                    ax2.set_xlabel('Z voxels')

            # Composition comparison (bottom row)
            ax_comp = fig.add_subplot(4, 4, (13, 14))
            phases = list(target_comp.keys())
            target_vals = [target_comp[p] for p in phases]

            x = np.arange(len(phases))
            width = 0.15

            ax_comp.bar(x - width*1.5, target_vals, width, label='Target', alpha=0.8, color='black')
            for i, (comp, color, model) in enumerate(zip(compositions, colors, models)):
                comp_vals = [comp.get(p, 0) for p in phases]
                ax_comp.bar(x - width*0.5 + i*width, comp_vals, width, label=model, alpha=0.8, color=color)

            ax_comp.set_xlabel('Mineral Phases')
            ax_comp.set_ylabel('Volume Percentage (%)')
            ax_comp.set_title('Composition Comparison', fontweight='bold')
            ax_comp.set_xticks(x)
            ax_comp.set_xticklabels(phases, rotation=45)
            ax_comp.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax_comp.grid(True, alpha=0.3)

            # Quality metrics comparison
            ax_metrics = fig.add_subplot(4, 4, (15, 16))
            metrics_data = []

            for i, (model, vol) in enumerate(zip(models, volumes)):
                intensity_std = np.std(vol)
                moran_i = morans_I(vol)
                porosity = np.mean(vol < 0.5)

                metrics_data.append({
                    'Model': model,
                    'Intensity Std': intensity_std,
                    "Moran's I": moran_i,
                    'Porosity': porosity
                })

            metrics_df = pd.DataFrame(metrics_data)
            x_metrics = np.arange(len(models))
            metrics = ['Intensity Std', "Moran's I", 'Porosity']

            for i, metric in enumerate(metrics):
                values = metrics_df[metric].values
                ax_metrics.bar(x_metrics + i*0.2, values, 0.2, label=metric, alpha=0.8)

            ax_metrics.set_xlabel('Models')
            ax_metrics.set_ylabel('Metric Values')
            ax_metrics.set_title('Quality Metrics Comparison', fontweight='bold')
            ax_metrics.set_xticks(x_metrics + 0.2)
            ax_metrics.set_xticklabels(models, rotation=45)
            ax_metrics.legend()
            ax_metrics.grid(True, alpha=0.3)

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_4model_comparison.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… 4-Model comparison saved: {output_path}")

        except Exception as e:
            print(f"âš ï¸  4-Model comparison visualization failed: {e}")
            import traceback
            traceback.print_exc()

    def _plot_3d_mineral_distribution(self, ax, labeled_volume, palette, sample_name):
        """Create 3D mineral distribution plot"""
        try:
            # Sample every 4th voxel for performance
            sample_mask = (np.arange(labeled_volume.size) % 8 == 0)
            x, y, z = np.meshgrid(np.arange(labeled_volume.shape[0]),
                                 np.arange(labeled_volume.shape[1]),
                                 np.arange(labeled_volume.shape[2]), indexing='ij')

            x_sample = x.ravel()[sample_mask]
            y_sample = y.ravel()[sample_mask]
            z_sample = z.ravel()[sample_mask]
            labels_sample = labeled_volume.ravel()[sample_mask]

            # Plot each mineral phase
            for phase_id in range(5):
                phase_mask = labels_sample == phase_id
                if np.sum(phase_mask) > 0:
                    phase_name = palette.I2P[phase_id]
                    color = palette.palette[phase_name]
                    ax.scatter(x_sample[phase_mask], y_sample[phase_mask], z_sample[phase_mask],
                              c=[color], s=2, alpha=0.6, label=phase_name, depthshade=False)

            ax.set_xlabel('X (voxels)'); ax.set_ylabel('Y (voxels)'); ax.set_zlabel('Z (voxels)')
            ax.set_title('3D Mineral Distribution', fontweight='bold')
            ax.legend(loc='upper left', bbox_to_anchor=(0, 1))

        except Exception as e:
            print(f"âš ï¸  3D mineral distribution plot failed: {e}")
            # Fallback: create a simple 3D plot
            ax.text(0.5, 0.5, 0.5, '3D Visualization\nNot Available',
                    transform=ax.transAxes, ha='center', va='center', fontsize=12)

    def _calculate_volume_composition(self, labeled_volume):
        """Calculate mineral composition from labeled volume"""
        total_voxels = labeled_volume.size
        composition = {}

        for phase_id in range(5):
            phase_name = ['Silicates', 'Carbonate', 'Others', 'Clay', 'Kerogen'][phase_id]
            percentage = (np.sum(labeled_volume == phase_id) / total_voxels) * 100
            composition[phase_name] = percentage

        return composition

    def create_slice_comparison(self, original_patches, generated_volume, sample_name):
        """Create enhanced slice comparison with mineral colors"""
        # Create mineral-colored 3D slices
        thresholds = np.percentile(generated_volume, [20, 40, 60, 80])
        labeled_volume = np.zeros_like(generated_volume, dtype=int)
        labeled_volume[generated_volume < thresholds[0]] = 4
        labeled_volume[(generated_volume >= thresholds[0]) & (generated_volume < thresholds[1])] = 3
        labeled_volume[(generated_volume >= thresholds[1]) & (generated_volume < thresholds[2])] = 2
        labeled_volume[(generated_volume >= thresholds[2]) & (generated_volume < thresholds[3])] = 1
        labeled_volume[generated_volume >= thresholds[3]] = 0

        palette = MineralPalette(self.base_path)
        palette.try_extract_from_folder(sample_name)

        colorful_volume = np.zeros(generated_volume.shape + (3,), dtype=np.float32)
        for phase_id in range(5):
            mask = labeled_volume == phase_id
            phase_name = palette.I2P[phase_id]
            color = palette.palette[phase_name]
            colorful_volume[mask] = color

        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        fig.suptitle(f'2D-3D Mineral Comparison - {sample_name}', fontsize=16, fontweight='bold')

        # Original SEM patches (grayscale)
        for i in range(4):
            axes[0, i].imshow(original_patches[i], cmap='gray')
            axes[0, i].set_title(f'Original SEM {i+1}'); axes[0, i].axis('off')

        # Generated 3D slices (colorful)
        slice_indices = [0, generated_volume.shape[2]//3, 2*generated_volume.shape[2]//3, generated_volume.shape[2]-1]
        for i, idx in enumerate(slice_indices):
            axes[1, i].imshow(colorful_volume[:, :, idx])
            axes[1, i].set_title(f'3D Mineral Slice Z={idx}'); axes[1, i].axis('off')

        # Add color legend
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor=palette.palette['Silicates'], label='Silicates'),
            Patch(facecolor=palette.palette['Carbonate'], label='Carbonate'),
            Patch(facecolor=palette.palette['Clay'], label='Clay'),
            Patch(facecolor=palette.palette['Kerogen'], label='Kerogen'),
            Patch(facecolor=palette.palette['Others'], label='Others')
        ]
        fig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.05),
                  ncol=5, fontsize=12, framealpha=0.9)

        plt.tight_layout()
        output_path = os.path.join(self.output_dir, f'{sample_name}_mineral_comparison.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… Enhanced mineral comparison saved: {output_path}")

    def create_composition_analysis(self, target_comp, volume_comp, sample_name):
        phases = list(target_comp.keys())
        target_vals = [target_comp[p] for p in phases]
        volume_vals = [volume_comp.get(p, 0) for p in phases]
        x = np.arange(len(phases)); width = 0.35
        fig, ax = plt.subplots(figsize=(12, 8))
        bars1 = ax.bar(x - width/2, target_vals, width, label='Target (SEM+Excel)',
                       alpha=0.7, color='navy', edgecolor='black')
        bars2 = ax.bar(x + width/2, volume_vals, width, label='Generated (3D Model)',
                       alpha=0.7, color='crimson', edgecolor='black')
        ax.set_xlabel('Mineral Phases', fontsize=12, fontweight='bold')
        ax.set_ylabel('Volume Percentage (%)', fontsize=12, fontweight='bold')
        ax.set_title(f'Mineral Composition Matching - {sample_name}', fontsize=14, fontweight='bold')
        ax.set_xticks(x); ax.set_xticklabels(phases, rotation=45, ha='right')
        ax.legend(fontsize=11); ax.grid(True, alpha=0.3)
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.annotate(f'{height:.1f}%',
                            xy=(bar.get_x() + bar.get_width() / 2, height),
                            xytext=(0, 3), textcoords="offset points",
                            ha='center', va='bottom', fontsize=9, fontweight='bold')
        similarity = 1 - np.mean(np.abs(np.array(target_vals) - np.array(volume_vals)) / 100.0)
        ax.text(0.02, 0.98, f'Composition Similarity: {similarity:.3f}',
                transform=ax.transAxes, fontsize=12, fontweight='bold',
                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        plt.tight_layout()
        output_path = os.path.join(self.output_dir, f'{sample_name}_composition_analysis.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight'); plt.close()
        print(f"âœ… Composition analysis saved: {output_path}")
        return similarity
    def create_model_comparison_visualization(self, scattered_vol, continuous_vol, realistic_vol,
                                            sample_name, target_comp,
                                            comp_scattered, comp_continuous, comp_realistic):
        """Create comprehensive comparison of all three models"""
        try:
            fig = plt.figure(figsize=(20, 15))
            fig.suptitle(f'3D Model Comparison - {sample_name}\nScattered vs Continuous vs Realistic Approaches',
                        fontsize=16, fontweight='bold')

            # Model 1: Scattered - XY slice
            ax1 = fig.add_subplot(3, 4, 1)
            ax1.imshow(scattered_vol[:, :, scattered_vol.shape[2]//2], cmap='gray')
            ax1.set_title('Scattered Model\nXY Slice', fontweight='bold')
            ax1.set_ylabel('Y voxels')
            ax1.text(0.05, 0.95, f'Similarity: {CompositionTools.similarity_score(comp_scattered, target_comp):.3f}',
                    transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                    fontweight='bold')

            # Model 1: Scattered - XZ slice
            ax2 = fig.add_subplot(3, 4, 2)
            ax2.imshow(scattered_vol[:, scattered_vol.shape[1]//2, :], cmap='gray')
            ax2.set_title('Scattered Model\nXZ Slice', fontweight='bold')
            ax2.set_ylabel('X voxels')

            # Model 2: Continuous - XY slice
            ax3 = fig.add_subplot(3, 4, 5)
            ax3.imshow(continuous_vol[:, :, continuous_vol.shape[2]//2], cmap='gray')
            ax3.set_title('Continuous Model\nXY Slice', fontweight='bold')
            ax3.set_ylabel('Y voxels')
            ax3.text(0.05, 0.95, f'Similarity: {CompositionTools.similarity_score(comp_continuous, target_comp):.3f}',
                    transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                    fontweight='bold')

            # Model 2: Continuous - XZ slice
            ax4 = fig.add_subplot(3, 4, 6)
            ax4.imshow(continuous_vol[:, continuous_vol.shape[1]//2, :], cmap='gray')
            ax4.set_title('Continuous Model\nXZ Slice', fontweight='bold')
            ax4.set_ylabel('X voxels')

            # Model 3: Realistic - XY slice
            ax5 = fig.add_subplot(3, 4, 9)
            ax5.imshow(realistic_vol[:, :, realistic_vol.shape[2]//2], cmap='gray')
            ax5.set_title('Realistic Model\nXY Slice', fontweight='bold')
            ax5.set_ylabel('Y voxels')
            ax5.set_xlabel('X voxels')
            ax5.text(0.05, 0.95, f'Similarity: {CompositionTools.similarity_score(comp_realistic, target_comp):.3f}',
                    transform=ax5.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                    fontweight='bold')

            # Model 3: Realistic - XZ slice
            ax6 = fig.add_subplot(3, 4, 10)
            ax6.imshow(realistic_vol[:, realistic_vol.shape[1]//2, :], cmap='gray')
            ax6.set_title('Realistic Model\nXZ Slice', fontweight='bold')
            ax6.set_ylabel('X voxels')
            ax6.set_xlabel('Z voxels')

            # Composition comparison
            ax7 = fig.add_subplot(3, 4, 7)
            phases = list(target_comp.keys())
            target_vals = [target_comp[p] for p in phases]
            scattered_vals = [comp_scattered.get(p, 0) for p in phases]
            continuous_vals = [comp_continuous.get(p, 0) for p in phases]
            realistic_vals = [comp_realistic.get(p, 0) for p in phases]

            x = np.arange(len(phases))
            width = 0.2

            ax7.bar(x - width*1.5, target_vals, width, label='Target', alpha=0.8, color='black')
            ax7.bar(x - width*0.5, scattered_vals, width, label='Scattered', alpha=0.8, color='blue')
            ax7.bar(x + width*0.5, continuous_vals, width, label='Continuous', alpha=0.8, color='green')
            ax7.bar(x + width*1.5, realistic_vals, width, label='Realistic', alpha=0.8, color='red')

            ax7.set_xlabel('Mineral Phases')
            ax7.set_ylabel('Volume Percentage (%)')
            ax7.set_title('Composition Comparison Across Models', fontweight='bold')
            ax7.set_xticks(x)
            ax7.set_xticklabels(phases, rotation=45)
            ax7.legend()
            ax7.grid(True, alpha=0.3)

            # Quality metrics comparison

            ax8 = fig.add_subplot(3, 4, 8)
            ax9 = fig.add_subplot(3, 4, 11)
            ax10 = fig.add_subplot(3, 4, 12)
            models = ['Scattered', 'Continuous', 'Realistic']
            volumes = [scattered_vol, continuous_vol, realistic_vol]

            metrics_data = []
            for i, (model, vol) in enumerate(zip(models, volumes)):
                intensity_std = np.std(vol)
                moran_i = morans_I(vol)
                porosity = np.mean(vol < 0.5)

                metrics_data.append({
                    'Model': model,
                    'Intensity Std': intensity_std,
                    "Moran's I": moran_i,
                    'Porosity': porosity
                })

            metrics_df = pd.DataFrame(metrics_data)
            x_metrics = np.arange(len(models))
            metrics = ['Intensity Std', "Moran's I", 'Porosity']
            axes = [ax7, ax8, ax9]

            for i, (metric, ax) in enumerate(zip(metrics, axes)):
                values = metrics_df[metric].values
                bars = ax.bar(models, values, alpha=0.8, color=['skyblue', 'lightgreen', 'lightcoral'][i])
                ax.set_title(f'{metric}')
                ax.set_ylabel(metric)
                ax.grid(True, alpha=0.3)

                # Add value labels on bars
                for bar, value in zip(bars, values):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

            # Use ax10 for a summary or leave empty
            ax10.text(0.5, 0.5, 'Quality Metrics\nComparison',
                      transform=ax10.transAxes, ha='center', va='center', fontsize=12, fontweight='bold')
            ax10.axis('off')
            colors = ['skyblue', 'lightgreen', 'lightcoral']

            for i, metric in enumerate(metrics):
                values = metrics_df[metric].values
                ax8.bar(x_metrics + i*0.25, values, 0.25, label=metric, alpha=0.8, color=colors[i])

            ax8.set_xlabel('Models')
            ax8.set_ylabel('Metric Values')
            ax8.set_title('Quality Metrics Comparison', fontweight='bold')
            ax8.set_xticks(x_metrics + 0.25)
            ax8.set_xticklabels(models)
            ax8.legend()
            ax8.grid(True, alpha=0.3)

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_model_comparison.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… Model comparison saved: {output_path}")

        except Exception as e:
            print(f"âš ï¸  Model comparison visualization failed: {e}")
            import traceback
            traceback.print_exc()
    def create_enhanced_model_comparison_visualization(self, scattered_vol, continuous_vol, realistic_vol, complete_continuous_vol,
                                                     sample_name, target_comp,
                                                     comp_scattered, comp_continuous, comp_realistic, comp_complete_continuous):
        """Create comprehensive comparison of all four models"""
        try:
            fig = plt.figure(figsize=(24, 18))
            fig.suptitle(f'4-Model Comparison - {sample_name}\nScattered vs Continuous vs Realistic vs Complete Continuous',
                        fontsize=18, fontweight='bold')

            models = ['Scattered', 'Continuous', 'Realistic', 'Complete Continuous']
            volumes = [scattered_vol, continuous_vol, realistic_vol, complete_continuous_vol]
            compositions = [comp_scattered, comp_continuous, comp_realistic, comp_complete_continuous]
            colors = ['blue', 'green', 'red', 'purple']

            # Create 4x4 grid for comprehensive comparison
            for i, (model, vol, comp, color) in enumerate(zip(models, volumes, compositions, colors)):
                row = i

                # XY slice
                ax1 = fig.add_subplot(4, 4, row*4 + 1)
                ax1.imshow(vol[:, :, vol.shape[2]//2], cmap='gray')
                ax1.set_title(f'{model}\nXY Slice', fontweight='bold')
                if row == 0:
                    ax1.set_ylabel('Y voxels')
                if row == 3:
                    ax1.set_xlabel('X voxels')
                similarity = CompositionTools.similarity_score(comp, target_comp)
                ax1.text(0.05, 0.95, f'Sim: {similarity:.3f}',
                        transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                        fontweight='bold')

                # XZ slice
                ax2 = fig.add_subplot(4, 4, row*4 + 2)
                ax2.imshow(vol[:, vol.shape[1]//2, :], cmap='gray')
                ax2.set_title(f'{model}\nXZ Slice', fontweight='bold')
                if row == 0:
                    ax2.set_ylabel('X voxels')
                if row == 3:
                    ax2.set_xlabel('Z voxels')

            # Composition comparison (bottom row)
            ax_comp = fig.add_subplot(4, 4, (13, 14))
            phases = list(target_comp.keys())
            target_vals = [target_comp[p] for p in phases]

            x = np.arange(len(phases))
            width = 0.15

            ax_comp.bar(x - width*1.5, target_vals, width, label='Target', alpha=0.8, color='black')
            for i, (comp, color, model) in enumerate(zip(compositions, colors, models)):
                comp_vals = [comp.get(p, 0) for p in phases]
                ax_comp.bar(x - width*0.5 + i*width, comp_vals, width, label=model, alpha=0.8, color=color)

            ax_comp.set_xlabel('Mineral Phases')
            ax_comp.set_ylabel('Volume Percentage (%)')
            ax_comp.set_title('Composition Comparison', fontweight='bold')
            ax_comp.set_xticks(x)
            ax_comp.set_xticklabels(phases, rotation=45)
            ax_comp.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            ax_comp.grid(True, alpha=0.3)

            # Quality metrics comparison
            ax_metrics = fig.add_subplot(4, 4, (15, 16))
            metrics_data = []

            for i, (model, vol) in enumerate(zip(models, volumes)):
                intensity_std = np.std(vol)
                moran_i = morans_I(vol)
                porosity = np.mean(vol < 0.5)

                metrics_data.append({
                    'Model': model,
                    'Intensity Std': intensity_std,
                    "Moran's I": moran_i,
                    'Porosity': porosity
                })

            metrics_df = pd.DataFrame(metrics_data)
            x_metrics = np.arange(len(models))
            metrics = ['Intensity Std', "Moran's I", 'Porosity']

            for i, metric in enumerate(metrics):
                values = metrics_df[metric].values
                ax_metrics.bar(x_metrics + i*0.2, values, 0.2, label=metric, alpha=0.8)

            ax_metrics.set_xlabel('Models')
            ax_metrics.set_ylabel('Metric Values')
            ax_metrics.set_title('Quality Metrics Comparison', fontweight='bold')
            ax_metrics.set_xticks(x_metrics + 0.2)
            ax_metrics.set_xticklabels(models, rotation=45)
            ax_metrics.legend()
            ax_metrics.grid(True, alpha=0.3)

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_4model_comparison.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… 4-Model comparison saved: {output_path}")

        except Exception as e:
            print(f"âš ï¸  4-Model comparison visualization failed: {e}")
            import traceback
            traceback.print_exc()
    def create_enhanced_7model_comparison_visualization(self, scattered_vol, continuous_vol, realistic_vol,
                                                  complete_continuous_vol, enhanced_continuous_vol, slicegan_vol, robust_realistic_vol,
                                                  sample_name, target_composition,
                                                  comp_scattered, comp_continuous, comp_realistic,
                                                  comp_complete_continuous, comp_enhanced_continuous, comp_slicegan, comp_robust_realistic):
        """Create comprehensive comparison of all 7 models"""
        try:
            fig = plt.figure(figsize=(28, 20))
            fig.suptitle(f'6-Model Comprehensive Comparison - {sample_name}\nAdvanced 3D Shale Reconstruction Methods',
                        fontsize=20, fontweight='bold')

            models = ['Scattered', 'Continuous', 'Realistic', 'Complete Continuous', 'Enhanced Continuous', 'SliceGAN', 'Robust Realistic']
            volumes = [scattered_vol, continuous_vol, realistic_vol, complete_continuous_vol, enhanced_continuous_vol, slicegan_vol, robust_realistic_vol]
            compositions = [comp_scattered, comp_continuous, comp_realistic, comp_complete_continuous, comp_enhanced_continuous, comp_slicegan, comp_robust_realistic]
            colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'cyan']

            # Create 7x3 grid for comprehensive comparison
            for i, (model, vol, comp, color) in enumerate(zip(models, volumes, compositions, colors)):
                row = i // 2
                col = (i % 2) * 3

                # XY slice
                ax1 = fig.add_subplot(6, 6, row*6 + col + 1)
                ax1.imshow(vol[:, :, vol.shape[2]//2], cmap='gray')
                ax1.set_title(f'{model}\nXY Slice', fontweight='bold', fontsize=10)
                if row == 5:
                    ax1.set_xlabel('X voxels')
                if col == 0:
                    ax1.set_ylabel('Y voxels')

                similarity = CompositionTools.similarity_score(comp, target_composition)
                ax1.text(0.05, 0.95, f'Sim: {similarity:.3f}',
                        transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                        fontweight='bold', fontsize=8)

                # XZ slice
                ax2 = fig.add_subplot(6, 6, row*6 + col + 2)
                ax2.imshow(vol[:, vol.shape[1]//2, :], cmap='gray')
                ax2.set_title(f'{model}\nXZ Slice', fontweight='bold', fontsize=10)
                if row == 5:
                    ax2.set_xlabel('Z voxels')
                if col == 0:
                    ax2.set_ylabel('X voxels')

                # YZ slice
                ax3 = fig.add_subplot(6, 6, row*6 + col + 3)
                ax3.imshow(vol[vol.shape[0]//2, :, :], cmap='gray')
                ax3.set_title(f'{model}\nYZ Slice', fontweight='bold', fontsize=10)
                if row == 5:
                    ax3.set_xlabel('Z voxels')
                if col == 0:
                    ax3.set_ylabel('Y voxels')

            plt.tight_layout()
            output_path = os.path.join(self.output_dir, f'{sample_name}_6model_comprehensive_comparison.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… 6-Model comprehensive comparison saved: {output_path}")

        except Exception as e:
            print(f"âš ï¸  6-Model comparison visualization failed: {e}")
            import traceback
            traceback.print_exc()
# =========================
# SECTION 4 / 4
# =========================
    # ============== FEM Analysis Functions ==============
class NumpyEncoder(json.JSONEncoder):
    """Custom encoder for numpy data types"""
    def default(self, obj):
        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,
                          np.int16, np.int32, np.int64, np.uint8,
                          np.uint16, np.uint32, np.uint64)):
            return int(obj)
        elif isinstance(obj, (np.float_, np.float16, np.float32,
                            np.float64)):
            return float(obj)
        elif isinstance(obj, (np.ndarray,)):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)


class FastShaleReconstructor3D:
    """Main class for fast 3D shale reconstruction"""

    def __init__(self, base_path, excel_path):
        self.base_path = base_path
        self.excel_path = excel_path
        self.mineral_processor = AdvancedMineralProcessor(excel_path)
        self.sem_processor = AdvancedSEMProcessor(base_path)
        self.abaqus_exporter = AdvancedAbaqusExporter(self.mineral_processor)
        self.visualizer = ComprehensiveVisualizer(base_path)
        self.selector = CompositionGuidedSelector(self.mineral_processor)
        self.mineral_processor.load_and_parse_excel()
        self.create_output_directories()
        print("ðŸš€ Advanced Fast Shale Reconstructor 3D Initialized")
        print("ðŸ“Š Following exact methodology from paper")
        print("â±ï¸  Optimized for 15-20 minute execution")
    def create_enhanced_synthetic_sem(self, sample_name, size=(1024, 1024)):
        """Create enhanced synthetic SEM image based on sample composition"""
        print("ðŸŽ¨ Creating enhanced synthetic SEM image based on composition...")

        # Get sample composition to guide synthetic image generation
        comp_data = self.mineral_processor.get_sample_composition(sample_name) or {}
        target_composition = comp_data.get('five_phase_area', {
            'Silicates': 70, 'Carbonate': 15, 'Clay': 10, 'Kerogen': 3, 'Others': 2
        })

        h, w = size
        base = np.random.rand(h, w).astype(np.float32) * 0.3

        # Use composition to guide feature generation
        clay_ratio = target_composition.get('Clay', 10) / 100.0
        silicate_ratio = target_composition.get('Silicates', 70) / 100.0

        # Add mineral grains based on composition
        if silicate_ratio > 0.6:
            # Add more quartz/feldspar grains for silicate-rich samples
            for _ in range(30):
                x, y = np.random.randint(0, w), np.random.randint(0, h)
                radius = np.random.randint(8, 25)
                intensity = 0.8 + 0.2 * np.random.random()

                for i in range(-radius, radius+1):
                    for j in range(-radius, radius+1):
                        xi, yi = x+i, y+j
                        if 0 <= xi < w and 0 <= yi < h:
                            dist = np.sqrt(i**2 + j**2)
                            if dist <= radius:
                                factor = 1.0 - (dist / radius)
                                base[yi, xi] = max(base[yi, xi], intensity * factor)

        # Add clay matrix based on clay content
        if clay_ratio > 0.15:
            # Create fine-grained clay texture
            clay_texture = np.random.rand(h, w) * 0.4 + 0.3
            clay_mask = np.random.rand(h, w) < clay_ratio
            base[clay_mask] = np.minimum(base[clay_mask], clay_texture[clay_mask])

        # Add fractures and bedding
        for _ in range(15):
            x, y = np.random.randint(0, w), np.random.randint(0, h)
            length = np.random.randint(100, 400)
            width = np.random.randint(10, 30)
            angle = np.random.uniform(0, 2*np.pi)

            for i in range(length):
                for j in range(-width//2, width//2):
                    xi = int(x + i * np.cos(angle) + j * np.sin(angle))
                    yi = int(y + i * np.sin(angle) - j * np.cos(angle))

                    if 0 <= xi < w and 0 <= yi < h:
                        dist_factor = 1.0 - abs(j) / (width/2)
                        intensity = 0.6 + 0.3 * dist_factor
                        base[yi, xi] = max(base[yi, xi], intensity)

        # Add pore spaces
        pore_mask = np.random.rand(h, w) > 0.95
        base[pore_mask] = np.maximum(base[pore_mask] * 0.3, 0.1)

        # Apply Gaussian blur for natural appearance
        base = cv2.GaussianBlur(base, (5, 5), 1.5)

        # Normalize to 0-1 range
        base = (base - base.min()) / (base.max() - base.min())

        print("âœ… Enhanced synthetic SEM image created based on composition")
        return base
    def create_output_directories(self):
        directories = [
            'Output/3D_Models', 'Output/Visualizations', 'Output/Abaqus',
            'Output/Training_Data', 'Output/Quality_Metrics', 'Output/Colored', 'Output/TIFF_Stacks'
        ]
        for directory in directories:
            full_path = os.path.join(self.base_path, directory)
            os.makedirs(full_path, exist_ok=True)
            print(f"ðŸ“ Created: {directory}")

        # Create sample-specific subdirectories
        sample_dirs = ['Training_Data', 'Quality_Metrics']
        for sample_dir in sample_dirs:
            sample_path = os.path.join(self.base_path, 'Output', sample_dir, 'sample1')
            os.makedirs(sample_path, exist_ok=True)
            sample_path = os.path.join(self.base_path, 'Output', sample_dir, 'sample6')
            os.makedirs(sample_path, exist_ok=True)

    def process_sample(self, sample_name, unet_epochs=40, slicegan_epochs=40):
        print(f"\nðŸŽ¯ PROCESSING SAMPLE: {sample_name}")
        print("=" * 70)
        start_time = pd.Timestamp.now()

        # Step 1: Load SEM data with enhanced validation
        print("ðŸ”¬ Step 1: Loading and validating SEM images...")
        sem_data = self.sem_processor.load_sample_images(sample_name)

        # Enhanced validation with composition awareness
        if sem_data['mineral_map'] is None and sem_data['bse'] is None:
            print("âŒ No SEM images found, creating enhanced synthetic data...")
            sem_data['mineral_map'] = self.create_enhanced_synthetic_sem(sample_name)
        elif sem_data['mineral_map'] is None:
            print("ðŸ”„ Using enhanced BSE image as mineral map...")
            sem_data['mineral_map'] = self.sem_processor.enhance_image_quality(sem_data['bse'])

        # Step 2: Create enhanced composition-aware training patches
        print("âœ‚ï¸ Step 2: Creating enhanced 10x10 training patches...")
        comp_data = self.mineral_processor.get_sample_composition(sample_name) or {}
        target_composition = comp_data.get('five_phase_area', {
            'Silicates': 70, 'Carbonate': 15, 'Clay': 10, 'Kerogen': 3, 'Others': 2
        })

        patches, positions = self.create_enhanced_10x10_patches(sem_data['mineral_map'], target_composition)
        self.patches = patches
        if patches is None:
            print("âŒ Patch creation failed"); return None
        print(f"âœ… Created {len(patches)} composition-aware patches for training")

        # Step 3: Train Enhanced Residual Attention U-Net
        print("ðŸ§  Step 3: Training Enhanced Residual Attention U-Net...")
        unet = ResidualAttentionUNet(input_size=(64, 64, 1), num_classes=5)

        # Enhanced training with composition guidance
        unet_history = unet.train_fast_enhanced(
            patches,
            target_composition,
            epochs=max(40, unet_epochs),
            batch_size=8
        )

        # Step 4: Segment patches (QC)
        print("ðŸŽ¨ Step 4: Segmenting patches...")
        segmented_patches = unet.segment_patches(patches)

        # Step 5: Generate MULTIPLE 3D models for comparison
        print("ðŸ—ï¸ Step 5: Generating multiple 3D models for comparison...")

        # Get target composition
        comp_data = self.mineral_processor.get_sample_composition(sample_name) or {}
        target_composition = comp_data.get('five_phase_area', {
            'Silicates': 70, 'Carbonate': 15, 'Clay': 10, 'Kerogen': 3, 'Others': 2
        })

        sem_reference = sem_data['mineral_map'] if sem_data['mineral_map'] is not None else sem_data['bse']

        # Model 1: Original scattered approach
        print("ðŸ”¹ Model 1: Original scattered approach...")
        assembler = MultiScaleVolumeAssembler(vol_size=128, patch_size=64, stride=32, smooth_sigma=0.8)
        banks = assembler.make_banks(sem_reference)
        scattered_vol = assembler.assemble(banks)

        # Match intensity statistics to SEM
        if sem_reference is not None:
            ref = sem_reference.astype(np.float32)
            if ref.max() > 2.0:
                ref = ref / ref.max()
            if ref.ndim == 3:
                ref = np.mean(ref, axis=2)
            ref_resized = cv2.resize(ref, (scattered_vol.shape[1], scattered_vol.shape[0]), interpolation=cv2.INTER_AREA)
            for z in range(scattered_vol.shape[2]):
                scattered_vol[:, :, z] = robust_minmax_match(scattered_vol[:, :, z], ref_resized)
        scattered_vol = clip01(scattered_vol)

        # Model 2: Original continuous approach
        print("ðŸ”¹ Model 2: Original continuous approach...")
        continuous_assembler = MultiScaleVolumeAssembler(vol_size=128, patch_size=64, stride=16, smooth_sigma=1.2)
        continuous_vol = continuous_assembler.assemble_continuous(banks)

        # Match intensity statistics for continuous model
        if sem_reference is not None:
            for z in range(continuous_vol.shape[2]):
                continuous_vol[:, :, z] = robust_minmax_match(continuous_vol[:, :, z], ref_resized)
        continuous_vol = clip01(continuous_vol)

        # Model 3: Realistic geological approach
        print("ðŸ”¹ Model 3: Realistic geological approach...")
        shale_assembler = RealisticShaleAssembler(
            vol_size=128,
            patch_size=64,
            mineral_composition=target_composition
        )
        realistic_vol = shale_assembler.create_realistic_shale_volume(sem_reference)

        # Model 4: NEW Complete Continuous 3D Shale Model
        print("ðŸ”¹ Model 4: Complete Continuous 3D Shale Model...")
        enhanced_continuous_assembler = EnhancedContinuousAssembler(vol_size=128, patch_size=64)
        complete_continuous_vol = self._create_enhanced_continuous_volume(sem_reference, target_composition)

        # Model 5: Enhanced Continuous with Geological Features
        print("ðŸ”¹ Model 5: Enhanced Continuous with Geological Features...")
        enhanced_continuous_vol = self._create_enhanced_continuous_with_geology(sem_reference, target_composition)

        # Match intensity statistics for complete continuous model
        if sem_reference is not None:
            if sem_reference.ndim == 3:
                sem_ref_2d = np.mean(sem_reference, axis=2)
            else:
                sem_ref_2d = sem_reference
            ref_resized = cv2.resize(sem_ref_2d, (complete_continuous_vol.shape[1], complete_continuous_vol.shape[0]), interpolation=cv2.INTER_AREA)
            for z in range(complete_continuous_vol.shape[2]):
                complete_continuous_vol[:, :, z] = robust_minmax_match(complete_continuous_vol[:, :, z], ref_resized)
        complete_continuous_vol = clip01(complete_continuous_vol)

        # Model 6: SliceGAN 3D Generation
        print("ðŸ”¹ Model 6: SliceGAN 3D Generation...")
        slicegan_vol = self._generate_slicegan_volume(patches, target_composition, sample_name)

        # Match intensity for SliceGAN model
        if sem_reference is not None and slicegan_vol is not None:
            for z in range(slicegan_vol.shape[2]):
                slicegan_vol[:, :, z] = robust_minmax_match(slicegan_vol[:, :, z], ref_resized)
            slicegan_vol = clip01(slicegan_vol)

        # ðŸ”¹ ADD THE NEW ROBUST MODEL
        print("ðŸ”¹ Model 7: Robust Realistic Geological Model...")
        robust_realistic = self._generate_robust_realistic_model(sem_reference, target_composition)

        print("âœ… Generated 7 different 3D models for comparison")

        # Step 6: Match slice stats to reference SEM (mineral map or BSE)
        print("ðŸ“ Step 6: Matching intensity statistics to SEM...")
        ref_img = sem_data['bse'] if sem_data['bse'] is not None else sem_data['mineral_map']
        if ref_img is not None:
            ref = ref_img.astype(np.float32)
            if ref.max() > 2.0:
                ref = ref / ref.max()
            if ref.ndim == 3:
                ref = np.mean(ref, axis=2)  # Convert to grayscale if RGB
            ref_resized = cv2.resize(ref, (scattered_vol.shape[1], scattered_vol.shape[0]), interpolation=cv2.INTER_AREA)
            for z in range(scattered_vol.shape[2]):
                scattered_vol[:, :, z] = robust_minmax_match(scattered_vol[:, :, z], ref_resized)
        scattered_vol = clip01(scattered_vol)

        # Step 7: 3D refinement for ALL models
        print("ðŸ§½ Step 7: Applying 3D refinement to all models...")
        cont = VolumeContinuityEnforcer(smooth_sigma=1.0)

        # Refine scattered model
        try:
            refiner = build_refiner3d(input_shape=(128,128,128,1))
            refiner.fit(scattered_vol[None,...,None], scattered_vol[None,...,None], epochs=1, batch_size=1, verbose=0)
            scattered_refined = refiner.predict(scattered_vol[None,...,None], verbose=0)[0,...,0]
        except Exception as e:
            print(f"âš ï¸  Scattered refiner skipped: {e}")
            scattered_refined = scattered_vol.copy()

        scattered_refined = cont.anisotropic_diffusion_3d(scattered_refined, iters=8, kappa=35.0, gamma=0.15)
        scattered_refined = self.enhance_3d_continuity(scattered_refined)
        scattered_refined = cont.sharpen_boundaries(scattered_refined, alpha=0.3)

        # Refine continuous model
        try:
            refiner_continuous = build_refiner3d(input_shape=(128,128,128,1))
            refiner_continuous.fit(continuous_vol[None,...,None], continuous_vol[None,...,None], epochs=1, batch_size=1, verbose=0)
            continuous_refined = refiner_continuous.predict(continuous_vol[None,...,None], verbose=0)[0,...,0]
        except Exception as e:
            print(f"âš ï¸  Continuous refiner skipped: {e}")
            continuous_refined = continuous_vol.copy()

        continuous_refined = cont.anisotropic_diffusion_3d(continuous_refined, iters=10, kappa=25.0, gamma=0.1)
        continuous_refined = self.enhance_3d_continuity(continuous_refined)
        continuous_refined = cont.sharpen_boundaries(continuous_refined, alpha=0.2)

        # Refine realistic model
        realistic_refined = cont.anisotropic_diffusion_3d(realistic_vol, iters=4, kappa=50.0, gamma=0.05)
        realistic_refined = cont.sharpen_boundaries(realistic_refined, alpha=0.1)

        # Refine complete continuous model (light refinement to preserve continuity)
        complete_continuous_refined = cont.anisotropic_diffusion_3d(complete_continuous_vol, iters=6, kappa=60.0, gamma=0.03)
        complete_continuous_refined = cont.sharpen_boundaries(complete_continuous_refined, alpha=0.05)

        # Refine enhanced continuous model
        enhanced_continuous_refined = cont.anisotropic_diffusion_3d(enhanced_continuous_vol, iters=8, kappa=40.0, gamma=0.12)
        enhanced_continuous_refined = self.enhance_geological_realism(enhanced_continuous_refined, target_composition)
        enhanced_continuous_refined = cont.sharpen_boundaries(enhanced_continuous_refined, alpha=0.25)

        # Refine SliceGAN model if available
        if slicegan_vol is not None:
            slicegan_refined = cont.anisotropic_diffusion_3d(slicegan_vol, iters=6, kappa=45.0, gamma=0.15)
            slicegan_refined = self.enhance_3d_continuity(slicegan_refined)
            slicegan_refined = cont.sharpen_boundaries(slicegan_refined, alpha=0.2)
        else:
            slicegan_refined = enhanced_continuous_refined.copy()  # Fallback

        # ðŸ”¹ ADD ROBUST REALISTIC REFINEMENT
        print("ðŸ”¹ Refining robust realistic model...")
        robust_realistic_refined = cont.anisotropic_diffusion_3d(robust_realistic, iters=8, kappa=35.0, gamma=0.12)
        robust_realistic_refined = self.enhance_geological_realism(robust_realistic_refined, target_composition)
        robust_realistic_refined = cont.sharpen_boundaries(robust_realistic_refined, alpha=0.3)

        print("âœ… All 7 models refined")

        # Step 7b: Process continuous 3D model
        print("ðŸ§½ Step 7b: 3D refinement for continuous model...")
        try:
            refiner_continuous = build_refiner3d(input_shape=(128,128,128,1))
            refiner_continuous.fit(continuous_vol[None,...,None], continuous_vol[None,...,None], epochs=1, batch_size=1, verbose=0)
            intensity_continuous_refined = refiner_continuous.predict(continuous_vol[None,...,None], verbose=0)[0,...,0]
        except Exception as e:
            print(f"âš ï¸  Continuous refiner skipped due to: {e}")
            intensity_continuous_refined = continuous_vol.copy() if 'continuous_vol' in locals() else continuous_assembler.assemble_continuous(banks)

        intensity_continuous_refined = cont.anisotropic_diffusion_3d(intensity_continuous_refined, iters=10, kappa=25.0, gamma=0.1)
        intensity_continuous_refined = self.enhance_3d_continuity(intensity_continuous_refined)
        intensity_continuous_refined = cont.sharpen_boundaries(intensity_continuous_refined, alpha=0.2)

        # Step 7c: Enhanced quality for continuous model
        print("ðŸŽ¨ Step 7c: Enhancing natural appearance for continuous model...")
        intensity_continuous_refined = self.enhance_3d_model_quality(intensity_continuous_refined)

        # Step 8: Labeling + composition prior for ALL models
        print("ðŸ·ï¸ Step 8: Applying composition-aware labeling to all models...")
        assigner = LabelAssigner(target_composition)

        # Label scattered model
        labels_scattered = assigner.assign(scattered_refined)
        labels_scattered = cont.prune_small_components(labels_scattered)
        labels_scattered = cont.smooth_labels_3d(labels_scattered, radius=1)
        labels_scattered = assigner.soft_rebalance(labels_scattered)
        achieved_comp_scattered = CompositionTools.composition_from_labels(labels_scattered)
        score_scattered = CompositionTools.similarity_score(achieved_comp_scattered, target_composition)

        # Label continuous model
        labels_continuous = assigner.assign(continuous_refined)
        labels_continuous = cont.prune_small_components(labels_continuous)
        labels_continuous = cont.smooth_labels_3d(labels_continuous, radius=2)
        labels_continuous = assigner.soft_rebalance(labels_continuous)
        achieved_comp_continuous = CompositionTools.composition_from_labels(labels_continuous)
        score_continuous = CompositionTools.similarity_score(achieved_comp_continuous, target_composition)

        # Label realistic model
        labels_realistic = assigner.assign(realistic_refined)
        labels_realistic = cont.prune_small_components(labels_realistic)
        labels_realistic = cont.smooth_labels_3d(labels_realistic, radius=1)
        labels_realistic = assigner.soft_rebalance(labels_realistic)
        achieved_comp_realistic = CompositionTools.composition_from_labels(labels_realistic)
        score_realistic = CompositionTools.similarity_score(achieved_comp_realistic, target_composition)

        # Label complete continuous model
        labels_complete_continuous = assigner.assign(complete_continuous_refined)
        labels_complete_continuous = cont.prune_small_components(labels_complete_continuous)
        labels_complete_continuous = cont.smooth_labels_3d(labels_complete_continuous, radius=1)
        labels_complete_continuous = assigner.soft_rebalance(labels_complete_continuous)
        achieved_comp_complete_continuous = CompositionTools.composition_from_labels(labels_complete_continuous)
        score_complete_continuous = CompositionTools.similarity_score(achieved_comp_complete_continuous, target_composition)

        # Label enhanced continuous model
        labels_enhanced_continuous = assigner.assign(enhanced_continuous_refined)
        labels_enhanced_continuous = cont.prune_small_components(labels_enhanced_continuous)
        labels_enhanced_continuous = cont.smooth_labels_3d(labels_enhanced_continuous, radius=2)
        labels_enhanced_continuous = assigner.soft_rebalance(labels_enhanced_continuous)
        achieved_comp_enhanced_continuous = CompositionTools.composition_from_labels(labels_enhanced_continuous)
        score_enhanced_continuous = CompositionTools.similarity_score(achieved_comp_enhanced_continuous, target_composition)

        # Label SliceGAN model
        labels_slicegan = assigner.assign(slicegan_refined)
        labels_slicegan = cont.prune_small_components(labels_slicegan)
        labels_slicegan = cont.smooth_labels_3d(labels_slicegan, radius=1)
        labels_slicegan = assigner.soft_rebalance(labels_slicegan)
        achieved_comp_slicegan = CompositionTools.composition_from_labels(labels_slicegan)
        score_slicegan = CompositionTools.similarity_score(achieved_comp_slicegan, target_composition)

        # ðŸ”¹ ADD ROBUST REALISTIC LABELING
        labels_robust_realistic = assigner.assign(robust_realistic_refined)
        labels_robust_realistic = cont.prune_small_components(labels_robust_realistic)
        labels_robust_realistic = cont.smooth_labels_3d(labels_robust_realistic, radius=2)
        labels_robust_realistic = assigner.soft_rebalance(labels_robust_realistic)
        achieved_comp_robust_realistic = CompositionTools.composition_from_labels(labels_robust_realistic)
        score_robust_realistic = CompositionTools.similarity_score(achieved_comp_robust_realistic, target_composition)

        print(f"ðŸ“Š Composition similarity scores:")
        print(f"   â€¢ Scattered model: {score_scattered:.4f}")
        print(f"   â€¢ Continuous model: {score_continuous:.4f}")
        print(f"   â€¢ Realistic model: {score_realistic:.4f}")
        print(f"   â€¢ Complete Continuous model: {score_complete_continuous:.4f}")
        print(f"   â€¢ Enhanced Continuous model: {score_enhanced_continuous:.4f}")
        print(f"   â€¢ SliceGAN model: {score_slicegan:.4f}")
        print(f"   â€¢ Robust Realistic model: {score_robust_realistic:.4f}")

        # Step 8b: Labeling for continuous model
        print("ðŸ·ï¸ Step 8b: Labeling continuous model with composition prior...")
        labels_continuous = assigner.assign(intensity_continuous_refined)
        labels_continuous = cont.prune_small_components(labels_continuous)
        labels_continuous = cont.smooth_labels_3d(labels_continuous, radius=2)  # Stronger smoothing for continuous
        labels_continuous = assigner.soft_rebalance(labels_continuous)
        achieved_comp_continuous = CompositionTools.composition_from_labels(labels_continuous)
        score_continuous = CompositionTools.similarity_score(achieved_comp_continuous, target_composition)
        print(f"ðŸ“Š Composition similarity (continuous model vs target): {score_continuous:.4f}")

        # Step 9: Enhanced visualizations & QC for ALL models
        print("ðŸ“Š Step 9: Creating comprehensive visualizations for all models...")
        palette = MineralPalette(self.base_path)
        palette.try_extract_from_folder(sample_name)
        color_tools = ColorAndExportTools(self.base_path, palette)

        # Save colored slices for all models
        color_tools.save_colored_center_slices(labels_scattered, sample_name + "_scattered")
        color_tools.save_colored_center_slices(labels_continuous, sample_name + "_continuous")
        color_tools.save_colored_center_slices(labels_realistic, sample_name + "_realistic")
        color_tools.save_colored_center_slices(labels_complete_continuous, sample_name + "_complete_continuous")
        color_tools.save_colored_center_slices(labels_enhanced_continuous, sample_name + "_enhanced_continuous")
        color_tools.save_colored_center_slices(labels_slicegan, sample_name + "_slicegan")
        color_tools.save_colored_center_slices(labels_robust_realistic, sample_name + "_robust_realistic")

        # Save TIFF stacks for all models
        color_tools.save_tiff_stack_labels(labels_scattered, sample_name + "_scattered")
        color_tools.save_tiff_stack_labels(labels_continuous, sample_name + "_continuous")
        color_tools.save_tiff_stack_labels(labels_realistic, sample_name + "_realistic")
        color_tools.save_tiff_stack_labels(labels_complete_continuous, sample_name + "_complete_continuous")
        color_tools.save_tiff_stack_labels(labels_enhanced_continuous, sample_name + "_enhanced_continuous")
        color_tools.save_tiff_stack_labels(labels_slicegan, sample_name + "_slicegan")
        color_tools.save_tiff_stack_labels(labels_robust_realistic, sample_name + "_robust_realistic")

        # Create comprehensive visualizations for all models
        self.visualizer.create_3d_volume_visualization(scattered_refined, sample_name + "_scattered")
        self.visualizer.create_3d_volume_visualization(continuous_refined, sample_name + "_continuous")
        self.visualizer.create_realistic_shale_visualization(realistic_refined, sample_name + "_realistic")
        self.visualizer.create_realistic_shale_visualization(complete_continuous_refined, sample_name + "_complete_continuous")
        self.visualizer.create_realistic_shale_visualization(enhanced_continuous_refined, sample_name + "_enhanced_continuous")
        self.visualizer.create_realistic_shale_visualization(slicegan_refined, sample_name + "_slicegan")
        self.visualizer.create_realistic_shale_visualization(robust_realistic_refined, sample_name + "_robust_realistic")

        # Create slice comparisons for all models
        self.visualizer.create_slice_comparison(patches[:4, :, :, 0], scattered_refined, sample_name + "_scattered")
        self.visualizer.create_slice_comparison(patches[:4, :, :, 0], continuous_refined, sample_name + "_continuous")
        self.visualizer.create_slice_comparison(patches[:4, :, :, 0], realistic_refined, sample_name + "_realistic")
        self.visualizer.create_slice_comparison(patches[:4, :, :, 0], complete_continuous_refined, sample_name + "_complete_continuous")
        self.visualizer.create_slice_comparison(patches[:4, :, :, 0], robust_realistic_refined, sample_name + "_robust_realistic")

        # Create enhanced model comparison visualization with all 7 models
        self.visualizer.create_enhanced_7model_comparison_visualization(
            scattered_refined, continuous_refined, realistic_refined,
            complete_continuous_refined, enhanced_continuous_refined, slicegan_refined, robust_realistic_refined,
            sample_name, target_composition,
            achieved_comp_scattered, achieved_comp_continuous,
            achieved_comp_realistic, achieved_comp_complete_continuous,
            achieved_comp_enhanced_continuous, achieved_comp_slicegan, achieved_comp_robust_realistic
        )

        extra_viz = ExtraVisualizer(self.base_path)

        # Add orthogrid for all models
        extra_viz.orthogrid(scattered_refined, sample_name + "_scattered", n=8)
        extra_viz.orthogrid(continuous_refined, sample_name + "_continuous", n=8)
        extra_viz.orthogrid(realistic_refined, sample_name + "_realistic", n=8)
        extra_viz.orthogrid(complete_continuous_refined, sample_name + "_complete_continuous", n=8)
        extra_viz.orthogrid(enhanced_continuous_refined, sample_name + "_enhanced_continuous", n=8)
        extra_viz.orthogrid(slicegan_refined, sample_name + "_slicegan", n=8)
        extra_viz.orthogrid(robust_realistic_refined, sample_name + "_robust_realistic", n=8)

        # Composition analysis for all models
        similarity_score_enhanced_continuous = self.visualizer.create_composition_analysis(target_composition, achieved_comp_enhanced_continuous, sample_name + "_enhanced_continuous")
        similarity_score_slicegan = self.visualizer.create_composition_analysis(target_composition, achieved_comp_slicegan, sample_name + "_slicegan")
        similarity_score_robust_realistic = self.visualizer.create_composition_analysis(target_composition, achieved_comp_robust_realistic, sample_name + "_robust_realistic")

        extra_viz.composition_radar(target_composition, achieved_comp_enhanced_continuous, sample_name + "_enhanced_continuous")
        extra_viz.composition_radar(target_composition, achieved_comp_slicegan, sample_name + "_slicegan")
        extra_viz.composition_radar(target_composition, achieved_comp_robust_realistic, sample_name + "_robust_realistic")

        extra_viz = ExtraVisualizer(self.base_path)
        extra_viz.orthogrid(scattered_refined, sample_name + "_scattered", n=8)
        extra_viz.orthogrid(continuous_refined, sample_name + "_continuous", n=8)
        extra_viz.orthogrid(realistic_refined, sample_name + "_realistic", n=8)
        extra_viz.orthogrid(complete_continuous_refined, sample_name + "_complete_continuous", n=8)
        extra_viz.orthogrid(robust_realistic_refined, sample_name + "_robust_realistic", n=8)

        similarity_score_scattered = self.visualizer.create_composition_analysis(target_composition, achieved_comp_scattered, sample_name + "_scattered")
        similarity_score_continuous = self.visualizer.create_composition_analysis(target_composition, achieved_comp_continuous, sample_name + "_continuous")
        similarity_score_realistic = self.visualizer.create_composition_analysis(target_composition, achieved_comp_realistic, sample_name + "_realistic")
        similarity_score_complete_continuous = self.visualizer.create_composition_analysis(target_composition, achieved_comp_complete_continuous, sample_name + "_complete_continuous")
        similarity_score_robust_realistic = self.visualizer.create_composition_analysis(target_composition, achieved_comp_robust_realistic, sample_name + "_robust_realistic")

        extra_viz.composition_radar(target_composition, achieved_comp_scattered, sample_name + "_scattered")
        extra_viz.composition_radar(target_composition, achieved_comp_continuous, sample_name + "_continuous")
        extra_viz.composition_radar(target_composition, achieved_comp_realistic, sample_name + "_realistic")
        extra_viz.composition_radar(target_composition, achieved_comp_complete_continuous, sample_name + "_complete_continuous")
        extra_viz.composition_radar(target_composition, achieved_comp_robust_realistic, sample_name + "_robust_realistic")

        # Step 10: Export BEST model to Abaqus based on composition similarity
        print("ðŸ’¾ Step 10: Exporting BEST model to Abaqus...")
        abaqus_dir = os.path.join(self.base_path, 'Output/Abaqus')

        # Find best model based on composition similarity
        models_data = [
            (realistic_refined, score_realistic, "realistic"),
            (enhanced_continuous_refined, score_enhanced_continuous, "enhanced_continuous"),
            (slicegan_refined, score_slicegan, "slicegan"),
            (complete_continuous_refined, score_complete_continuous, "complete_continuous"),
            (robust_realistic_refined, score_robust_realistic, "robust_realistic")
        ]

        best_model, best_score, best_name = max(models_data, key=lambda x: x[1])

        print(f"ðŸ† Exporting {best_name} model (score: {best_score:.4f}) to Abaqus")
        inp_path, prop_path = self.abaqus_exporter.export_to_abaqus(best_model, f"{sample_name}_{best_name}", abaqus_dir)
        print("ðŸ’¾ Step 10b: Exporting CAE-compatible small file...")
        cae_path = self.abaqus_exporter.export_abaqus_cae_compatible(best_model, f"{sample_name}_{best_name}")

        # Step 11: Save ALL model results
        print("ðŸ’¾ Step 11: Saving results for all models...")
        self.save_comprehensive_results(
            scattered_refined, continuous_refined, realistic_refined,
            complete_continuous_refined, enhanced_continuous_refined, slicegan_refined, robust_realistic_refined,
            sample_name, target_composition,
            achieved_comp_scattered, achieved_comp_continuous, achieved_comp_realistic,
            achieved_comp_complete_continuous, achieved_comp_enhanced_continuous, achieved_comp_slicegan, achieved_comp_robust_realistic
        )

        print("ðŸ“Š Step 12: Calculating equivalent modulus and comprehensive analysis...")
        youngs_modulus_dict = {
            'Silicates': 89.6, 'Carbonate': 74.6, 'Clay': 22.3,
            'Kerogen': 9.2, 'Others': 12.392
        }

        # Calculate equivalent modulus using realistic model
        equivalent_modulus = calculate_equivalent_modulus_fem(realistic_refined, youngs_modulus_dict)

        # Calculate comprehensive statistics for realistic model
        volume_stats = calculate_volume_statistics(realistic_refined)
        spatial_corr = calculate_spatial_correlation(realistic_refined)
        connectivity = calculate_phase_connectivity(realistic_refined)

        print(f"âœ… Equivalent modulus: {equivalent_modulus:.2f} GPa")
        print(f"ðŸ“ˆ Volume statistics: Mean={volume_stats['mean_intensity']:.3f}, "
              f"Std={volume_stats['std_intensity']:.3f}, Porosity={volume_stats['porosity_05']:.1%}")
        print(f"ðŸ”— Spatial correlation: X={spatial_corr['correlation_x']:.3f}, "
              f"Y={spatial_corr['correlation_y']:.3f}, Z={spatial_corr['correlation_z']:.3f}")

        end_time = pd.Timestamp.now()
        processing_time = (end_time - start_time).total_seconds() / 60.0
        print(f"\nðŸŽ‰ PROCESSING COMPLETED FOR {sample_name}")
        print("=" * 70)
        print(f"â±ï¸  Total time: {processing_time:.1f} minutes")
        print(f"ðŸ“Š Composition similarity: {score_realistic:.4f}")
        print(f"ðŸŽ¯ Scattered volume dimensions: {scattered_refined.shape}")
        print(f"ðŸŽ¯ Continuous volume dimensions: {continuous_refined.shape}")
        print(f"ðŸ† Best model: {best_name} (score: {best_score:.4f})")
        print(f"ðŸŽ¯ Model dimensions: {best_model.shape}")
        print(f"ðŸŽ¯ Realistic volume dimensions: {realistic_refined.shape}")
        print(f"ðŸ’¾ Abaqus INP: {inp_path}")
        print(f"ðŸ”§ Material props: {prop_path}")
        print(f"ðŸ“ˆ Visualizations saved in Output/ folder")

        return {
            'scattered_volume': scattered_refined,
            'continuous_volume': continuous_refined,
            'realistic_volume': realistic_refined,
            'complete_continuous_volume': complete_continuous_refined,
            'enhanced_continuous_volume': enhanced_continuous_refined,
            'slicegan_volume': slicegan_refined,
            'robust_realistic_volume': robust_realistic_refined,
            'scattered_labels': labels_scattered,
            'continuous_labels': labels_continuous,
            'realistic_labels': labels_realistic,
            'complete_continuous_labels': labels_complete_continuous,
            'enhanced_continuous_labels': labels_enhanced_continuous,
            'slicegan_labels': labels_slicegan,
            'robust_realistic_labels': labels_robust_realistic,
            'scattered_similarity': score_scattered,
            'continuous_similarity': score_continuous,
            'realistic_similarity': score_realistic,
            'complete_continuous_similarity': score_complete_continuous,
            'enhanced_continuous_similarity': score_enhanced_continuous,
            'slicegan_similarity': score_slicegan,
            'robust_realistic_similarity': score_robust_realistic,
            'best_model': best_name,
            'best_score': best_score,
            'processing_time': processing_time,
            'abaqus_files': [inp_path, prop_path, cae_path],
            'composition': {
                'target': target_composition,
                'scattered_achieved': achieved_comp_scattered,
                'continuous_achieved': achieved_comp_continuous,
                'realistic_achieved': achieved_comp_realistic,
                'complete_continuous_achieved': achieved_comp_complete_continuous,
                'enhanced_continuous_achieved': achieved_comp_enhanced_continuous,
                'slicegan_achieved': achieved_comp_slicegan,
                'robust_realistic_achieved': achieved_comp_robust_realistic
            }
        }
    def _create_enhanced_continuous_with_geology(self, sem_reference, target_composition):
        """Create enhanced continuous model with geological features"""
        V = 128
        volume = np.zeros((V, V, V), dtype=np.float32)

        # Create base with strong geological continuity
        base_profile = self._create_geological_depth_profile(target_composition, V)

        for z in range(V):
            if z == 0:
                # Start with SEM reference pattern
                if sem_reference is not None:
                    if sem_reference.ndim == 3:
                        sem_ref = np.mean(sem_reference, axis=2)
                    else:
                        sem_ref = sem_reference
                    base_slice = cv2.resize(sem_ref, (V, V))
                else:
                    base_slice = np.random.rand(V, V) * 0.5 + 0.3
            else:
                # Very strong correlation with previous slice (95%)
                base_slice = 0.95 * volume[:, :, z-1] + 0.05 * np.random.rand(V, V)

            # Apply geological depth profile
            volume[:, :, z] = base_slice * base_profile[z]

        # Add composition-specific features
        volume = self._add_composition_features(volume, target_composition)

        return clip01(volume)

    def _generate_slicegan_volume(self, patches, target_composition, sample_name):
        """Generate 3D volume using memory-optimized SliceGAN"""
        try:
            print("ðŸ”ï¸ Initializing Memory-Optimized SliceGAN...")

            # Clear memory first
            import gc
            gc.collect()

            # Initialize with smaller dimensions
            slicegan = AdvancedSliceGAN3D(latent_dim=128, volume_size=64)

            # Train with better parameters - UPDATED
            print("ðŸš€ Starting memory-optimized training...")
            d_losses, g_losses = slicegan.train_with_memory_management(
                patches,
                target_composition,
                epochs=12,  # Increased from 6 to 12 for better quality
                batch_size=4
            )

            # Generate volumes
            print("ðŸŽ¨ Generating geological volumes...")
            generated_volumes = slicegan.generate_geological_volumes(
                num_samples=4,
                target_composition=target_composition
            )

            # Select best volume
            best_volume = self.select_most_geological_volume(generated_volumes, target_composition)

            print("âœ… Memory-optimized SliceGAN completed")
            return best_volume

        except Exception as e:
            print(f"âŒ SliceGAN failed: {e}")
            print("ðŸ”„ Using enhanced continuous model as fallback...")
            return self._create_enhanced_continuous_volume(None, target_composition)

    def select_most_geological_volume(self, volumes, target_composition):
        """Select volume with best geological realism"""
        best_volume = None
        best_score = -1

        for volume in volumes:
            # Calculate geological realism score
            bedding_score = self.calculate_bedding_quality(volume)
            fracture_score = self.calculate_fracture_realism(volume)
            composition_score = self.calculate_composition_match(volume, target_composition)

            total_score = bedding_score * 0.4 + fracture_score * 0.3 + composition_score * 0.3

            if total_score > best_score:
                best_score = total_score
                best_volume = volume

        print(f"ðŸ† Selected volume with geological score: {best_score:.4f}")
        return best_volume

    def calculate_bedding_quality(self, volume):
        """Calculate bedding structure quality score"""
        try:
            # Analyze horizontal continuity (bedding should be strong horizontally)
            horizontal_variance = np.var(np.mean(volume, axis=(0, 1)))  # Variance along Z
            vertical_variance = np.var(np.mean(volume, axis=(0, 2)))    # Variance along Y

            # Good bedding has low vertical variance, moderate horizontal variance
            bedding_score = 1.0 / (1.0 + vertical_variance) * (1.0 + horizontal_variance)
            return min(bedding_score, 1.0)
        except:
            return 0.5

    def calculate_fracture_realism(self, volume):
        """Calculate fracture network realism score"""
        try:
            # Analyze gradient patterns for fracture-like features
            grad_z = np.gradient(volume, axis=2)
            fracture_like = np.mean(np.abs(grad_z) > 0.1)  # Percentage of strong gradients

            # Good fractures: present but not overwhelming (5-15% of volume)
            ideal_fracture_ratio = 0.1
            fracture_score = 1.0 - abs(fracture_like - ideal_fracture_ratio) / ideal_fracture_ratio
            return max(fracture_score, 0.0)
        except:
            return 0.5

    def calculate_composition_match(self, volume, target_composition):
        """Calculate how well volume matches target composition"""
        try:
            if not target_composition:
                return 0.5

            # Estimate composition from intensity distribution
            percentiles = np.percentile(volume, [20, 40, 60, 80])
            intensity_ranges = [
                np.mean(volume < percentiles[0]),
                np.mean((volume >= percentiles[0]) & (volume < percentiles[1])),
                np.mean((volume >= percentiles[1]) & (volume < percentiles[2])),
                np.mean((volume >= percentiles[2]) & (volume < percentiles[3])),
                np.mean(volume >= percentiles[3])
            ]

            # Normalize
            total = sum(intensity_ranges)
            estimated_comp = [r/total * 100 for r in intensity_ranges]

            # Compare with target
            target_vals = list(target_composition.values())
            similarity = 1.0 - np.mean(np.abs(np.array(estimated_comp) - np.array(target_vals))) / 100.0
            return max(similarity, 0.0)
        except:
            return 0.5

    def _select_best_slicegan_volume(self, volumes, reference_patches, target_composition):
        """Select best SliceGAN volume based on quality metrics"""
        best_volume = None
        best_score = -1

        for i, volume in enumerate(volumes):
            # Calculate quality score
            intensity_score = self._calculate_intensity_similarity(volume, reference_patches)
            texture_score = self._calculate_texture_similarity(volume, reference_patches)
            composition_score = self._estimate_composition_similarity(volume, target_composition)

            total_score = intensity_score * 0.4 + texture_score * 0.4 + composition_score * 0.2

            if total_score > best_score:
                best_score = total_score
                best_volume = volume

        print(f"ðŸ† Selected SliceGAN volume with score: {best_score:.4f}")
        return best_volume

    def _calculate_intensity_similarity(self, volume, reference_patches):
        """Calculate intensity distribution similarity"""
        vol_mean = np.mean(volume)
        ref_mean = np.mean(reference_patches)
        vol_std = np.std(volume)
        ref_std = np.std(reference_patches)

        mean_sim = 1.0 - abs(vol_mean - ref_mean)
        std_sim = 1.0 - abs(vol_std - ref_std) / max(vol_std, ref_std)

        return (mean_sim + std_sim) / 2.0

    def _calculate_texture_similarity(self, volume, reference_patches):
        """Calculate texture similarity using gradient analysis"""
        def calculate_gradient_energy(image):
            grad_x = np.gradient(image, axis=1)
            grad_y = np.gradient(image, axis=0)
            return np.mean(grad_x**2 + grad_y**2)

        vol_energy = calculate_gradient_energy(volume[:, :, volume.shape[2]//2])
        ref_energy = calculate_gradient_energy(reference_patches[0] if reference_patches.ndim == 3 else reference_patches)

        return 1.0 - abs(vol_energy - ref_energy) / max(vol_energy, ref_energy)

    def _estimate_composition_similarity(self, volume, target_composition):
        """Estimate composition similarity without full labeling"""
        # Use intensity percentiles as proxy for composition
        percentiles = np.percentile(volume, [20, 40, 60, 80])
        intensity_ranges = [
            np.mean(volume < percentiles[0]),
            np.mean((volume >= percentiles[0]) & (volume < percentiles[1])),
            np.mean((volume >= percentiles[1]) & (volume < percentiles[2])),
            np.mean((volume >= percentiles[2]) & (volume < percentiles[3])),
            np.mean(volume >= percentiles[3])
        ]

        # Normalize to match target composition scale
        total = sum(intensity_ranges)
        estimated_comp = [r/total * 100 for r in intensity_ranges]

        # Simple similarity calculation
        target_vals = list(target_composition.values())
        similarity = 1.0 - np.mean(np.abs(np.array(estimated_comp) - np.array(target_vals))) / 100.0

        return max(0, similarity)  # Ensure non-negative

    def _create_geological_depth_profile(self, target_composition, depth):
        """Create geological depth profile based on composition"""
        profile = np.ones(depth)

        clay_ratio = target_composition.get('Clay', 10) / 100.0

        # Create bedding pattern based on clay content
        if clay_ratio > 0.15:
            # Stronger bedding for clay-rich shales
            for i in range(depth):
                profile[i] = 0.85 + 0.3 * np.sin(i * 0.15) * clay_ratio
        else:
            # More homogeneous for other shales
            profile = np.ones(depth) * (0.9 + 0.1 * np.random.random())

        return profile

    def _add_composition_features(self, volume, target_composition):
        """Add composition-specific geological features"""
        clay_ratio = target_composition.get('Clay', 10) / 100.0
        silicate_ratio = target_composition.get('Silicates', 70) / 100.0

        # Add features based on composition
        if clay_ratio > 0.2:
            volume = self._add_clay_features(volume, clay_ratio)
        elif silicate_ratio > 0.6:
            volume = self._add_silicate_features(volume, silicate_ratio)

        return volume

    def _add_clay_features(self, volume, clay_ratio):
        """Add clay-specific geological features"""
        H, W, D = volume.shape
        enhanced = volume.copy()

        # Add stronger horizontal bedding for clay-rich shales
        for z in range(D):
            bedding_strength = 0.25 * clay_ratio * np.sin(z * 0.12)
            enhanced[:, :, z] = enhanced[:, :, z] * (1.0 + bedding_strength)

        return enhanced

    def _add_silicate_features(self, volume, silicate_ratio):
        """Add silicate-specific geological features"""
        H, W, D = volume.shape
        enhanced = volume.copy()

        # Add grain-like structures for silicate-rich shales
        n_grains = int(80 * silicate_ratio)
        for _ in range(n_grains):
            x, y, z = np.random.randint(3, W-3), np.random.randint(3, H-3), np.random.randint(3, D-3)
            radius = np.random.randint(2, 5)
            intensity = 0.8 + 0.2 * np.random.random()

            for i in range(-radius, radius+1):
                for j in range(-radius, radius+1):
                    for k in range(-1, 2):  # Slightly elongated in Z
                        xi, yj, zk = x+i, y+j, z+k
                        if 0 <= xi < W and 0 <= yj < H and 0 <= zk < D:
                            dist = np.sqrt(i**2 + j**2 + (k*2)**2)  # Z elongation
                            if dist <= radius:
                                factor = 1.0 - (dist / radius)
                                enhanced[yj, xi, zk] = max(enhanced[yj, xi, zk], intensity * factor)

        return enhanced

    def save_comprehensive_results(self, scattered_vol, continuous_vol, realistic_vol,
                                 complete_continuous_vol, enhanced_continuous_vol, slicegan_vol,
                                 sample_name, target_composition,
                                 comp_scattered, comp_continuous, comp_realistic,
                                 comp_complete_continuous, comp_enhanced_continuous, comp_slicegan):
        """Save comprehensive results for all models"""
        print("ðŸ’¾ Saving comprehensive results for all 6 models...")

        # Save each model individually
        models_data = [
            (scattered_vol, comp_scattered, "scattered"),
            (continuous_vol, comp_continuous, "continuous"),
            (realistic_vol, comp_realistic, "realistic"),
            (complete_continuous_vol, comp_complete_continuous, "complete_continuous"),
            (enhanced_continuous_vol, comp_enhanced_continuous, "enhanced_continuous"),
            (slicegan_vol, comp_slicegan, "slicegan")
        ]

        for volume, composition, model_name in models_data:
            if volume is not None:
                self.save_final_results(volume, f"{sample_name}_{model_name}",
                                      CompositionTools.similarity_score(composition, target_composition),
                                      CompositionTools.similarity_score(composition, target_composition),
                                      target_composition, composition)

        print("âœ… Comprehensive results saved for all 6 models")
    # ---- Kept from your original (helper generators) ----
    def create_realistic_sem_image(self, size=(1024, 1024)):
        print("ðŸŽ¨ Creating realistic SEM image...")
        h, w = size
        base = np.random.rand(h, w).astype(np.float32) * 0.3
        for _ in range(20):
            x, y = np.random.randint(0, w), np.random.randint(0, h)
            length = np.random.randint(200, 500)
            width = np.random.randint(15, 50)
            angle = np.random.uniform(0, 2*np.pi)
            for i in range(length):
                for j in range(-width//2, width//2):
                    xi = int(x + i * np.cos(angle) + j * np.sin(angle))
                    yi = int(y + i * np.sin(angle) - j * np.cos(angle))
                    if 0 <= xi < w and 0 <= yi < h:
                        dist_factor = 1.0 - abs(j) / (width/2)
                        intensity = 0.8 + 0.2 * dist_factor
                        base[yi, xi] = max(base[yi, xi], intensity)
        clay_mask = np.random.rand(h, w) > 0.7
        base[clay_mask] = np.maximum(base[clay_mask], 0.5)
        for _ in range(100):
            x, y = np.random.randint(0, w), np.random.randint(0, h)
            radius = np.random.randint(3, 15)
            y_coords, x_coords = np.ogrid[-y:h-y, -x:w-x]
            mask = x_coords*x_coords + y_coords*y_coords <= radius*radius
            base[mask] = np.minimum(base[mask], 0.3)
        base = cv2.GaussianBlur(base, (7, 7), 2.0)
        base = (base - base.min()) / (base.max() - base.min())
        print("âœ… Realistic SEM image created")
        return base
    def create_model_comparison_visualization(self, scattered_vol, continuous_vol, realistic_vol,
                                            sample_name, target_comp,
                                            comp_scattered, comp_continuous, comp_realistic):
        """Create comprehensive comparison of all three models"""
        self.visualizer.create_model_comparison_visualization(
            scattered_vol, continuous_vol, realistic_vol,
            sample_name, target_comp,
            comp_scattered, comp_continuous, comp_realistic
        )
    def create_output_directories(self):
        """Create all necessary output directories with proper path handling"""
        directories = [
            'Output/3D_Models', 'Output/Visualizations', 'Output/Abaqus',
            'Output/Training_Data', 'Output/Quality_Metrics', 'Output/Colored', 'Output/TIFF_Stacks'
        ]

        for directory in directories:
            full_path = os.path.join(self.base_path, directory)
            # Create directory and all parent directories
            os.makedirs(full_path, exist_ok=True)
            print(f"ðŸ“ Created: {full_path}")

        # Create sample-specific subdirectories
        sample_dirs = ['Training_Data', 'Quality_Metrics']
        for sample_dir in sample_dirs:
            for sample_name in ['sample1', 'sample6']:  # Add other samples as needed
                sample_path = os.path.join(self.base_path, 'Output', sample_dir, sample_name)
                os.makedirs(sample_path, exist_ok=True)
                print(f"ðŸ“ Created sample directory: {sample_path}")

    def safe_makedirs(self, path):
        """Safely create directory if it doesn't exist"""
        try:
            os.makedirs(path, exist_ok=True)
            return True
        except Exception as e:
            print(f"âŒ Error creating directory {path}: {e}")
            return False
    def save_final_results(self, volume, sample_name, comp_score, similarity_score, target_comp, volume_comp):
        print("ðŸ’¾ Saving final results with comprehensive analysis...")

        # ðŸ”§ NEW: Ensure output directory exists before saving
        output_dir = os.path.join(self.base_path, 'Output/3D_Models')
        safe_makedirs(output_dir)

        self.save_training_patches_properly(self.patches, sample_name)
        self.save_quality_metrics(volume, sample_name, comp_score, target_comp, volume_comp)
        # Calculate equivalent modulus and comprehensive metrics
        youngs_modulus_dict = {
            'Silicates': 89.6, 'Carbonate': 74.6, 'Clay': 22.3,
            'Kerogen': 9.2, 'Others': 12.392
        }

        equivalent_modulus = calculate_equivalent_modulus_fem(volume, youngs_modulus_dict)
        volume_stats = calculate_volume_statistics(volume)
        spatial_corr = calculate_spatial_correlation(volume)
        connectivity = calculate_phase_connectivity(volume)

        volume_path = os.path.join(self.base_path, 'Output/3D_Models', f'{sample_name}_3d_volume.npz')

        # ðŸ”§ NEW: Safe save with error handling
        try:
            np.savez_compressed(volume_path, volume=volume, composition=volume_comp)
            print(f"âœ… Volume saved: {volume_path}")
        except Exception as e:
            print(f"âŒ Error saving volume: {e}")
            # Retry after ensuring directory exists
            safe_makedirs(output_dir)
            np.savez_compressed(volume_path, volume=volume, composition=volume_comp)
            print(f"âœ… Volume saved after retry: {volume_path}")

        metadata = {
            'sample_name': sample_name,
            'volume_shape': list(volume.shape),
            'composition_similarity': comp_score,
            'equivalent_modulus_gpa': equivalent_modulus,
            'volume_statistics': volume_stats,
            'spatial_correlation': spatial_corr,
            'phase_connectivity': connectivity,
            'target_composition': target_comp,
            'achieved_composition': volume_comp,
            'timestamp': pd.Timestamp.now().isoformat(),
            'methodology': 'SliceGAN+ResNet18+FEM+ComprehensiveAnalysis',
            'reference': 'A Data-Driven Approach to Generating Stochastic Mesoscale 3D Shale Volume Elements From 2D SEM Images'
        }

        metadata_path = os.path.join(self.base_path, 'Output/3D_Models', f'{sample_name}_metadata.json')

        # ðŸ”§ NEW: Safe metadata save
        try:
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, cls=NumpyEncoder)
            print(f"âœ… Metadata saved: {metadata_path}")
        except Exception as e:
            print(f"âŒ Error saving metadata: {e}")

        # FIX: Save training patches properly
        try:
            training_dir = os.path.join(self.base_path, 'Output/Training_Data', sample_name)
            safe_makedirs(training_dir)

            # Get patches from the current process
            sem_data = self.sem_processor.load_sample_images(sample_name)
            if sem_data['mineral_map'] is not None:
                patches, _ = self.create_10x10_patches(sem_data['mineral_map'])

                # Save first 16 patches
                for i in range(min(16, len(patches))):
                    patch = patches[i].squeeze()
                    if patch.ndim == 2:  # Grayscale
                        patch_img = (patch * 255).astype(np.uint8)
                        cv2.imwrite(os.path.join(training_dir, f'patch_{i:02d}.png'), patch_img)
                    else:  # Multi-channel
                        patch_img = (patch * 255).astype(np.uint8)
                        if patch_img.shape[-1] == 1:
                            patch_img = patch_img.squeeze()
                        cv2.imwrite(os.path.join(training_dir, f'patch_{i:02d}.png'), patch_img)

            print(f"âœ… Training data saved: {training_dir}")
        except Exception as e:
            print(f"âš ï¸  Could not save training data: {e}")

        print(f"âœ… Results saved:\n   - 3D volume: {volume_path}\n   - Metadata: {metadata_path}")
        print(f"ðŸ“Š Equivalent modulus: {equivalent_modulus:.2f} GPa")
        print(f"ðŸ“ˆ Volume porosity: {volume_stats['porosity_05']:.1%}")
        print(f"ðŸ”— Spatial anisotropy: {spatial_corr['anisotropy_index']:.3f}")
    def create_enhanced_10x10_patches(self, image, target_composition, patch_size=64):
        """Create composition-aware 10x10 patches"""
        if image is None:
            return None, None

        h, w = image.shape[:2]
        if h != 1024 or w != 1024:
            image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_AREA)

        patches = []
        positions = []

        # Calculate stride for exactly 10 patches in each dimension
        stride_h = (h - patch_size) // 9
        stride_w = (w - patch_size) // 9

        for i in range(10):
            for j in range(10):
                y = i * stride_h
                x = j * stride_w
                patch = image[y:y+patch_size, x:x+patch_size]

                # Enhance patch based on composition
                enhanced_patch = self.enhance_patch_composition(patch, target_composition)
                patches.append(enhanced_patch)
                positions.append((y, x))

        patches = np.array(patches)
        if patches.ndim == 3:
            patches = patches[..., None]  # Add channel dimension

        print(f"âœ… Created {len(patches)} composition-aware patches")
        return patches, positions

    def enhance_patch_composition(self, patch, target_composition):
        """Enhance patch based on target composition"""
        if target_composition is None:
            return patch

        clay_ratio = target_composition.get('Clay', 10) / 100.0

        # Enhance texture based on clay content
        if clay_ratio > 0.15:
            # Apply stronger texture enhancement for clay-rich samples
            patch_uint8 = (patch * 255).astype(np.uint8)
            patch_enhanced = cv2.bilateralFilter(patch_uint8, 9, 75, 75)
            patch = patch_enhanced.astype(np.float32) / 255.0

        return patch

    def enhance_intensity_matching(self, volume, sem_reference, target_composition):
        """Enhanced intensity matching with composition awareness"""
        if sem_reference is None:
            return volume

        # Convert reference to proper format
        if sem_reference.ndim == 3:
            ref = np.mean(sem_reference, axis=2)
        else:
            ref = sem_reference

        if ref.max() > 2.0:
            ref = ref / ref.max()

        ref_resized = cv2.resize(ref, (volume.shape[1], volume.shape[0]), interpolation=cv2.INTER_AREA)

        # Composition-aware matching
        clay_ratio = target_composition.get('Clay', 10) / 100.0 if target_composition else 0.1

        for z in range(volume.shape[2]):
            if clay_ratio > 0.2:
                # More aggressive matching for clay-rich shales
                volume[:, :, z] = robust_minmax_match(volume[:, :, z], ref_resized)
            else:
                # Gentle matching for other shales
                vol_slice = volume[:, :, z]
                matched = (vol_slice - np.mean(vol_slice)) / np.std(vol_slice) * np.std(ref_resized) + np.mean(ref_resized)
                volume[:, :, z] = np.clip(matched, 0, 1)

        return volume

    def enhance_geological_realism(self, volume, target_composition):
        """Enhance geological realism based on composition"""
        print("ðŸ”ï¸ Enhancing geological realism...")

        H, W, D = volume.shape
        enhanced = volume.copy()

        # Add composition-dependent features
        clay_ratio = target_composition.get('Clay', 10) / 100.0 if target_composition else 0.1
        silicate_ratio = target_composition.get('Silicates', 70) / 100.0 if target_composition else 0.7

        # Clay-rich: more layering and fractures
        if clay_ratio > 0.15:
            enhanced = self._add_clay_features(enhanced, clay_ratio)

        # Silicate-rich: more homogeneous with grains
        if silicate_ratio > 0.6:
            enhanced = self._add_silicate_features(enhanced, silicate_ratio)

        # Final coherence
        enhanced = ndimage.gaussian_filter(enhanced, sigma=(0.8, 0.8, 0.6))

        return clip01(enhanced)

    def _add_clay_features(self, volume, clay_ratio):
        """Add clay-specific geological features"""
        H, W, D = volume.shape
        enhanced = volume.copy()

        # Strong horizontal bedding
        for z in range(D):
            bedding_strength = 0.2 * clay_ratio * np.sin(z * 0.15)
            enhanced[:, :, z] = enhanced[:, :, z] * (1.0 + bedding_strength)

        # Add micro-fractures
        fracture_density = int(50 * clay_ratio)
        for _ in range(fracture_density):
            x = np.random.randint(10, W-10)
            length = np.random.randint(20, 60)
            for i in range(length):
                xi = min(x + i, W-1)
                for y in range(H):
                    if np.random.random() < 0.3:
                        enhanced[y, xi, :] *= 0.7

        return enhanced

    def _add_silicate_features(self, volume, silicate_ratio):
        """Add silicate-specific geological features"""
        H, W, D = volume.shape
        enhanced = volume.copy()

        # Add grain-like structures
        n_grains = int(100 * silicate_ratio)
        for _ in range(n_grains):
            x, y, z = np.random.randint(5, W-5), np.random.randint(5, H-5), np.random.randint(5, D-5)
            radius = np.random.randint(2, 6)
            intensity = 0.7 + 0.3 * np.random.random()

            for i in range(-radius, radius+1):
                for j in range(-radius, radius+1):
                    for k in range(-1, 2):  # Flat grains
                        xi, yj, zk = x+i, y+j, z+k
                        if 0 <= xi < W and 0 <= yj < H and 0 <= zk < D:
                            dist = np.sqrt(i**2 + j**2)
                            if dist <= radius:
                                factor = 1.0 - (dist / radius)
                                enhanced[yj, xi, zk] = max(enhanced[yj, xi, zk], intensity * factor)

        return enhanced
    def _create_enhanced_continuous_with_geology(self, sem_reference, target_composition):
        """Create enhanced continuous model with geological features"""
        V = 128
        volume = np.zeros((V, V, V), dtype=np.float32)

        # Create base with strong geological continuity
        base_profile = self._create_geological_depth_profile(target_composition, V)

        for z in range(V):
            if z == 0:
                # Start with SEM reference pattern
                if sem_reference is not None:
                    if sem_reference.ndim == 3:
                        sem_ref = np.mean(sem_reference, axis=2)
                    else:
                        sem_ref = sem_reference
                    base_slice = cv2.resize(sem_ref, (V, V))
                else:
                    base_slice = np.random.rand(V, V) * 0.5 + 0.3
            else:
                # Very strong correlation with previous slice (95%)
                base_slice = 0.95 * volume[:, :, z-1] + 0.05 * np.random.rand(V, V)

            # Apply geological depth profile
            volume[:, :, z] = base_slice * base_profile[z]

        # Add composition-specific features
        volume = self._add_composition_features(volume, target_composition)

        return clip01(volume)

    def _calculate_intensity_similarity(self, volume, reference_patches):
        """Calculate intensity distribution similarity"""
        vol_mean = np.mean(volume)
        ref_mean = np.mean(reference_patches)
        vol_std = np.std(volume)
        ref_std = np.std(reference_patches)

        mean_sim = 1.0 - abs(vol_mean - ref_mean)
        std_sim = 1.0 - abs(vol_std - ref_std) / max(vol_std, ref_std)

        return (mean_sim + std_sim) / 2.0

    def _calculate_texture_similarity(self, volume, reference_patches):
        """Calculate texture similarity using gradient analysis"""
        def calculate_gradient_energy(image):
            grad_x = np.gradient(image, axis=1)
            grad_y = np.gradient(image, axis=0)
            return np.mean(grad_x**2 + grad_y**2)

        vol_energy = calculate_gradient_energy(volume[:, :, volume.shape[2]//2])
        ref_energy = calculate_gradient_energy(reference_patches[0] if reference_patches.ndim == 3 else reference_patches)

        return 1.0 - abs(vol_energy - ref_energy) / max(vol_energy, ref_energy)

    def _estimate_composition_similarity(self, volume, target_composition):
        """Estimate composition similarity without full labeling"""
        # Use intensity percentiles as proxy for composition
        percentiles = np.percentile(volume, [20, 40, 60, 80])
        intensity_ranges = [
            np.mean(volume < percentiles[0]),
            np.mean((volume >= percentiles[0]) & (volume < percentiles[1])),
            np.mean((volume >= percentiles[1]) & (volume < percentiles[2])),
            np.mean((volume >= percentiles[2]) & (volume < percentiles[3])),
            np.mean(volume >= percentiles[3])
        ]

        # Normalize to match target composition scale
        total = sum(intensity_ranges)
        estimated_comp = [r/total * 100 for r in intensity_ranges]

        # Simple similarity calculation
        target_vals = list(target_composition.values())
        similarity = 1.0 - np.mean(np.abs(np.array(estimated_comp) - np.array(target_vals))) / 100.0

        return max(0, similarity)  # Ensure non-negative

    def _create_geological_depth_profile(self, target_composition, depth):
        """Create geological depth profile based on composition"""
        profile = np.ones(depth)

        clay_ratio = target_composition.get('Clay', 10) / 100.0

        # Create bedding pattern based on clay content
        if clay_ratio > 0.15:
            # Stronger bedding for clay-rich shales
            for i in range(depth):
                profile[i] = 0.85 + 0.3 * np.sin(i * 0.15) * clay_ratio
        else:
            # More homogeneous for other shales
            profile = np.ones(depth) * (0.9 + 0.1 * np.random.random())

        return profile

    def _add_composition_features(self, volume, target_composition):
        """Add composition-specific geological features"""
        clay_ratio = target_composition.get('Clay', 10) / 100.0
        silicate_ratio = target_composition.get('Silicates', 70) / 100.0

        # Add features based on composition
        if clay_ratio > 0.2:
            volume = self._add_clay_features(volume, clay_ratio)
        elif silicate_ratio > 0.6:
            volume = self._add_silicate_features(volume, silicate_ratio)

        return volume

    def save_comprehensive_results(self, scattered_vol, continuous_vol, realistic_vol,
                                 complete_continuous_vol, enhanced_continuous_vol, slicegan_vol, robust_realistic_vol,
                                 sample_name, target_composition,
                                 comp_scattered, comp_continuous, comp_realistic,
                                 comp_complete_continuous, comp_enhanced_continuous, comp_slicegan, comp_robust_realistic):
        """Save comprehensive results for all 7 models"""
        print("ðŸ’¾ Saving comprehensive results for all 7 models...")

        # Save each model individually
        models_data = [
            (scattered_vol, comp_scattered, "scattered"),
            (continuous_vol, comp_continuous, "continuous"),
            (realistic_vol, comp_realistic, "realistic"),
            (complete_continuous_vol, comp_complete_continuous, "complete_continuous"),
            (enhanced_continuous_vol, comp_enhanced_continuous, "enhanced_continuous"),
            (slicegan_vol, comp_slicegan, "slicegan"),
            (robust_realistic_vol, comp_robust_realistic, "robust_realistic")
        ]

        for volume, composition, model_name in models_data:
            if volume is not None:
                self.save_final_results(volume, f"{sample_name}_{model_name}",
                                      CompositionTools.similarity_score(composition, target_composition),
                                      CompositionTools.similarity_score(composition, target_composition),
                                      target_composition, composition)

        print("âœ… Comprehensive results saved for all 7 models")
    def create_10x10_patches(self, image, patch_size=64):
        """Create exactly 100 patches (10x10 grid) from 1024x1024 image"""
        if image is None:
            return None, None

        h, w = image.shape[:2]
        if h != 1024 or w != 1024:
            image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_AREA)

        patches = []
        positions = []

        # Calculate stride for exactly 10 patches in each dimension
        stride_h = (h - patch_size) // 9
        stride_w = (w - patch_size) // 9

        for i in range(10):
            for j in range(10):
                y = i * stride_h
                x = j * stride_w
                patch = image[y:y+patch_size, x:x+patch_size]
                patches.append(patch)
                positions.append((y, x))

        patches = np.array(patches)
        if patches.ndim == 3:
            patches = patches[..., None]  # Add channel dimension

        print(f"âœ… Created exactly {len(patches)} patches (10x10 grid)")
        return patches, positions
    def _create_enhanced_continuous_volume(self, sem_reference, target_composition):
        """Create enhanced continuous volume as fallback"""
        V = 128
        volume = np.zeros((V, V, V), dtype=np.float32)

        # Create base pattern with perfect continuity
        z_profile = np.linspace(0.8, 1.2, V)
        for z in range(V):
            # Base pattern with strong horizontal correlation
            if z == 0:
                base_slice = np.random.rand(V, V) * 0.5 + 0.3
            else:
                # Very strong correlation with previous slice
                base_slice = 0.95 * volume[:, :, z-1] + 0.05 * np.random.rand(V, V)

            # Apply SEM reference if available
            if sem_reference is not None:
                if sem_reference.ndim == 3:
                    sem_ref = np.mean(sem_reference, axis=2)
                else:
                    sem_ref = sem_reference
                sem_resized = cv2.resize(sem_ref, (V, V))
                base_slice = 0.7 * base_slice + 0.3 * sem_resized

            volume[:, :, z] = base_slice * z_profile[z]

        # Apply composition-based intensity mapping
        total = sum(target_composition.values())
        cumulative = 0
        thresholds = []

        phases = ['Silicates', 'Carbonate', 'Clay', 'Kerogen', 'Others']
        for phase in phases:
            cumulative += target_composition.get(phase, 0) / total
            thresholds.append(cumulative)

        # Apply thresholds
        volume_flat = volume.ravel()
        sorted_idx = np.argsort(volume_flat)

        current_idx = 0
        for i, threshold in enumerate(thresholds):
            phase_count = int(len(volume_flat) * threshold)
            end_idx = min(current_idx + phase_count, len(volume_flat))

            phase_intensity = 0.1 + 0.8 * (i / len(thresholds))
            volume_flat[sorted_idx[current_idx:end_idx]] = phase_intensity
            current_idx = end_idx

        volume = volume_flat.reshape(volume.shape)

        # Final smoothing for perfect continuity
        volume = ndimage.gaussian_filter(volume, sigma=(1.5, 1.5, 1.0))

        return clip01(volume)
    def enhance_3d_continuity(self, volume):
        """Enhanced 3D continuity with proper structure preservation for Abaqus"""
        print("ðŸ”„ Enhancing 3D continuity for better quality...")

        # Step 1: Stronger 3D Gaussian smoothing to improve continuity
        vol_smooth = ndimage.gaussian_filter(volume, sigma=(1.5, 1.5, 1.0))

        # Step 2: Apply anisotropic diffusion for edge preservation
        for z in range(volume.shape[2]):
            slice_2d = volume[:, :, z]

            # Use bilateral filter for better edge preservation
            slice_uint8 = (slice_2d * 255).astype(np.uint8)
            slice_bilateral = cv2.bilateralFilter(slice_uint8, d=9, sigmaColor=75, sigmaSpace=75)
            vol_smooth[:, :, z] = slice_bilateral.astype(np.float32) / 255.0

        # Step 3: Enhance contrast significantly
        for z in range(volume.shape[2]):
            slice_2d = vol_smooth[:, :, z]
            slice_uint8 = (slice_2d * 255).astype(np.uint8)

            # Apply strong CLAHE for better contrast
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
            slice_enhanced = clahe.apply(slice_uint8)
            vol_smooth[:, :, z] = slice_enhanced.astype(np.float32) / 255.0

        # Step 4: More aggressive blending to preserve original texture
        local_std = ndimage.generic_filter(volume, np.std, size=5)
        blend_weights = np.clip(local_std * 15, 0.4, 0.8)  # Preserve more original texture

        result = blend_weights * volume + (1 - blend_weights) * vol_smooth

        # Step 5: Final coherence with stronger smoothing
        result = ndimage.gaussian_filter(result, sigma=(0.8, 0.8, 0.5))

        return clip01(result)
    def _create_robust_realistic_shale_volume(self, sem_reference, target_composition):
        """Create highly realistic 3D shale volume with geological complexity"""
        V = 128
        print("ðŸ”ï¸ Creating robust realistic shale volume with geological complexity...")

        # Start with base geological structure
        volume = self._create_base_geological_structure(V, target_composition)

        # Add multi-scale geological features
        volume = self._add_multiscale_geological_features(volume, target_composition)

        # Enhance with SEM-like texture
        volume = self._enhance_sem_like_texture(volume, sem_reference)

        # Add realistic mineral distribution
        volume = self._add_realistic_mineral_distribution(volume, target_composition)

        # Final geological refinement
        volume = self._apply_geological_refinement(volume)

        print("âœ… Robust realistic shale volume created")
        return clip01(volume)

    def _generate_robust_realistic_model(self, sem_reference, target_composition):
        """Generate robust realistic geological model with enhanced features"""
        print("ðŸ”¹ Creating robust realistic geological model...")

        # Use the existing robust realistic shale volume method
        robust_vol = self._create_robust_realistic_shale_volume(sem_reference, target_composition)

        # Apply additional enhancements for robustness
        robust_vol = self._apply_robust_enhancements(robust_vol, target_composition)

        return robust_vol

    def _apply_robust_enhancements(self, volume, target_composition):
        """Apply robust enhancements to improve model quality"""
        print("ðŸ”¹ Applying robust enhancements...")

        # Enhanced geological features
        volume = self._enhance_stratification(volume, strength=0.7)
        volume = self._optimize_phase_distribution(volume, target_composition)
        volume = self._smooth_phase_transitions(volume)

        # Final quality refinement
        volume = self._final_quality_refinement(volume)

        return volume

    def _enhance_stratification(self, volume, strength=0.7):
        """Enhanced stratigraphic layering"""
        # Create horizontal stratification pattern
        z_coords = np.linspace(0, 2*np.pi, volume.shape[2])
        stratification = np.sin(z_coords * 3)

        # Apply stratification influence
        for z in range(volume.shape[2]):
            layer_influence = 1.0 + strength * stratification[z] * 0.3
            volume[:, :, z] = np.clip(volume[:, :, z] * layer_influence, 0, 1)

        return volume

    def _optimize_phase_distribution(self, volume, target_composition):
        """Optimize phase distribution to better match target composition"""
        # Simple histogram matching approach
        target_bins = np.linspace(0, 1, 6)

        # Calculate target cumulative distribution based on composition
        comp_values = list(target_composition.values())
        comp_cumsum = np.cumsum([0] + [c/100 for c in comp_values])

        # Apply histogram specification
        current_hist, bins = np.histogram(volume.flatten(), bins=256, range=(0, 1))
        current_cdf = current_hist.cumsum()
        current_cdf = current_cdf / current_cdf[-1]

        # Map to target distribution
        target_values = np.interp(current_cdf, comp_cumsum, target_bins)
        volume_mapped = np.interp(volume.flatten(), bins[:-1], target_values)
        volume_optimized = volume_mapped.reshape(volume.shape)

        return np.clip(volume_optimized, 0, 1)

    def _smooth_phase_transitions(self, volume):
        """Smooth transitions between different phases"""
        from scipy import ndimage

        # Apply gentle Gaussian smoothing
        volume_smoothed = ndimage.gaussian_filter(volume, sigma=0.8)

        # Preserve edges while smoothing interiors
        edges = ndimage.sobel(volume)
        edge_mask = np.abs(edges) > 0.1

        # Blend original and smoothed based on edges
        result = np.where(edge_mask, volume, volume_smoothed)

        return np.clip(result, 0, 1)

    def _final_quality_refinement(self, volume):
        """Final quality refinement for robust model"""
        # Apply bilateral filter for edge-preserving smoothing
        result = volume.copy()
        for z in range(volume.shape[2]):
            slice_2d = volume[:, :, z]
            slice_uint8 = (slice_2d * 255).astype(np.uint8)
            slice_bilateral = cv2.bilateralFilter(slice_uint8, d=9, sigmaColor=75, sigmaSpace=75)
            result[:, :, z] = slice_bilateral.astype(np.float32) / 255.0

        # Final contrast enhancement
        result = np.clip(result * 1.1 - 0.05, 0, 1)

        return result

    def _create_base_geological_structure(self, size, target_composition):
        """Create base geological structure with proper bedding and fractures"""
        volume = np.zeros((size, size, size), dtype=np.float32)

        # Create geological depth profile
        depth_profile = self._create_geological_depth_profile(target_composition, size)

        # Generate base geological layers with natural variation
        for z in range(size):
            if z == 0:
                # Start with random geological pattern
                layer = np.random.rand(size, size) * 0.3 + 0.4
            else:
                # Strong correlation with previous layer + geological variation
                base_correlation = 0.92  # Very strong vertical continuity
                geological_variation = 0.08 * np.random.rand(size, size)
                layer = base_correlation * volume[:, :, z-1] + geological_variation

            # Apply depth-dependent geological effects
            layer = layer * depth_profile[z]

            # Add natural layer boundaries
            layer_boundary = 0.1 * np.sin(z * 0.15 + np.random.rand() * 2)
            layer = layer * (0.9 + layer_boundary)

            volume[:, :, z] = layer

        return volume

    def _add_multiscale_geological_features(self, volume, target_composition):
        """Add multi-scale geological features"""
        print("   Adding multi-scale geological features...")

        # 1. Large-scale bedding structures
        volume = self._add_bedding_structures(volume)

        # 2. Medium-scale fracture networks
        volume = self._add_fracture_networks(volume, target_composition)

        # 3. Small-scale pore systems
        volume = self._add_pore_systems(volume, target_composition)

        # 4. Mineral grain boundaries
        volume = self._add_mineral_boundaries(volume)

        return volume

    def _add_bedding_structures(self, volume):
        """Add realistic bedding structures"""
        H, W, D = volume.shape

        # Multiple bedding scales
        for scale in [0.02, 0.05, 0.1]:
            for z in range(D):
                # Variable bedding thickness and intensity
                bedding_freq = scale * (0.8 + 0.4 * np.random.rand())
                bedding_amp = 0.15 * scale * (0.7 + 0.6 * np.random.rand())

                bedding_pattern = bedding_amp * np.sin(z * bedding_freq + np.random.rand() * np.pi)

                # Apply with some horizontal variation
                for x in range(W):
                    horizontal_variation = 0.1 * np.sin(x * 0.1)
                    volume[:, x, z] = volume[:, x, z] * (0.95 + bedding_pattern + horizontal_variation)

        return volume

    def _add_fracture_networks(self, volume, target_composition):
        """Add realistic fracture networks"""
        H, W, D = volume.shape

        clay_ratio = target_composition.get('Clay', 10) / 100.0

        # Vertical fractures (more common in brittle shales)
        if clay_ratio < 0.2:  # More fractures in silicate-rich shales
            num_fractures = np.random.randint(8, 15)
        else:  # Fewer fractures in clay-rich shales
            num_fractures = np.random.randint(3, 8)

        for _ in range(num_fractures):
            # Fracture position and characteristics
            x_pos = np.random.randint(5, W-5)
            z_start = np.random.randint(0, D-20)
            fracture_height = np.random.randint(10, min(40, D-z_start))
            fracture_width = np.random.randint(1, 3)

            # Fracture intensity varies with depth
            for i in range(fracture_width):
                x = min(x_pos + i, W-1)
                for z in range(z_start, z_start + fracture_height):
                    depth_factor = 1.0 - 0.5 * (z - z_start) / fracture_height
                    fracture_intensity = 0.6 * depth_factor
                    volume[:, x, z] *= fracture_intensity

        # Horizontal bedding-parallel fractures
        for _ in range(num_fractures // 2):
            z_pos = np.random.randint(10, D-10)
            fracture_length = np.random.randint(15, 30)
            fracture_intensity = 0.7 + 0.3 * np.random.rand()

            for z in range(z_pos, min(z_pos + 2, D)):
                for x in range(0, W, 2):  # Sparse horizontal fractures
                    if np.random.rand() < 0.3:
                        volume[:, x:x+fracture_length, z] *= fracture_intensity

        return volume

    def _add_pore_systems(self, volume, target_composition):
        """Add realistic pore systems"""
        H, W, D = volume.shape

        clay_ratio = target_composition.get('Clay', 10) / 100.0
        kerogen_ratio = target_composition.get('Kerogen', 2) / 100.0

        # Determine porosity based on composition
        if clay_ratio > 0.15:
            porosity = 0.08 + 0.04 * clay_ratio  # Higher porosity in clay-rich shales
        else:
            porosity = 0.05 + 0.02 * kerogen_ratio  # Moderate porosity

        # Add random pore distribution
        num_pores = int(porosity * H * W * D)

        for _ in range(num_pores):
            # Pore characteristics
            x = np.random.randint(2, W-2)
            y = np.random.randint(2, H-2)
            z = np.random.randint(2, D-2)
            pore_size = np.random.randint(1, 3)
            pore_intensity = 0.3 + 0.4 * np.random.rand()  # Dark pores

            # Create pore
            for dx in range(-pore_size, pore_size+1):
                for dy in range(-pore_size, pore_size+1):
                    for dz in range(-pore_size, pore_size+1):
                        xi, yi, zi = x+dx, y+dy, z+dz
                        if 0 <= xi < W and 0 <= yi < H and 0 <= zi < D:
                            # Ensure we don't go below 0
                            volume[yi, xi, zi] = max(0, volume[yi, xi, zi] * pore_intensity)

        return volume

    def _add_mineral_boundaries(self, volume):
        """Add realistic mineral grain boundaries"""
        from scipy import ndimage

        H, W, D = volume.shape

        # Create grain-like structures using multiple noise scales
        grain_scale1 = np.random.rand(H, W, D) * 0.1
        grain_scale2 = ndimage.gaussian_filter(np.random.rand(H, W, D), sigma=1.5) * 0.15
        grain_scale3 = ndimage.gaussian_filter(np.random.rand(H, W, D), sigma=0.5) * 0.08

        # Combine grain textures
        grain_texture = grain_scale1 + grain_scale2 + grain_scale3

        # Apply grain boundaries as intensity variations
        volume = volume * (0.85 + 0.3 * grain_texture)

        return volume

    def _enhance_sem_like_texture(self, volume, sem_reference):
        """Enhance volume to look like SEM images"""
        from scipy import ndimage

        print("   Enhancing SEM-like texture...")

        # 1. Add high-frequency SEM-like noise
        hf_noise = np.random.normal(0, 0.03, volume.shape)
        volume = volume + hf_noise

        # 2. Apply anisotropic smoothing (preserves edges like SEM)
        volume = ndimage.gaussian_filter(volume, sigma=0.3)

        # 3. Enhance contrast for SEM-like appearance
        volume = self._adjust_contrast_sem(volume, 1.3, 0.1)

        # 4. Add SEM-specific texture patterns
        volume = self._add_sem_texture_patterns(volume)

        return volume

    def _adjust_contrast_sem(self, volume, contrast_factor, brightness):
        """Adjust contrast and brightness for SEM-like appearance"""
        volume = (volume - 0.5) * contrast_factor + 0.5 + brightness
        return np.clip(volume, 0, 1)

    def _add_sem_texture_patterns(self, volume):
        """Add SEM-specific texture patterns"""
        H, W, D = volume.shape

        # Create micro-scale texture variations
        for z in range(D):
            if z % 10 == 0:  # Apply texture every 10 slices
                texture = np.random.rand(H, W) * 0.1 - 0.05
                volume[:, :, z] = volume[:, :, z] + texture

        return np.clip(volume, 0, 1)

    def _add_realistic_mineral_distribution(self, volume, target_composition):
        """Add composition-aware mineral distribution"""
        print("   Adding realistic mineral distribution...")

        # Convert volume to composition-based intensities
        if target_composition:
            # Create mineral probability maps based on composition
            silicate_map = self._create_mineral_probability_map(volume.shape,
                                                               target_composition.get('Silicates', 70) / 100.0)
            clay_map = self._create_mineral_probability_map(volume.shape,
                                                           target_composition.get('Clay', 15) / 100.0)
            carbonate_map = self._create_mineral_probability_map(volume.shape,
                                                               target_composition.get('Carbonate', 10) / 100.0)

            # Combine mineral maps (simplified approach)
            mineral_composite = (silicate_map * 0.6 + clay_map * 0.3 + carbonate_map * 0.4)

            # Blend with original volume
            volume = 0.7 * volume + 0.3 * mineral_composite

        return volume

    def _create_mineral_probability_map(self, shape, mineral_ratio):
        """Create probability map for specific mineral distribution"""
        from scipy import ndimage

        # Base random field
        base = np.random.rand(*shape)

        # Apply smoothing based on mineral type
        if mineral_ratio > 0.3:  # Major minerals get more continuous distribution
            base = ndimage.gaussian_filter(base, sigma=2.0)
        else:  # Minor minerals get more clustered distribution
            base = ndimage.gaussian_filter(base, sigma=1.0)

        # Threshold to achieve desired ratio
        threshold = np.percentile(base, (1 - mineral_ratio) * 100)
        mineral_map = (base > threshold).astype(np.float32)

        return mineral_map

    def _apply_geological_refinement(self, volume):
        """Apply final geological refinement"""
        from scipy import ndimage

        # 1. Gentle smoothing while preserving geological features
        volume = ndimage.gaussian_filter(volume, sigma=0.5)

        # 2. Enhance geological boundaries - FIXED VERSION
        try:
            # Calculate gradient properly
            grad_x, grad_y, grad_z = np.gradient(volume)
            grad_magnitude = np.sqrt(grad_x**2 + grad_y**2 + grad_z**2)
            boundary_enhance = 1.0 + 0.3 * grad_magnitude
            volume = volume * boundary_enhance
        except Exception as e:
            print(f"âš ï¸  Gradient enhancement skipped: {e}")
            # Continue without boundary enhancement

        # 3. Final contrast adjustment
        volume = np.clip(volume * 1.1, 0, 1)

        return volume

    def _create_geological_depth_profile(self, target_composition, depth):
        """Create depth-dependent geological profile"""
        profile = np.ones(depth)

        clay_ratio = target_composition.get('Clay', 10) / 100.0

        # Depth-dependent effects
        for z in range(depth):
            # General compaction trend
            compaction = 1.0 - 0.3 * (z / depth)

            # Composition-specific depth effects
            if clay_ratio > 0.15:
                # Clay-rich: stronger depth variation
                clay_effect = 0.2 * np.sin(z * 0.2) * clay_ratio
            else:
                # Silicate-rich: milder depth variation
                clay_effect = 0.1 * np.sin(z * 0.1)

            profile[z] = compaction + clay_effect

        return np.clip(profile, 0.7, 1.3)
    def save_training_patches_properly(self, patches, sample_name):
        """Properly save training patches to Training_Data folder"""
        try:
            training_dir = os.path.join(self.base_path, 'Output/Training_Data', sample_name)
            os.makedirs(training_dir, exist_ok=True)

            if patches is None or len(patches) == 0:
                print("âš ï¸  No patches to save")
                return

            # Save first 20 patches as PNG
            saved_count = 0
            for i in range(min(20, len(patches))):
                patch = patches[i]

                # Handle different patch dimensions - FIXED
                if patch.ndim == 4:  # (batch, height, width, channels)
                    patch = patch[0] if patch.shape[0] > 0 else patch.squeeze()

                # Ensure we have a 2D image
                if patch.ndim == 3:
                    if patch.shape[-1] == 1:  # Single channel
                        patch = patch.squeeze()
                    elif patch.shape[-1] == 3:  # RGB, convert to grayscale for saving
                        patch = np.mean(patch, axis=-1)
                    else:
                        patch = patch[:, :, 0]  # Take first channel

                # Normalize to 0-255 range
                if patch.dtype != np.uint8:
                    patch_normalized = (patch - np.min(patch)) / (np.max(patch) - np.min(patch) + 1e-8)
                    patch_uint8 = (patch_normalized * 255).astype(np.uint8)
                else:
                    patch_uint8 = patch

                # Ensure 2D for saving
                if patch_uint8.ndim != 2:
                    patch_uint8 = patch_uint8.squeeze()

                if patch_uint8.ndim != 2:
                    print(f"âš ï¸  Patch {i} has wrong dimensions: {patch_uint8.shape}, skipping")
                    continue

                # Save as PNG
                patch_path = os.path.join(training_dir, f'patch_{i:03d}.png')

                # Use PIL for more reliable saving
                try:
                    from PIL import Image
                    img = Image.fromarray(patch_uint8)
                    img.save(patch_path)
                    saved_count += 1
                except Exception as e:
                    # Fallback to OpenCV
                    success = cv2.imwrite(patch_path, patch_uint8)
                    if success:
                        saved_count += 1
                    else:
                        print(f"âš ï¸  Failed to save patch {i} with shape {patch_uint8.shape} and dtype {patch_uint8.dtype}")

            print(f"âœ… Saved {saved_count}/{min(20, len(patches))} training patches to {training_dir}")

        except Exception as e:
            print(f"âŒ Error saving training patches: {e}")
            import traceback
            traceback.print_exc()
    def save_quality_metrics(self, volume, sample_name, composition_similarity, target_comp, achieved_comp):
        """Save comprehensive quality metrics to Quality_Metrics folder"""
        try:
            metrics_dir = os.path.join(self.base_path, 'Output/Quality_Metrics', sample_name)
            os.makedirs(metrics_dir, exist_ok=True)

            # Calculate comprehensive metrics
            youngs_modulus_dict = {
                'Silicates': 89.6, 'Carbonate': 74.6, 'Clay': 22.3,
                'Kerogen': 9.2, 'Others': 12.392
            }

            equivalent_modulus = calculate_equivalent_modulus_fem(volume, youngs_modulus_dict)
            volume_stats = calculate_volume_statistics(volume)
            spatial_corr = calculate_spatial_correlation(volume)
            connectivity = calculate_phase_connectivity(volume)
            moran_i = morans_I(volume)

            # Create metrics dictionary
            metrics = {
                'sample_name': sample_name,
                'composition_similarity': float(composition_similarity),
                'equivalent_modulus_gpa': float(equivalent_modulus),
                'spatial_autocorrelation': float(moran_i),
                'volume_statistics': volume_stats,
                'spatial_correlation': spatial_corr,
                'phase_connectivity': connectivity,
                'target_composition': target_comp,
                'achieved_composition': achieved_comp,
                'volume_dimensions': list(volume.shape),
                'timestamp': pd.Timestamp.now().isoformat()
            }

            # Save as JSON
            metrics_path = os.path.join(metrics_dir, f'{sample_name}_quality_metrics.json')
            with open(metrics_path, 'w') as f:
                json.dump(metrics, f, indent=2, cls=NumpyEncoder)

            # Save as CSV for easy reading
            csv_metrics = {
                'Metric': [
                    'Composition Similarity', 'Equivalent Modulus (GPa)',
                    'Spatial Autocorrelation (Moran\'s I)', 'Mean Intensity',
                    'Std Intensity', 'Porosity (<0.5)', 'X-Correlation',
                    'Y-Correlation', 'Z-Correlation', 'Anisotropy Index'
                ],
                'Value': [
                    f"{composition_similarity:.4f}",
                    f"{equivalent_modulus:.2f}",
                    f"{moran_i:.4f}",
                    f"{volume_stats['mean_intensity']:.4f}",
                    f"{volume_stats['std_intensity']:.4f}",
                    f"{volume_stats['porosity_05']:.4f}",
                    f"{spatial_corr['correlation_x']:.4f}",
                    f"{spatial_corr['correlation_y']:.4f}",
                    f"{spatial_corr['correlation_z']:.4f}",
                    f"{spatial_corr['anisotropy_index']:.4f}"
                ]
            }

            csv_path = os.path.join(metrics_dir, f'{sample_name}_quality_summary.csv')
            pd.DataFrame(csv_metrics).to_csv(csv_path, index=False)

            # Create a visual metrics plot
            self.create_metrics_visualization(metrics, sample_name, metrics_dir)

            print(f"âœ… Quality metrics saved to: {metrics_dir}")

        except Exception as e:
            print(f"âŒ Error saving quality metrics: {e}")
            import traceback
            traceback.print_exc()

    def create_metrics_visualization(self, metrics, sample_name, save_dir):
        """Create visual representation of quality metrics"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Quality Metrics - {sample_name}', fontsize=16, fontweight='bold')

            # Composition comparison
            phases = list(metrics['target_composition'].keys())
            target_vals = [metrics['target_composition'][p] for p in phases]
            achieved_vals = [metrics['achieved_composition'].get(p, 0) for p in phases]

            x = np.arange(len(phases))
            width = 0.35
            axes[0,0].bar(x - width/2, target_vals, width, label='Target', alpha=0.7, color='blue')
            axes[0,0].bar(x + width/2, achieved_vals, width, label='Achieved', alpha=0.7, color='red')
            axes[0,0].set_xlabel('Mineral Phases')
            axes[0,0].set_ylabel('Volume Percentage (%)')
            axes[0,0].set_title('Composition Matching')
            axes[0,0].set_xticks(x)
            axes[0,0].set_xticklabels(phases, rotation=45)
            axes[0,0].legend()
            axes[0,0].grid(True, alpha=0.3)

            # Spatial metrics
            spatial_metrics = ['X-Correlation', 'Y-Correlation', 'Z-Correlation', 'Anisotropy']
            spatial_values = [
                metrics['spatial_correlation']['correlation_x'],
                metrics['spatial_correlation']['correlation_y'],
                metrics['spatial_correlation']['correlation_z'],
                metrics['spatial_correlation']['anisotropy_index']
            ]
            axes[0,1].bar(spatial_metrics, spatial_values, color='green', alpha=0.7)
            axes[0,1].set_title('Spatial Correlation Metrics')
            axes[0,1].set_ylabel('Correlation Coefficient')
            axes[0,1].grid(True, alpha=0.3)

            # Volume statistics
            vol_metrics = ['Mean', 'Std Dev', 'Porosity']
            vol_values = [
                metrics['volume_statistics']['mean_intensity'],
                metrics['volume_statistics']['std_intensity'],
                metrics['volume_statistics']['porosity_05']
            ]
            axes[1,0].bar(vol_metrics, vol_values, color='orange', alpha=0.7)
            axes[1,0].set_title('Volume Statistics')
            axes[1,0].set_ylabel('Value')
            axes[1,0].grid(True, alpha=0.3)

            # Key metrics summary
            key_metrics = ['Comp Similarity', 'Modulus (GPa)', 'Moran\'s I']
            key_values = [
                metrics['composition_similarity'],
                metrics['equivalent_modulus_gpa'],
                metrics['spatial_autocorrelation']
            ]
            axes[1,1].bar(key_metrics, key_values, color='purple', alpha=0.7)
            axes[1,1].set_title('Key Quality Indicators')
            axes[1,1].set_ylabel('Value')
            axes[1,1].grid(True, alpha=0.3)

            plt.tight_layout()
            plot_path = os.path.join(save_dir, f'{sample_name}_metrics_visualization.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            print(f"âš ï¸  Could not create metrics visualization: {e}")
    def enhance_3d_model_quality(self, volume):
        """Enhanced 3D model quality for realistic shale appearance with colorful plates"""
        print("ðŸŽ¨ Enhancing 3D model quality for realistic shale appearance...")

        # Step 1: Create layered structure (like real shale)
        layered_volume = volume.copy()
        H, W, D = volume.shape

        # Add horizontal layering (shale bedding)
        for z in range(D):
            layer_intensity = 0.3 + 0.4 * np.sin(z * 0.1)  # Sinusoidal layering
            layered_volume[:, :, z] = layered_volume[:, :, z] * (0.7 + 0.3 * layer_intensity)

        # Step 2: Add vertical fractures and cracks
        fracture_mask = np.zeros_like(volume, dtype=bool)
        for _ in range(5):  # Add 5 major fracture systems
            fracture_width = np.random.randint(2, 8)
            fracture_length = np.random.randint(20, 60)
            x_start = np.random.randint(0, W - fracture_length)
            y_start = np.random.randint(0, H - fracture_width)
            z_start = np.random.randint(0, D)

            for i in range(fracture_length):
                for j in range(fracture_width):
                    x = min(x_start + i, W-1)
                    y = min(y_start + j, H-1)
                    # Create fracture that extends through depth
                    for z in range(D):
                        if np.random.random() > 0.3:  # 70% chance of fracture at each depth
                            layered_volume[y, x, z] *= 0.3  # Darken fracture areas

        # Step 3: Add mineral vein patterns
        for _ in range(8):  # Add mineral veins
            vein_thickness = np.random.randint(3, 10)
            vein_length = np.random.randint(30, 80)
            angle = np.random.uniform(0, 2*np.pi)

            center_x, center_y = np.random.randint(0, W), np.random.randint(0, H)

            for i in range(vein_length):
                for j in range(-vein_thickness//2, vein_thickness//2):
                    x = int(center_x + i * np.cos(angle) + j * np.sin(angle))
                    y = int(center_y + i * np.sin(angle) - j * np.cos(angle))

                    if 0 <= x < W and 0 <= y < H:
                        # Brighten vein areas (mineral deposits)
                        for z in range(D):
                            if np.random.random() > 0.5:
                                layered_volume[y, x, z] = np.clip(layered_volume[y, x, z] + 0.4, 0, 1)

        # Step 4: Multi-scale filtering for natural texture
        vol_smooth = ndimage.gaussian_filter(layered_volume, sigma=1.0)

        # Step 5: Preserve important geological features
        gradient = np.gradient(layered_volume)
        gradient_magnitude = np.sqrt(sum(g**2 for g in gradient))
        feature_mask = gradient_magnitude > np.percentile(gradient_magnitude, 75)

        # Blend features back
        result = np.where(feature_mask, layered_volume * 0.6 + vol_smooth * 0.4, vol_smooth)

        # Step 6: Add natural geological noise
        geological_noise = np.random.normal(0, 0.05, volume.shape).astype(np.float32)
        result = result + geological_noise

        # Step 7: Ensure proper dynamic range and final smoothing
        result = np.clip(result, 0, 1)
        result = ndimage.gaussian_filter(result, sigma=0.5)

        print("âœ… 3D shale model enhanced with realistic geological features")
        return result
class ShaleAnalysisPipeline:
    """Complete pipeline for shale analysis"""

    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.processor = ShaleProcessor()

    def run_complete_pipeline(self, sem_image_dir):
        """Run complete pipeline from SEM images to modulus prediction"""

        print("Step 1: Processing SEM images...")
        dataset = ShaleDataset(sem_image_dir)
        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

        print("Step 2: Training DCGAN for 2D samples...")
        dcgan_g = DCGANGenerator()
        dcgan_d = DCGANDiscriminator()
        dcgan_trainer = DCGANTrainer(dcgan_g, dcgan_d, self.device)
        g_losses_2d, d_losses_2d = dcgan_trainer.train(dataloader)

        print("Step 3: Generating 2D samples...")
        # Generate 10,000 2D samples
        dcgan_g.eval()
        generated_2d_samples = []
        with torch.no_grad():
            for _ in range(156):  # 64 * 156 â‰ˆ 10,000
                noise = torch.randn(64, 100, 1, 1, device=self.device)
                fake_samples = dcgan_g(noise)
                generated_2d_samples.append(fake_samples.cpu())

        generated_2d_samples = torch.cat(generated_2d_samples, dim=0)[:10000]

        print("Step 4: Training SliceGAN for 3D volumes...")
        slicegan_g = SliceGANGenerator()
        slicegan_ds = [SliceGANDiscriminator() for _ in range(3)]  # 3 discriminators
        slicegan_trainer = SliceGANTrainer(slicegan_g, slicegan_ds, self.device)

        # Create dataloader from generated 2D samples
        generated_dataset = torch.utils.data.TensorDataset(generated_2d_samples)
        generated_dataloader = DataLoader(generated_dataset, batch_size=32, shuffle=True)

        g_losses_3d, d_losses_3d = slicegan_trainer.train(generated_dataloader)

        print("Step 5: Generating 3D volume elements...")
        slicegan_g.eval()
        generated_3d_volumes = []
        with torch.no_grad():
            for _ in range(313):  # 32 * 313 â‰ˆ 10,000
                noise = torch.randn(32, 64, 4, 4, 4, device=self.device)
                fake_volumes = slicegan_g(noise)
                generated_3d_volumes.append(fake_volumes.cpu())

        generated_3d_volumes = torch.cat(generated_3d_volumes, dim=0)[:10000]

        print("Step 6: Calculating equivalent modulus using FEM...")
        # This would interface with Abaqus or use a PyTorch FEM implementation
        equivalent_moduli = self._calculate_equivalent_modulus_fem(generated_3d_volumes)

        print("Step 7: Training ResNet-18 for modulus prediction...")
        # Create dataset for training
        volume_dataset = torch.utils.data.TensorDataset(generated_3d_volumes,
                                                       torch.tensor(equivalent_moduli, dtype=torch.float32))

        train_size = int(0.63 * len(volume_dataset))  # 6,300
        val_size = int(0.27 * len(volume_dataset))    # 2,700
        test_size = len(volume_dataset) - train_size - val_size  # 1,000

        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
            volume_dataset, [train_size, val_size, test_size])

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        resnet = ResNet183D()
        predictor = EquivalentModulusPredictor(resnet, self.device)
        train_losses, val_losses = predictor.train(train_loader, val_loader)

        print("Step 8: Testing prediction model...")
        predictions, actuals = predictor.predict(test_loader)

        # Calculate metrics
        mse = np.mean((predictions.squeeze() - actuals) ** 2)
        relative_error = np.mean(np.abs(predictions.squeeze() - actuals) / actuals) * 100

        print(f"Final Results:")
        print(f"Test MSE: {mse:.4f}")
        print(f"Average Relative Error: {relative_error:.2f}%")

        return {
            'generated_2d_samples': generated_2d_samples,
            'generated_3d_volumes': generated_3d_volumes,
            'equivalent_moduli': equivalent_moduli,
            'predictions': predictions,
            'actuals': actuals,
            'metrics': {'mse': mse, 'relative_error': relative_error}
        }

    def _calculate_equivalent_modulus_fem(self, volumes):
        """Calculate equivalent modulus using FEM (simplified implementation)"""
        # This is a simplified version - in practice, you'd use Abaqus or FEniCS
        # For demonstration, we'll use a rule-of-mixtures approximation

        moduli = []
        for volume in volumes:
            volume_np = volume.squeeze().numpy()

            # Calculate volume fractions of each phase
            unique, counts = np.unique(volume_np, return_counts=True)
            total_voxels = volume_np.size

            # Simplified modulus calculation (rule of mixtures)
            effective_modulus = 0
            for phase_val, count in zip(unique, counts):
                volume_fraction = count / total_voxels
                # Map phase value to Young's modulus
                if phase_val < -0.6:  # Adjust thresholds based on your data
                    youngs_modulus = self.processor.youngs_modulus['kerogen']
                elif phase_val < -0.2:
                    youngs_modulus = self.processor.youngs_modulus['others']
                elif phase_val < 0.2:
                    youngs_modulus = self.processor.youngs_modulus['clay']
                elif phase_val < 0.6:
                    youngs_modulus = self.processor.youngs_modulus['carbonate']
                else:
                    youngs_modulus = self.processor.youngs_modulus['silicate']

                effective_modulus += volume_fraction * youngs_modulus

            moduli.append(effective_modulus)

        return np.array(moduli)
# ============== MAIN ==============
if __name__ == "__main__":
    set_global_seed(42)

    base_path = r"C:\Users\çº¢ç±³\Desktop\Files"
    excel_path = os.path.join(base_path, "Mineral_quant_all_samples.xlsx")

    print("ðŸš€ ADVANCED 3D SHALE VOLUME ELEMENTS GENERATOR")
    print("=" * 70)

    reconstructor = FastShaleReconstructor3D(base_path, excel_path)

    # ðŸ› ï¸ FIX: Ensure directories are created before processing
    print("ðŸ“ Creating output directories...")
    reconstructor.create_output_directories()

    target_sample = "sample6"
    print(f"\nðŸŽ¯ Target sample: {target_sample}\n" + "="*70)

    # INCREASE EPOCHS FOR BETTER QUALITY
    try:
        result = reconstructor.process_sample(
            target_sample,
            unet_epochs=40,
            slicegan_epochs=50
        )

        if result:
            print(f"\nðŸŽ‰ SUCCESS! 7 different 3D shale volumes generated for {target_sample}")
            print(f"ðŸ“Š Scattered volume shape: {result['scattered_volume'].shape}")
            print(f"ðŸ“Š Continuous volume shape: {result['continuous_volume'].shape}")
            print(f"ðŸ“Š Realistic volume shape: {result['realistic_volume'].shape}")
            print(f"ðŸ“Š Complete Continuous volume shape: {result['complete_continuous_volume'].shape}")
            print(f"ðŸ“ˆ Scattered similarity: {result['scattered_similarity']:.4f}")
            print(f"ðŸ“ˆ Continuous similarity: {result['continuous_similarity']:.4f}")
            print(f"ðŸ“ˆ Realistic similarity: {result['realistic_similarity']:.4f}")
            print(f"ðŸ“ˆ Complete Continuous similarity: {result['complete_continuous_similarity']:.4f}")
            print(f"â±ï¸  Processing time: {result['processing_time']:.1f} minutes")
            print(f"ðŸ’¾ Abaqus files: {result['abaqus_files']}")
            print(f"ðŸ“ All results saved in Output/ directory")

            # Check quality metrics for realistic model (primary)
            volume = result['realistic_volume']
            intensity_std = np.std(volume)
            moran_i = morans_I(volume)

            print(f"\nðŸ“Š FINAL QUALITY METRICS:")
            print(f"   â€¢ Intensity standard deviation: {intensity_std:.4f}")
            print(f"   â€¢ Moran's I (spatial autocorrelation): {moran_i:.4f}")
            print(f"   â€¢ Realistic model similarity: {result['realistic_similarity']:.4f}")

            if intensity_std > 0.1 and moran_i > 0.1:
                print("âœ… Quality metrics are good!")
            else:
                print("âš ï¸  Consider increasing training epochs further for better quality")

    except FileNotFoundError as e:
        print(f"âŒ Directory error: {e}")
        print("ðŸ› ï¸  Creating missing directories and retrying...")
        reconstructor.create_output_directories()
        print("âœ… Directories created. Please run the script again.")
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()

    print("\n" + "="*70)
    print("ðŸŽ¯ RUNNING PAPER'S COMPLETE PIPELINE")
    print("="*70)

    # ðŸŽ¯ FIX: Use your existing sample data instead of requiring SEM image folder
    paper_pipeline = ShaleAnalysisPipeline()

    # Use the same base path and let the pipeline find SEM images automatically
    # or create synthetic data if no SEM images are found
    try:
        # Try to find SEM images in common locations
        possible_paths = [
            os.path.join(base_path, "SEM_Images"),
            os.path.join(base_path, "Images"),
            os.path.join(base_path, "SEM"),
            os.path.join(base_path, "Data"),
            base_path  # Use base path itself
        ]

        sem_images_path = None
        for path in possible_paths:
            if os.path.exists(path):
                # Check if there are image files in the directory
                image_files = [f for f in os.listdir(path) if f.endswith(('.png', '.jpg', '.tif', '.tiff'))]
                if image_files:
                    sem_images_path = path
                    print(f"ðŸ“ Found SEM images in: {sem_images_path}")
                    print(f"ðŸ“¸ Number of images: {len(image_files)}")
                    break

        if sem_images_path:
            paper_results = paper_pipeline.run_complete_pipeline(sem_images_path)
            print("âœ… Paper pipeline completed successfully!")
        else:
            print("ðŸ“ No SEM image folder found. Using synthetic data generation...")
            # The pipeline will automatically create synthetic data if no images are found
            paper_results = paper_pipeline.run_complete_pipeline(base_path)
            print("âœ… Paper pipeline completed with synthetic data!")

    except Exception as e:
        print(f"âŒ Paper pipeline error: {e}")
        print("ðŸ”„ Continuing with existing results...")

    print("\n" + "="*70)
    print("ðŸŽŠ ALL PROCESSING COMPLETED!")
    print("="*70)
    print("ðŸ“Š You now have:")
    print("   â€¢ 7 different 3D shale volume models")
    print("   â€¢ Abaqus export files for FEM analysis")
    print("   â€¢ Comprehensive quality metrics")
    print("   â€¢ Paper methodology implementation")
    print("   â€¢ Visualizations and analysis results")
    print("=" * 70)

